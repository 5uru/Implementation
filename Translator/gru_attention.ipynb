{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-09T15:43:33.712521Z",
     "start_time": "2025-04-09T15:43:32.022299Z"
    }
   },
   "source": [
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "from flax import nnx\n",
    "\n",
    "import tiktoken\n",
    "import grain.python as grain\n",
    "import tqdm\n",
    "\n",
    "from datasets import load_dataset\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Data",
   "id": "c0cc7c1dbf225d4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:43:40.427284Z",
     "start_time": "2025-04-09T15:43:33.725718Z"
    }
   },
   "cell_type": "code",
   "source": "ds = load_dataset(\"jonathansuru/fr_fon\")",
   "id": "fce278c45627db1d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:43:40.540437Z",
     "start_time": "2025-04-09T15:43:40.538586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def data_to_pairs(data):\n",
    "    text_pairs = []\n",
    "    for line in data:\n",
    "        fon = line[\"fon\"]\n",
    "        french = line[\"french\"]\n",
    "        french = \"[start] \" + french + \" [end]\"\n",
    "        text_pairs.append((fon, french))\n",
    "    return text_pairs"
   ],
   "id": "69c94cdc8a0c5bb9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:43:41.002017Z",
     "start_time": "2025-04-09T15:43:40.544860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_pairs = data_to_pairs(ds[\"train\"])\n",
    "val_pairs = data_to_pairs(ds[\"validation\"])\n",
    "test_pairs = data_to_pairs(ds[\"test\"])"
   ],
   "id": "10d1c57b044615ea",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:43:41.169322Z",
     "start_time": "2025-04-09T15:43:41.006798Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = tiktoken.get_encoding(\"o200k_base\")",
   "id": "bc43a4bad297901e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:43:41.204922Z",
     "start_time": "2025-04-09T15:43:41.202599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "strip_chars = string.punctuation + \"Â¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "vocab_size = tokenizer.n_vocab\n",
    "sequence_length = 512"
   ],
   "id": "705e32ba4beb4e27",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:43:41.233729Z",
     "start_time": "2025-04-09T15:43:41.230163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def custom_standardization(input_string):\n",
    "    lowercase = input_string.lower()\n",
    "    return re.sub(f\"[{re.escape(strip_chars)}]\", \"\", lowercase)\n",
    "\n",
    "\n",
    "def tokenize_and_pad(text, tokenizer, max_length):\n",
    "    tokens = tokenizer.encode(text)[:max_length]\n",
    "    padded = tokens + [0] * (max_length - len(tokens)) if len(tokens) < max_length else tokens ##assumes list-like - (https://github.com/openai/tiktoken/blob/main/tiktoken/core.py#L81 current tiktoken out)\n",
    "    return padded\n",
    "\n",
    "def format_dataset(fon, french, tokenizer, sequence_length):\n",
    "    fon = custom_standardization(fon)\n",
    "    french = custom_standardization(french)\n",
    "    fon = tokenize_and_pad(fon, tokenizer, sequence_length)\n",
    "    french = tokenize_and_pad(french, tokenizer, sequence_length)\n",
    "    return {\n",
    "            \"encoder_inputs\": fon,\n",
    "            \"decoder_inputs\": french[:-1],\n",
    "            \"target_output\": french[1:],\n",
    "    }"
   ],
   "id": "c37fe5b9841c6088",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:43:43.436136Z",
     "start_time": "2025-04-09T15:43:41.240445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data = [format_dataset(fon, french, tokenizer, sequence_length) for fon, french in train_pairs]\n",
    "val_data = [format_dataset(fon, french, tokenizer, sequence_length) for fon, french in val_pairs]\n",
    "test_data = [format_dataset(fon, french, tokenizer, sequence_length) for fon, french in test_pairs]"
   ],
   "id": "17faf77362599a93",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:48:00.774107Z",
     "start_time": "2025-04-09T15:48:00.771517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 1 #set here for the loader and model train later on\n",
    "\n",
    "class CustomPreprocessing(grain.MapTransform):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def map(self, data):\n",
    "        return {\n",
    "                \"encoder_inputs\": np.array(data[\"encoder_inputs\"]),\n",
    "                \"decoder_inputs\": np.array(data[\"decoder_inputs\"]),\n",
    "                \"target_output\": np.array(data[\"target_output\"]),\n",
    "        }\n"
   ],
   "id": "49cfe2240a31dc26",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:48:01.087585Z",
     "start_time": "2025-04-09T15:48:01.083402Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_sampler = grain.IndexSampler(\n",
    "        len(train_data) ,\n",
    "        shuffle=True ,\n",
    "        seed=12 ,  # Seed for reproducibility\n",
    "        shard_options=grain.NoSharding( ) ,  # No sharding since it's a single-device setup\n",
    "        num_epochs=1 ,  # Iterate over the dataset for one epoch\n",
    ")\n",
    "\n",
    "val_sampler = grain.IndexSampler(\n",
    "        len(val_data) ,\n",
    "        shuffle=False ,\n",
    "        seed=12 ,\n",
    "        shard_options=grain.NoSharding( ) ,\n",
    "        num_epochs=1 ,\n",
    ")\n",
    "\n",
    "train_loader = grain.DataLoader(\n",
    "        data_source=train_data ,\n",
    "        sampler=train_sampler ,  # Sampler to determine how to access the data\n",
    "        worker_count=4 ,  # Number of child processes launched to parallelize the transformations\n",
    "        worker_buffer_size=2 ,  # Count of output batches to produce in advance per worker\n",
    "        operations=[\n",
    "                CustomPreprocessing( ) ,\n",
    "                grain.Batch(batch_size=batch_size , drop_remainder=True) ,\n",
    "        ]\n",
    ")\n",
    "\n",
    "val_loader = grain.DataLoader(\n",
    "        data_source=val_data ,\n",
    "        sampler=val_sampler ,\n",
    "        worker_count=4 ,\n",
    "        worker_buffer_size=2 ,\n",
    "        operations=[\n",
    "                CustomPreprocessing( ) ,\n",
    "                grain.Batch(batch_size=batch_size) ,\n",
    "        ]\n",
    ")\n",
    "\n"
   ],
   "id": "9b490df3a982c7f8",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:53:29.428433Z",
     "start_time": "2025-04-09T15:53:29.421370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LuongAttention(nnx.Module):\n",
    "    def __init__(self, hidden_size, src_vocab_size, tgt_vocab_size, rngs=nnx.Rngs):\n",
    "        self.source_embedding = nnx.Embed(src_vocab_size, hidden_size, rngs=rngs)\n",
    "        self.target_embedding = nnx.Embed(tgt_vocab_size, hidden_size, rngs=rngs)\n",
    "\n",
    "        # Initialize RNNs for encoder and decoder\n",
    "        self.encoder = nnx.RNN(\n",
    "                nnx.GRUCell(hidden_size, hidden_size, rngs=rngs),\n",
    "                return_carry=True\n",
    "        )\n",
    "        self.decoder = nnx.RNN(\n",
    "                nnx.GRUCell(hidden_size, hidden_size, rngs=rngs),\n",
    "                return_carry=True\n",
    "        )\n",
    "\n",
    "        self.W_c = nnx.Linear(hidden_size * 2, hidden_size, rngs=rngs)\n",
    "        self.W_y = nnx.Linear(hidden_size, tgt_vocab_size, rngs=rngs)\n",
    "\n",
    "    def __call__(self, source, target, h_init):\n",
    "        # Compute embeddings; shape: (batch, seq_len, features)\n",
    "        source_seq = self.source_embedding(source)\n",
    "        target_seq = self.target_embedding(target)\n",
    "\n",
    "        # Encoder and decoder passes\n",
    "        h_final, h_t = self.encoder(source_seq, initial_carry=h_init)\n",
    "        # Reshape h_t to (batch, seq_len, hidden_size)\n",
    "        h_t = jnp.squeeze(h_t, axis=2)  # Remove the extra dimension\n",
    "        s_final, s_t = self.decoder(target_seq, initial_carry=h_final)\n",
    "        s_t = jnp.squeeze(s_t, axis=2)\n",
    "\n",
    "\n",
    "        # Compute attention scores: (batch, tgt_seq_len, hidden) @ (batch, hidden, src_seq_len)\n",
    "        e_t_i = jnp.matmul(s_t, jnp.transpose(h_t, (0, 2, 1)))\n",
    "\n",
    "        # Apply softmax to get alignment weights\n",
    "        alignment_scores = nnx.softmax(e_t_i, axis=-1)\n",
    "\n",
    "        # Compute context vectors: (batch, tgt_seq_len, src_seq_len) @ (batch, src_seq_len, hidden)\n",
    "        c_t = jnp.matmul(alignment_scores, h_t)\n",
    "\n",
    "        # Compute combined representation\n",
    "        s_hat_t = nnx.tanh(self.W_c(jnp.concatenate([s_t, c_t], axis=-1)))\n",
    "\n",
    "        # Project to vocabulary space\n",
    "        y_t = self.W_y(s_hat_t)\n",
    "\n",
    "        # Return in the expected format\n",
    "        return jnp.transpose(y_t, (1, 0, 2))"
   ],
   "id": "e6130cb82b7af76a",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:53:30.039726Z",
     "start_time": "2025-04-09T15:53:29.900202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inicializar red\n",
    "net = LuongAttention(\n",
    "        hidden_size=256,\n",
    "        src_vocab_size=3371,\n",
    "        tgt_vocab_size=2810,\n",
    "        rngs=nnx.Rngs(42)\n",
    ")\n",
    "\n",
    "# Crear entradas de prueba\n",
    "source = jnp.array([[10, 23, 5]])  # (batch=1, src_seq_len=3)\n",
    "target = jnp.array([[4, 9]])       # (batch=1, tgt_seq_len=2)\n",
    "\n",
    "# Create initial hidden state with proper dimensions\n",
    "batch_size = source.shape[0]\n",
    "h_init = jnp.zeros((batch_size, 256))  # (batch=1, hidden_size)\n",
    "\n",
    "# Forward pass\n",
    "output = net(source, target, h_init)\n",
    "print(output.shape)"
   ],
   "id": "71415f2885053b0f",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot select an axis to squeeze out which has size not equal to one, got shape=(1, 3, 256) and dimensions=(2,)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[41], line 18\u001B[0m\n\u001B[1;32m     15\u001B[0m h_init \u001B[38;5;241m=\u001B[39m jnp\u001B[38;5;241m.\u001B[39mzeros((batch_size, \u001B[38;5;241m256\u001B[39m))  \u001B[38;5;66;03m# (batch=1, hidden_size)\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh_init\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(output\u001B[38;5;241m.\u001B[39mshape)\n",
      "Cell \u001B[0;32mIn[40], line 27\u001B[0m, in \u001B[0;36mLuongAttention.__call__\u001B[0;34m(self, source, target, h_init)\u001B[0m\n\u001B[1;32m     25\u001B[0m h_final, h_t \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(source_seq, initial_carry\u001B[38;5;241m=\u001B[39mh_init)\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# Reshape h_t to (batch, seq_len, hidden_size)\u001B[39;00m\n\u001B[0;32m---> 27\u001B[0m h_t \u001B[38;5;241m=\u001B[39m \u001B[43mjnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[43mh_t\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Remove the extra dimension\u001B[39;00m\n\u001B[1;32m     28\u001B[0m s_final, s_t \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(target_seq, initial_carry\u001B[38;5;241m=\u001B[39mh_final)\n\u001B[1;32m     29\u001B[0m s_t \u001B[38;5;241m=\u001B[39m jnp\u001B[38;5;241m.\u001B[39msqueeze(s_t, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n",
      "File \u001B[0;32m/opt/miniconda3/envs/Implementation/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:2359\u001B[0m, in \u001B[0;36msqueeze\u001B[0;34m(a, axis)\u001B[0m\n\u001B[1;32m   2303\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Remove one or more length-1 axes from array\u001B[39;00m\n\u001B[1;32m   2304\u001B[0m \n\u001B[1;32m   2305\u001B[0m \u001B[38;5;124;03mJAX implementation of :func:`numpy.sqeeze`, implemented via :func:`jax.lax.squeeze`.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2356\u001B[0m \u001B[38;5;124;03m  Array([0, 1, 2], dtype=int32)\u001B[39;00m\n\u001B[1;32m   2357\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2358\u001B[0m arr \u001B[38;5;241m=\u001B[39m util\u001B[38;5;241m.\u001B[39mensure_arraylike(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msqueeze\u001B[39m\u001B[38;5;124m\"\u001B[39m, a)\n\u001B[0;32m-> 2359\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_squeeze\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_ensure_index_tuple\u001B[49m\u001B[43m(\u001B[49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "    \u001B[0;31m[... skipping hidden 16 frame]\u001B[0m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/Implementation/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:2369\u001B[0m, in \u001B[0;36m_squeeze\u001B[0;34m(a, axis)\u001B[0m\n\u001B[1;32m   2367\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjnp.squeeze with axis=None is not supported with shape polymorphism\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2368\u001B[0m   axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mtuple\u001B[39m(i \u001B[38;5;28;01mfor\u001B[39;00m i, d \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(a_shape) \u001B[38;5;28;01mif\u001B[39;00m d \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m-> 2369\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlax\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n",
      "    \u001B[0;31m[... skipping hidden 10 frame]\u001B[0m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/Implementation/lib/python3.12/site-packages/jax/_src/lax/lax.py:6627\u001B[0m, in \u001B[0;36m_compute_squeeze_shape\u001B[0;34m(shape, dimensions)\u001B[0m\n\u001B[1;32m   6625\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdimensions outside range [0, ndim): \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdimensions\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   6626\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;129;01mnot\u001B[39;00m core\u001B[38;5;241m.\u001B[39mdefinitely_equal(shape[d], \u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m dimensions):\n\u001B[0;32m-> 6627\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   6628\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcannot select an axis to squeeze out which has size not equal to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   6629\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mone, got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mshape\u001B[38;5;132;01m=}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdimensions\u001B[38;5;132;01m=}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   6630\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(s \u001B[38;5;28;01mfor\u001B[39;00m i, s \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(shape) \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m dims_set)\n",
      "\u001B[0;31mValueError\u001B[0m: cannot select an axis to squeeze out which has size not equal to one, got shape=(1, 3, 256) and dimensions=(2,)"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:53:35.674907Z",
     "start_time": "2025-04-09T15:53:35.672054Z"
    }
   },
   "cell_type": "code",
   "source": "output",
   "id": "a73f1f61eedd2fa1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.04860308, 0.00570555, 0.00494949, ..., 0.01821722,\n",
       "         0.03329365, 0.04287938]],\n",
       "\n",
       "       [[0.03826261, 0.03472698, 0.01133941, ..., 0.00335046,\n",
       "         0.02681436, 0.03405739]]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:53:36.400429Z",
     "start_time": "2025-04-09T15:53:36.398209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_loss(logits, labels):\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=labels)\n",
    "    return jnp.mean(loss)"
   ],
   "id": "438b85dfd84bedd5",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:53:36.721902Z",
     "start_time": "2025-04-09T15:53:36.717962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@nnx.jit\n",
    "def train_step(model, optimizer, batch):\n",
    "    def loss_fn(model, train_encoder_input, train_decoder_input, train_target_input):\n",
    "        h = jnp.zeros((1, train_encoder_input.shape[0],embed_dim))\n",
    "        logits = model(train_encoder_input, train_decoder_input, h)\n",
    "        loss = compute_loss(logits, train_target_input)\n",
    "        return loss\n",
    "\n",
    "    grad_fn = nnx.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(model, jnp.array(batch[\"encoder_inputs\"]), jnp.array(batch[\"decoder_inputs\"]), jnp.array(batch[\"target_output\"]))\n",
    "    optimizer.update(grads)\n",
    "    return loss\n",
    "\n",
    "@nnx.jit\n",
    "def eval_step(model, batch, eval_metrics):\n",
    "    h = jnp.zeros((1, jnp.array(batch[\"encoder_inputs\"]).shape[0], embed_dim))\n",
    "    logits = model(jnp.array(batch[\"encoder_inputs\"]), jnp.array(batch[\"decoder_inputs\"]), h)\n",
    "    loss = compute_loss(logits, jnp.array(batch[\"target_output\"]))\n",
    "    labels = jnp.array(batch[\"target_output\"])\n",
    "\n",
    "    eval_metrics.update(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            labels=labels,\n",
    "    )"
   ],
   "id": "46ddad8063802659",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:53:37.078704Z",
     "start_time": "2025-04-09T15:53:37.075263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "eval_metrics = nnx.MultiMetric(\n",
    "        loss=nnx.metrics.Average('loss'),\n",
    "        accuracy=nnx.metrics.Accuracy(),\n",
    ")\n",
    "\n",
    "train_metrics_history = {\n",
    "        \"train_loss\": [],\n",
    "}\n",
    "\n",
    "eval_metrics_history = {\n",
    "        \"test_loss\": [],\n",
    "        \"test_accuracy\": [],\n",
    "}"
   ],
   "id": "7cc944aa85974fb",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:53:37.404377Z",
     "start_time": "2025-04-09T15:53:37.401694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Hyperparameters\n",
    "rng = nnx.Rngs(0)\n",
    "embed_dim = 256\n",
    "latent_dim = 2048\n",
    "num_heads = 8\n",
    "dropout_rate = 0.5\n",
    "vocab_size = tokenizer.n_vocab\n",
    "sequence_length = 512\n",
    "learning_rate = 1.5e-3\n",
    "num_epochs = 10"
   ],
   "id": "368ae3024d5261ec",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:53:38.639772Z",
     "start_time": "2025-04-09T15:53:37.767516Z"
    }
   },
   "cell_type": "code",
   "source": "model = LuongAttention( hidden_size=embed_dim, src_vocab_size=vocab_size, tgt_vocab_size=vocab_size, rngs=rng)",
   "id": "94fa6ed40ad04219",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:53:38.677547Z",
     "start_time": "2025-04-09T15:53:38.667914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bar_format = \"{desc}[{n_fmt}/{total_fmt}]{postfix} [{elapsed}<{remaining}]\"\n",
    "train_total_steps = len(train_data) // batch_size\n",
    "\n",
    "def train_one_epoch(epoch):\n",
    "    model.train()  # Set model to the training mode: e.g. update batch statistics\n",
    "    with tqdm.tqdm(\n",
    "            desc=f\"[train] epoch: {epoch}/{num_epochs}, \",\n",
    "            total=train_total_steps,\n",
    "            bar_format=bar_format,\n",
    "            leave=True,\n",
    "    ) as pbar:\n",
    "        for batch in train_loader:\n",
    "            loss = train_step(model, optimizer, batch)\n",
    "            train_metrics_history[\"train_loss\"].append(loss.item())\n",
    "            pbar.set_postfix({\"loss\": loss.item()})\n",
    "            pbar.update(1)\n",
    "\n",
    "\n",
    "def evaluate_model(epoch):\n",
    "    # Compute the metrics on the train and val sets after each training epoch.\n",
    "    model.eval()  # Set model to evaluation model: e.g. use stored batch statistics\n",
    "\n",
    "    eval_metrics.reset()  # Reset the eval metrics\n",
    "    for val_batch in val_loader:\n",
    "        eval_step(model, val_batch, eval_metrics)\n",
    "\n",
    "    for metric, value in eval_metrics.compute().items():\n",
    "        eval_metrics_history[f'test_{metric}'].append(value)\n",
    "\n",
    "    print(f\"[test] epoch: {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"- total loss: {eval_metrics_history['test_loss'][-1]:0.4f}\")\n",
    "    print(f\"- Accuracy: {eval_metrics_history['test_accuracy'][-1]:0.4f}\")"
   ],
   "id": "73ecf3770d58a286",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:53:39.122108Z",
     "start_time": "2025-04-09T15:53:38.948494Z"
    }
   },
   "cell_type": "code",
   "source": "optimizer = nnx.Optimizer(model, optax.adamw(learning_rate))",
   "id": "8671c58cc87bb425",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:54:09.628645Z",
     "start_time": "2025-04-09T15:53:39.148319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(epoch)\n",
    "    evaluate_model(epoch)"
   ],
   "id": "ea70ffbbfda8d7b9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train] epoch: 0/10, [6/35039], loss=1.08 [00:27<33:17:17] Process SpawnProcess-12:\n",
      "Process SpawnProcess-9:\n",
      "Process SpawnProcess-10:\n",
      "Process SpawnProcess-11:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/Implementation/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/envs/Implementation/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/envs/Implementation/lib/python3.12/site-packages/grain/_src/python/grain_pool.py\", line 236, in _worker_loop\n",
      "    if not multiprocessing_common.add_element_to_queue(  # pytype: disable=wrong-arg-types\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/Implementation/lib/python3.12/site-packages/grain/_src/python/multiprocessing_common.py\", line 54, in add_element_to_queue\n",
      "    elements_queue.put(element, timeout=_QUEUE_WAIT_TIMEOUT_SECONDS)\n",
      "  File \"/opt/miniconda3/envs/Implementation/lib/python3.12/multiprocessing/queues.py\", line 89, in put\n",
      "    if not self._sem.acquire(block, timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/Implementation/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/envs/Implementation/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/envs/Implementation/lib/python3.12/site-packages/grain/_src/python/grain_pool.py\", line 236, in _worker_loop\n",
      "    if not multiprocessing_common.add_element_to_queue(  # pytype: disable=wrong-arg-types\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/Implementation/lib/python3.12/site-packages/grain/_src/python/multiprocessing_common.py\", line 54, in add_element_to_queue\n",
      "    elements_queue.put(element, timeout=_QUEUE_WAIT_TIMEOUT_SECONDS)\n",
      "  File \"/opt/miniconda3/envs/Implementation/lib/python3.12/multiprocessing/queues.py\", line 89, in put\n",
      "    if not self._sem.acquire(block, timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/miniconda3/envs/Implementation/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/envs/Implementation/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/envs/Implementation/lib/python3.12/site-packages/grain/_src/python/grain_pool.py\", line 236, in _worker_loop\n",
      "    if not multiprocessing_common.add_element_to_queue(  # pytype: disable=wrong-arg-types\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/Implementation/lib/python3.12/site-packages/grain/_src/python/multiprocessing_common.py\", line 54, in add_element_to_queue\n",
      "    elements_queue.put(element, timeout=_QUEUE_WAIT_TIMEOUT_SECONDS)\n",
      "  File \"/opt/miniconda3/envs/Implementation/lib/python3.12/multiprocessing/queues.py\", line 89, in put\n",
      "    if not self._sem.acquire(block, timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/miniconda3/envs/Implementation/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/miniconda3/envs/Implementation/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/miniconda3/envs/Implementation/lib/python3.12/site-packages/grain/_src/python/grain_pool.py\", line 236, in _worker_loop\n",
      "    if not multiprocessing_common.add_element_to_queue(  # pytype: disable=wrong-arg-types\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/Implementation/lib/python3.12/site-packages/grain/_src/python/multiprocessing_common.py\", line 54, in add_element_to_queue\n",
      "    elements_queue.put(element, timeout=_QUEUE_WAIT_TIMEOUT_SECONDS)\n",
      "  File \"/opt/miniconda3/envs/Implementation/lib/python3.12/multiprocessing/queues.py\", line 89, in put\n",
      "    if not self._sem.acquire(block, timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[train] epoch: 0/10, [6/35039], loss=1.08 [00:30<49:17:48]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[50], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[0;32m----> 2\u001B[0m     \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m     evaluate_model(epoch)\n",
      "Cell \u001B[0;32mIn[48], line 14\u001B[0m, in \u001B[0;36mtrain_one_epoch\u001B[0;34m(epoch)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[1;32m     13\u001B[0m     loss \u001B[38;5;241m=\u001B[39m train_step(model, optimizer, batch)\n\u001B[0;32m---> 14\u001B[0m     train_metrics_history[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(\u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     15\u001B[0m     pbar\u001B[38;5;241m.\u001B[39mset_postfix({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m: loss\u001B[38;5;241m.\u001B[39mitem()})\n\u001B[1;32m     16\u001B[0m     pbar\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m/opt/miniconda3/envs/Implementation/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:213\u001B[0m, in \u001B[0;36m_item\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    211\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_item\u001B[39m(\u001B[38;5;28mself\u001B[39m: Array, \u001B[38;5;241m*\u001B[39margs: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28mcomplex\u001B[39m:\n\u001B[1;32m    212\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Copy an element of an array to a standard Python scalar and return it.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 213\u001B[0m   arr \u001B[38;5;241m=\u001B[39m \u001B[43mcore\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconcrete_or_error\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masarray\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mThis occurred in the item() method of jax.Array\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    214\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m dtypes\u001B[38;5;241m.\u001B[39missubdtype(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdtype, dtypes\u001B[38;5;241m.\u001B[39mextended):\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo Python scalar type for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00marr\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;132;01m=}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/miniconda3/envs/Implementation/lib/python3.12/site-packages/jax/_src/core.py:1624\u001B[0m, in \u001B[0;36mconcrete_or_error\u001B[0;34m(force, val, context)\u001B[0m\n\u001B[1;32m   1622\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m force(maybe_concrete)\n\u001B[1;32m   1623\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1624\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mval\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "16e27c4802d5faf5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
