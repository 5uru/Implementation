{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-08T16:32:47.397314Z",
     "start_time": "2025-04-08T16:32:45.269884Z"
    }
   },
   "source": [
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "from flax import nnx\n",
    "\n",
    "import tiktoken\n",
    "import grain.python as grain\n",
    "import tqdm\n",
    "\n",
    "from datasets import load_dataset\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T19:28:44.358916Z",
     "start_time": "2025-04-08T19:28:44.341412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LuongAttention(nnx.Module):\n",
    "    def __init__(self, hidden_size, src_vocab_size, tgt_vocab_size, rngs=nnx.Rngs):\n",
    "        self.source_embedding = nnx.Embed(src_vocab_size, hidden_size, rngs=rngs)\n",
    "        self.target_embedding = nnx.Embed(tgt_vocab_size, hidden_size, rngs=rngs)\n",
    "\n",
    "        # Initialize RNNs for encoder and decoder\n",
    "        self.encoder = nnx.RNN(\n",
    "                nnx.GRUCell(hidden_size, hidden_size, rngs=rngs),\n",
    "                return_carry=True\n",
    "        )\n",
    "        self.decoder = nnx.RNN(\n",
    "                nnx.GRUCell(hidden_size, hidden_size, rngs=rngs),\n",
    "                return_carry=True\n",
    "        )\n",
    "\n",
    "        self.W_c = nnx.Linear(hidden_size * 2, hidden_size, rngs=rngs)\n",
    "        self.W_y = nnx.Linear(hidden_size, tgt_vocab_size, rngs=rngs)\n",
    "\n",
    "    def __call__(self, source, target, h_init):\n",
    "        # Compute embeddings; shape: (batch, seq_len, features)\n",
    "        source_seq = self.source_embedding(source)\n",
    "        target_seq = self.target_embedding(target)\n",
    "\n",
    "        # Encoder and decoder passes\n",
    "        h_t, h_final = self.encoder(source_seq, initial_carry=h_init)\n",
    "        s_t, s_final = self.decoder(target_seq, initial_carry=h_final)\n",
    "\n",
    "        # Handle shape transformations properly\n",
    "        # If outputs have shape (seq_len, batch, hidden), transpose to (batch, seq_len, hidden)\n",
    "        if h_t.shape[0] != source.shape[0]:\n",
    "            h_t = jnp.transpose(h_t, (1, 0, 2))  # (batch, src_seq_len, hidden)\n",
    "            s_t = jnp.transpose(s_t, (1, 0, 2))  # (batch, tgt_seq_len, hidden)\n",
    "\n",
    "        # Compute attention scores: (batch, tgt_seq_len, hidden) @ (batch, hidden, src_seq_len)\n",
    "        e_t_i = jnp.matmul(s_t, jnp.transpose(h_t, (0, 2, 1)))\n",
    "\n",
    "        # Apply softmax to get alignment weights\n",
    "        alignment_scores = nnx.softmax(e_t_i, axis=-1)\n",
    "\n",
    "        # Compute context vectors: (batch, tgt_seq_len, src_seq_len) @ (batch, src_seq_len, hidden)\n",
    "        c_t = jnp.matmul(alignment_scores, h_t)\n",
    "\n",
    "        # Compute combined representation\n",
    "        s_hat_t = nnx.tanh(self.W_c(jnp.concatenate([s_t, c_t], axis=-1)))\n",
    "\n",
    "        # Project to vocabulary space\n",
    "        y_t = self.W_y(s_hat_t)\n",
    "\n",
    "        # Return in expected format\n",
    "        return jnp.transpose(y_t, (1, 0, 2))  # (tgt_seq_len, batch, tgt_vocab_size)"
   ],
   "id": "e6130cb82b7af76a",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T19:28:45.598560Z",
     "start_time": "2025-04-08T19:28:45.039224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inicializar red\n",
    "net = LuongAttention(\n",
    "        hidden_size=256,\n",
    "        src_vocab_size=3371,\n",
    "        tgt_vocab_size=2810,\n",
    "        rngs=nnx.Rngs(42)\n",
    ")\n",
    "\n",
    "# Crear entradas de prueba\n",
    "source = jnp.array([[10, 23, 5]])  # (batch=1, src_seq_len=3)\n",
    "target = jnp.array([[4, 9]])       # (batch=1, tgt_seq_len=2)\n",
    "\n",
    "# Create initial hidden state with proper dimensions\n",
    "batch_size = source.shape[0]\n",
    "h_init = jnp.zeros((batch_size, 256))  # (batch=1, hidden_size)\n",
    "\n",
    "# Forward pass\n",
    "output = net(source, target, h_init)\n",
    "print(output.shape)  # Debería mostrar (2, 1, 2810)"
   ],
   "id": "71415f2885053b0f",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axis 2 is out of bounds for array of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[53], line 18\u001B[0m\n\u001B[1;32m     15\u001B[0m h_init \u001B[38;5;241m=\u001B[39m jnp\u001B[38;5;241m.\u001B[39mzeros((batch_size, \u001B[38;5;241m256\u001B[39m))  \u001B[38;5;66;03m# (batch=1, hidden_size)\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mh_init\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(output\u001B[38;5;241m.\u001B[39mshape)  \u001B[38;5;66;03m# Debería mostrar (2, 1, 2810)\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[52], line 35\u001B[0m, in \u001B[0;36mLuongAttention.__call__\u001B[0;34m(self, source, target, h_init)\u001B[0m\n\u001B[1;32m     32\u001B[0m     s_t \u001B[38;5;241m=\u001B[39m jnp\u001B[38;5;241m.\u001B[39mtranspose(s_t, (\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m))  \u001B[38;5;66;03m# (batch, tgt_seq_len, hidden)\u001B[39;00m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# Compute attention scores: (batch, tgt_seq_len, hidden) @ (batch, hidden, src_seq_len)\u001B[39;00m\n\u001B[0;32m---> 35\u001B[0m e_t_i \u001B[38;5;241m=\u001B[39m jnp\u001B[38;5;241m.\u001B[39mmatmul(s_t, \u001B[43mjnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranspose\u001B[49m\u001B[43m(\u001B[49m\u001B[43mh_t\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# Apply softmax to get alignment weights\u001B[39;00m\n\u001B[1;32m     38\u001B[0m alignment_scores \u001B[38;5;241m=\u001B[39m nnx\u001B[38;5;241m.\u001B[39msoftmax(e_t_i, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m/opt/miniconda3/envs/Implementation/lib/python3.12/site-packages/jax/_src/numpy/lax_numpy.py:1208\u001B[0m, in \u001B[0;36mtranspose\u001B[0;34m(a, axes)\u001B[0m\n\u001B[1;32m   1206\u001B[0m util\u001B[38;5;241m.\u001B[39mcheck_arraylike(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtranspose\u001B[39m\u001B[38;5;124m\"\u001B[39m, a)\n\u001B[1;32m   1207\u001B[0m axes_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mrange\u001B[39m(np\u001B[38;5;241m.\u001B[39mndim(a))[::\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;28;01mif\u001B[39;00m axes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m axes\n\u001B[0;32m-> 1208\u001B[0m axes_ \u001B[38;5;241m=\u001B[39m [\u001B[43m_canonicalize_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mndim\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m axes_]\n\u001B[1;32m   1209\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m lax\u001B[38;5;241m.\u001B[39mtranspose(a, axes_)\n",
      "File \u001B[0;32m/opt/miniconda3/envs/Implementation/lib/python3.12/site-packages/jax/_src/util.py:433\u001B[0m, in \u001B[0;36mcanonicalize_axis\u001B[0;34m(axis, num_dims)\u001B[0m\n\u001B[1;32m    431\u001B[0m axis \u001B[38;5;241m=\u001B[39m operator\u001B[38;5;241m.\u001B[39mindex(axis)\n\u001B[1;32m    432\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;241m-\u001B[39mnum_dims \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m axis \u001B[38;5;241m<\u001B[39m num_dims:\n\u001B[0;32m--> 433\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maxis \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maxis\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is out of bounds for array of dimension \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_dims\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    434\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m axis \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    435\u001B[0m   axis \u001B[38;5;241m=\u001B[39m axis \u001B[38;5;241m+\u001B[39m num_dims\n",
      "\u001B[0;31mValueError\u001B[0m: axis 2 is out of bounds for array of dimension 2"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T18:12:58.175460Z",
     "start_time": "2025-04-08T18:12:58.173334Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a73f1f61eedd2fa1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "438b85dfd84bedd5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
