{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-09T16:53:45.309240Z",
     "start_time": "2025-04-09T16:53:43.565233Z"
    }
   },
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "from flax import nnx\n",
    "\n",
    "import tiktoken\n",
    "import grain.python as grain\n",
    "import tqdm\n",
    "\n",
    "from datasets import load_dataset"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Data Loading and Preparation",
   "id": "c0cc7c1dbf225d4e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:53:51.715745Z",
     "start_time": "2025-04-09T16:53:45.312794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the French-Fon dataset\n",
    "ds = load_dataset(\"jonathansuru/fr_fon\")"
   ],
   "id": "fce278c45627db1d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:53:51.845343Z",
     "start_time": "2025-04-09T16:53:51.842781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def data_to_pairs(data):\n",
    "    text_pairs = []\n",
    "    for line in data:\n",
    "        fon = line[\"fon\"]\n",
    "        french = line[\"french\"]\n",
    "        french = \"[start] \" + french + \" [end]\"\n",
    "        text_pairs.append((fon, french))\n",
    "    return text_pairs"
   ],
   "id": "69c94cdc8a0c5bb9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:53:52.367176Z",
     "start_time": "2025-04-09T16:53:51.854692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create train/validation/test pairs\n",
    "train_pairs = data_to_pairs(ds[\"train\"])\n",
    "val_pairs = data_to_pairs(ds[\"validation\"])\n",
    "test_pairs = data_to_pairs(ds[\"test\"])"
   ],
   "id": "10d1c57b044615ea",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Tokenization and Preprocessing",
   "id": "9e40db98eb181fd2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:53:52.538586Z",
     "start_time": "2025-04-09T16:53:52.369879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"o200k_base\")"
   ],
   "id": "bc43a4bad297901e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:53:52.545744Z",
     "start_time": "2025-04-09T16:53:52.543335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define characters to strip from a text\n",
    "strip_chars = string.punctuation + \"Â¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "# Set vocabulary size based on tokenizer\n",
    "vocab_size = tokenizer.n_vocab\n",
    "sequence_length = 512"
   ],
   "id": "705e32ba4beb4e27",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:53:52.553145Z",
     "start_time": "2025-04-09T16:53:52.550153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def custom_standardization(input_string):\n",
    "    lowercase = input_string.lower()\n",
    "    return re.sub(f\"[{re.escape(strip_chars)}]\", \"\", lowercase)\n",
    "\n",
    "\n",
    "def tokenize_and_pad(text, tokenizer, max_length):\n",
    "    tokens = tokenizer.encode(text)[:max_length]\n",
    "    padded = tokens + [0] * (max_length - len(tokens)) if len(tokens) < max_length else tokens ##assumes list-like - (https://github.com/openai/tiktoken/blob/main/tiktoken/core.py#L81 current tiktoken out)\n",
    "    return padded\n",
    "\n",
    "def format_dataset(fon, french, tokenizer, sequence_length):\n",
    "    fon = custom_standardization(fon)\n",
    "    french = custom_standardization(french)\n",
    "    fon = tokenize_and_pad(fon, tokenizer, sequence_length)\n",
    "    french = tokenize_and_pad(french, tokenizer, sequence_length)\n",
    "    return {\n",
    "            \"encoder_inputs\": fon,\n",
    "            \"decoder_inputs\": french[:-1],\n",
    "            \"target_output\": french[1:],\n",
    "    }"
   ],
   "id": "c37fe5b9841c6088",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:53:54.513755Z",
     "start_time": "2025-04-09T16:53:52.557536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare datasets\n",
    "train_data = [format_dataset(fon, french, tokenizer, sequence_length) for fon, french in train_pairs]\n",
    "val_data = [format_dataset(fon, french, tokenizer, sequence_length) for fon, french in val_pairs]\n",
    "test_data = [format_dataset(fon, french, tokenizer, sequence_length) for fon, french in test_pairs]"
   ],
   "id": "17faf77362599a93",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Data Loaders",
   "id": "438104f729be0265"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:54:27.685985Z",
     "start_time": "2025-04-09T16:54:27.683127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set batch size\n",
    "batch_size = 2 #set here for the loader and model train later on\n",
    "\n",
    "class CustomPreprocessing(grain.MapTransform):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def map(self, data):\n",
    "        return {\n",
    "                \"encoder_inputs\": np.array(data[\"encoder_inputs\"]),\n",
    "                \"decoder_inputs\": np.array(data[\"decoder_inputs\"]),\n",
    "                \"target_output\": np.array(data[\"target_output\"]),\n",
    "        }\n"
   ],
   "id": "49cfe2240a31dc26",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:54:28.073269Z",
     "start_time": "2025-04-09T16:54:28.069641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create data loaders with grain\n",
    "train_sampler = grain.IndexSampler(\n",
    "        len(train_data) ,\n",
    "        shuffle=True ,\n",
    "        seed=12 ,  # Seed for reproducibility\n",
    "        shard_options=grain.NoSharding( ) ,  # No sharding since it's a single-device setup\n",
    "        num_epochs=1 ,  # Iterate over the dataset for one epoch\n",
    ")\n",
    "\n",
    "val_sampler = grain.IndexSampler(\n",
    "        len(val_data) ,\n",
    "        shuffle=False ,\n",
    "        seed=12 ,\n",
    "        shard_options=grain.NoSharding( ) ,\n",
    "        num_epochs=1 ,\n",
    ")\n",
    "\n",
    "train_loader = grain.DataLoader(\n",
    "        data_source=train_data ,\n",
    "        sampler=train_sampler ,  # Sampler to determine how to access the data\n",
    "        worker_count=4 ,  # Number of child processes launched to parallelize the transformations\n",
    "        worker_buffer_size=2 ,  # Count of output batches to produce in advance per worker\n",
    "        operations=[\n",
    "                CustomPreprocessing( ) ,\n",
    "                grain.Batch(batch_size=batch_size , drop_remainder=True) ,\n",
    "        ]\n",
    ")\n",
    "\n",
    "val_loader = grain.DataLoader(\n",
    "        data_source=val_data ,\n",
    "        sampler=val_sampler ,\n",
    "        worker_count=4 ,\n",
    "        worker_buffer_size=2 ,\n",
    "        operations=[\n",
    "                CustomPreprocessing( ) ,\n",
    "                grain.Batch(batch_size=batch_size) ,\n",
    "        ]\n",
    ")\n",
    "\n"
   ],
   "id": "9b490df3a982c7f8",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Model Architecture",
   "id": "8cba50e557e5431"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T17:04:31.479184Z",
     "start_time": "2025-04-09T17:04:31.459536Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LuongAttention(nnx.Module):\n",
    "    def __init__(self, hidden_size, src_vocab_size, tgt_vocab_size, rngs=nnx.Rngs):\n",
    "        self.source_embedding = nnx.Embed(src_vocab_size, hidden_size, rngs=rngs)\n",
    "        self.target_embedding = nnx.Embed(tgt_vocab_size, hidden_size, rngs=rngs)\n",
    "\n",
    "        # Initialize RNNs for encoder and decoder\n",
    "        self.encoder = nnx.RNN(\n",
    "                nnx.GRUCell(hidden_size, hidden_size, rngs=rngs),\n",
    "                return_carry=True\n",
    "        )\n",
    "        self.decoder = nnx.RNN(\n",
    "                nnx.GRUCell(hidden_size, hidden_size, rngs=rngs),\n",
    "                return_carry=True\n",
    "        )\n",
    "\n",
    "        self.W_c = nnx.Linear(hidden_size * 2, hidden_size, rngs=rngs)\n",
    "        self.W_y = nnx.Linear(hidden_size, tgt_vocab_size, rngs=rngs)\n",
    "\n",
    "    def __call__(self, source, target, h_init):\n",
    "        # Compute embeddings\n",
    "        source_seq = self.source_embedding(source)  # (batch, src_seq_len, hidden)\n",
    "        target_seq = self.target_embedding(target)  # (batch, tgt_seq_len, hidden)\n",
    "\n",
    "        # Encoder pass\n",
    "        h_final, h_t = self.encoder(source_seq, initial_carry=h_init)\n",
    "\n",
    "        # Decoder pass\n",
    "        s_final, s_t = self.decoder(target_seq, initial_carry=h_final)\n",
    "\n",
    "        # Reshape hidden states for attention calculation\n",
    "        # s_t shape: (batch, tgt_seq_len, hidden)\n",
    "        # h_t shape: (batch, src_seq_len, hidden)\n",
    "\n",
    "        # Compute attention scores using dot product\n",
    "        # (batch, tgt_seq_len, hidden) @ (batch, hidden, src_seq_len) = (batch, tgt_seq_len, src_seq_len)\n",
    "        e_t_i = jnp.matmul(s_t, jnp.transpose(h_t, (0, 2, 1)))\n",
    "\n",
    "        # Apply softmax to get alignment weights (along src_seq_len dimension)\n",
    "        alignment_scores = nnx.softmax(e_t_i, axis=-1)\n",
    "\n",
    "        # Compute context vectors\n",
    "        # (batch, tgt_seq_len, src_seq_len) @ (batch, src_seq_len, hidden) = (batch, tgt_seq_len, hidden)\n",
    "        c_t = jnp.matmul(alignment_scores, h_t)\n",
    "\n",
    "        # Concatenate and process for final output\n",
    "        # Concatenate decoder output with context vector\n",
    "        s_hat_t = jnp.concatenate([s_t, c_t], axis=-1)  # (batch, tgt_seq_len, hidden*2)\n",
    "        s_hat_t = nnx.tanh(self.W_c(s_hat_t))  # (batch, tgt_seq_len, hidden)\n",
    "\n",
    "        # Project to vocabulary space\n",
    "        y_t = self.W_y(s_hat_t)  # (batch, tgt_seq_len, vocab_size)\n",
    "\n",
    "        return y_t"
   ],
   "id": "e6130cb82b7af76a",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T17:04:34.833465Z",
     "start_time": "2025-04-09T17:04:34.693729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inicializar red\n",
    "net = LuongAttention(\n",
    "        hidden_size=256,\n",
    "        src_vocab_size=3371,\n",
    "        tgt_vocab_size=2810,\n",
    "        rngs=nnx.Rngs(42)\n",
    ")\n",
    "\n",
    "# Crear entradas de prueba\n",
    "source = jnp.array([[10, 23, 5]])  # (batch=1, src_seq_len=3)\n",
    "target = jnp.array([[4, 9]])       # (batch=1, tgt_seq_len=2)\n",
    "\n",
    "# Create initial hidden state with proper dimensions\n",
    "batch_size = source.shape[0]\n",
    "h_init = jnp.zeros((batch_size, 256))  # (batch=1, hidden_size)\n",
    "\n",
    "# Forward pass\n",
    "output = net(source, target, h_init)\n",
    "print(output.shape)"
   ],
   "id": "971bc7797a5fa792",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 2810)\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Training Functions",
   "id": "50b542da5aa6fdb6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T17:04:38.860384Z",
     "start_time": "2025-04-09T17:04:38.858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_loss(logits, labels):\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=labels)\n",
    "    return jnp.mean(loss)"
   ],
   "id": "438b85dfd84bedd5",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T17:04:39.095043Z",
     "start_time": "2025-04-09T17:04:39.091834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@nnx.jit\n",
    "def train_step(model, optimizer, batch):\n",
    "    def loss_fn(model, train_encoder_input, train_decoder_input, train_target_input):\n",
    "        h = jnp.zeros((train_encoder_input.shape[0],embed_dim))\n",
    "        logits = model(train_encoder_input, train_decoder_input, h)\n",
    "        loss = compute_loss(logits, train_target_input)\n",
    "        return loss\n",
    "\n",
    "    grad_fn = nnx.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(model, jnp.array(batch[\"encoder_inputs\"]), jnp.array(batch[\"decoder_inputs\"]), jnp.array(batch[\"target_output\"]))\n",
    "    optimizer.update(grads)\n",
    "    return loss\n",
    "\n",
    "@nnx.jit\n",
    "def eval_step(model, batch, eval_metrics):\n",
    "    h = jnp.zeros(( jnp.array(batch[\"encoder_inputs\"]).shape[0], embed_dim))\n",
    "    logits = model(jnp.array(batch[\"encoder_inputs\"]), jnp.array(batch[\"decoder_inputs\"]), h)\n",
    "    loss = compute_loss(logits, jnp.array(batch[\"target_output\"]))\n",
    "    labels = jnp.array(batch[\"target_output\"])\n",
    "\n",
    "    eval_metrics.update(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            labels=labels,\n",
    "    )"
   ],
   "id": "46ddad8063802659",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T17:04:39.300813Z",
     "start_time": "2025-04-09T17:04:39.297942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize metrics tracking\n",
    "eval_metrics = nnx.MultiMetric(\n",
    "        loss=nnx.metrics.Average('loss'),\n",
    "        accuracy=nnx.metrics.Accuracy(),\n",
    ")\n",
    "\n",
    "train_metrics_history = {\n",
    "        \"train_loss\": [],\n",
    "}\n",
    "\n",
    "eval_metrics_history = {\n",
    "        \"test_loss\": [],\n",
    "        \"test_accuracy\": [],\n",
    "}"
   ],
   "id": "7cc944aa85974fb",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " ## 7. Hyperparameters and Model Setup",
   "id": "a0fd2697162d25ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T17:04:39.729074Z",
     "start_time": "2025-04-09T17:04:39.726262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Hyperparameters\n",
    "rng = nnx.Rngs(0)\n",
    "embed_dim = 256\n",
    "latent_dim = 2048\n",
    "vocab_size = tokenizer.n_vocab\n",
    "sequence_length = 512\n",
    "learning_rate = 1.5e-3\n",
    "num_epochs = 10"
   ],
   "id": "368ae3024d5261ec",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T17:04:40.725956Z",
     "start_time": "2025-04-09T17:04:40.102908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize model\n",
    "model = LuongAttention( hidden_size=embed_dim, src_vocab_size=vocab_size, tgt_vocab_size=vocab_size, rngs=rng)"
   ],
   "id": "94fa6ed40ad04219",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T17:04:41.334370Z",
     "start_time": "2025-04-09T17:04:41.083300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize optimizer\n",
    "optimizer = nnx.Optimizer(model, optax.adamw(learning_rate))"
   ],
   "id": "8671c58cc87bb425",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. Training Loop",
   "id": "53e8a6145ca071c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T17:04:41.913343Z",
     "start_time": "2025-04-09T17:04:41.908673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training utilities\n",
    "bar_format = \"{desc}[{n_fmt}/{total_fmt}]{postfix} [{elapsed}<{remaining}]\"\n",
    "train_total_steps = len(train_data) // batch_size\n",
    "\n",
    "def train_one_epoch(epoch):\n",
    "    model.train()  # Set model to the training mode: e.g. update batch statistics\n",
    "    with tqdm.tqdm(\n",
    "            desc=f\"[train] epoch: {epoch}/{num_epochs}, \",\n",
    "            total=train_total_steps,\n",
    "            bar_format=bar_format,\n",
    "            leave=True,\n",
    "    ) as pbar:\n",
    "        for batch in train_loader:\n",
    "            loss = train_step(model, optimizer, batch)\n",
    "            train_metrics_history[\"train_loss\"].append(loss.item())\n",
    "            pbar.set_postfix({\"loss\": loss.item()})\n",
    "            pbar.update(1)\n",
    "\n",
    "\n",
    "def evaluate_model(epoch):\n",
    "    # Compute the metrics on the train and val sets after each training epoch.\n",
    "    model.eval()  # Set model to evaluation model: e.g. use stored batch statistics\n",
    "\n",
    "    eval_metrics.reset()  # Reset the eval metrics\n",
    "    for val_batch in val_loader:\n",
    "        eval_step(model, val_batch, eval_metrics)\n",
    "\n",
    "    for metric, value in eval_metrics.compute().items():\n",
    "        eval_metrics_history[f'test_{metric}'].append(value)\n",
    "\n",
    "    print(f\"[test] epoch: {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"- total loss: {eval_metrics_history['test_loss'][-1]:0.4f}\")\n",
    "    print(f\"- Accuracy: {eval_metrics_history['test_accuracy'][-1]:0.4f}\")"
   ],
   "id": "73ecf3770d58a286",
   "outputs": [],
   "execution_count": 86
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Run training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(epoch)\n",
    "    evaluate_model(epoch)"
   ],
   "id": "ea70ffbbfda8d7b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "16e27c4802d5faf5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
