{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-09T16:10:29.468584Z",
     "start_time": "2025-04-09T16:10:27.621386Z"
    }
   },
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "from flax import nnx\n",
    "\n",
    "import tiktoken\n",
    "import grain.python as grain\n",
    "import tqdm\n",
    "\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from semhash import SemHash"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Data Preparation\n",
    "# Load, preprocess, and tokenize the dataset."
   ],
   "id": "74ffc40125ff1d32"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:11:12.842189Z",
     "start_time": "2025-04-09T16:11:06.191541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the dataset\n",
    "ds = load_dataset(\"jonathansuru/fr_fon\")"
   ],
   "id": "c0cd23c0870e434d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:11:18.628398Z",
     "start_time": "2025-04-09T16:11:18.625283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to convert data to pairs\n",
    "def data_to_pairs(data):\n",
    "    text_pairs = []\n",
    "    for line in data:\n",
    "        fon = line[\"fon\"]\n",
    "        french = line[\"french\"]\n",
    "        french = \"[start] \" + french + \" [end]\"  # Add start and end tokens\n",
    "        text_pairs.append((fon, french))\n",
    "    return text_pairs\n"
   ],
   "id": "d293ef64185b0f41",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Convert the dataset to a list of dictionaries\n",
    "records = [dict(row) for row in ds[\"train\"]]\n",
    "\n",
    "# Initialize SemHash with the columns to deduplicate\n",
    "semhash = SemHash.from_records(records=records, columns=[\"fon\", \"french\"])\n",
    "# Deduplicate the test data against the training data\n",
    "deduplicated_test_texts = semhash.deduplicate(records=ds[\"test\"]).deduplicated"
   ],
   "id": "227a48d3a7b9e52d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:11:43.020619Z",
     "start_time": "2025-04-09T16:11:42.418101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create train, validation, and test pairs\n",
    "train_pairs = data_to_pairs(ds[\"train\"])\n",
    "val_pairs = data_to_pairs(ds[\"validation\"])\n",
    "test_pairs = data_to_pairs(deduplicated_test_texts)"
   ],
   "id": "a036fe154a0a9c4",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:11:55.018419Z",
     "start_time": "2025-04-09T16:11:55.015429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display the number of pairs in each set\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ],
   "id": "c1dbeb6f74f3cf23",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35039 training pairs\n",
      "8760 validation pairs\n",
      "10950 test pairs\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:12:05.265871Z",
     "start_time": "2025-04-09T16:12:05.049120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"o200k_base\")"
   ],
   "id": "8135c65625d5cc7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:12:19.622635Z",
     "start_time": "2025-04-09T16:12:19.617601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define parameters\n",
    "strip_chars = string.punctuation + \"Â¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "vocab_size = tokenizer.n_vocab\n",
    "sequence_length = 512"
   ],
   "id": "b0ddb9515827dae1",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:12:29.352318Z",
     "start_time": "2025-04-09T16:12:29.350183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Custom standardization function\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = input_string.lower()\n",
    "    return re.sub(f\"[{re.escape(strip_chars)}]\", \"\", lowercase)"
   ],
   "id": "9c65ff2b269e6582",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:12:38.351062Z",
     "start_time": "2025-04-09T16:12:38.348366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tokenize and pad function\n",
    "def tokenize_and_pad(text, tokenizer, max_length):\n",
    "    tokens = tokenizer.encode(text)[:max_length]\n",
    "    padded = tokens + [0] * (max_length - len(tokens)) if len(tokens) < max_length else tokens\n",
    "    return padded"
   ],
   "id": "d5cf4a5d9ae2cec7",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:12:47.778756Z",
     "start_time": "2025-04-09T16:12:47.775979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Format dataset function\n",
    "def format_dataset(fon, french, tokenizer, sequence_length):\n",
    "    fon = custom_standardization(fon)\n",
    "    french = custom_standardization(french)\n",
    "    fon = tokenize_and_pad(fon, tokenizer, sequence_length)\n",
    "    french = tokenize_and_pad(french, tokenizer, sequence_length)\n",
    "    return {\n",
    "            \"encoder_inputs\": fon,\n",
    "            \"decoder_inputs\": french[:-1],\n",
    "            \"target_output\": french[1:],\n",
    "    }\n"
   ],
   "id": "d711e2613922dae1",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:13:00.332531Z",
     "start_time": "2025-04-09T16:12:58.244125Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create train, validation, and test data\n",
    "train_data = [format_dataset(fon, french, tokenizer, sequence_length) for fon, french in train_pairs]\n",
    "val_data = [format_dataset(fon, french, tokenizer, sequence_length) for fon, french in val_pairs]\n",
    "test_data = [format_dataset(fon, french, tokenizer, sequence_length) for fon, french in test_pairs]\n"
   ],
   "id": "4f61c27792a7fc8c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Model Definition\n",
    "# Define the Transformer model components."
   ],
   "id": "b52299a1da949474"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Encoder",
   "id": "b7e6e886756c261c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:14:07.382905Z",
     "start_time": "2025-04-06T15:14:07.365742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerEncoder(nnx.Module):\n",
    "    def __init__(self, embed_dim: int, dense_dim: int, num_heads: int, rngs: nnx.Rngs, **kwargs):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.attention = nnx.MultiHeadAttention(num_heads=num_heads,\n",
    "                                                in_features=embed_dim,\n",
    "                                                decode=False,\n",
    "                                                rngs=rngs)\n",
    "        self.dense_proj = nnx.Sequential(\n",
    "                nnx.Linear(embed_dim, dense_dim, rngs=rngs),\n",
    "                nnx.relu,\n",
    "                nnx.Linear(dense_dim, embed_dim, rngs=rngs),\n",
    "        )\n",
    "\n",
    "        self.layernorm_1 = nnx.LayerNorm(embed_dim, rngs=rngs)\n",
    "        self.layernorm_2 = nnx.LayerNorm(embed_dim, rngs=rngs)\n",
    "\n",
    "    def __call__(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = jnp.expand_dims(mask, axis=1).astype(jnp.int32)\n",
    "        else:\n",
    "            padding_mask = None\n",
    "\n",
    "        attention_output = self.attention(\n",
    "                inputs_q = inputs, inputs_k = inputs, inputs_v = inputs, mask=padding_mask, decode = False\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)"
   ],
   "id": "8ba5aeadfded145a",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Positional Embed",
   "id": "3fb69f772593ff6c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:14:34.558759Z",
     "start_time": "2025-04-06T15:14:34.554166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEmbedding(nnx.Module):\n",
    "    def __init__(self, sequence_length: int, vocab_size: int, embed_dim: int, rngs: nnx.Rngs, **kwargs):\n",
    "        self.token_embeddings = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, rngs=rngs)\n",
    "        self.position_embeddings = nnx.Embed(num_embeddings=sequence_length, features=embed_dim, rngs=rngs)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        length = inputs.shape[1]\n",
    "        positions = jnp.arange(0, length)[None, :]\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if mask is None:\n",
    "            return None\n",
    "        else:\n",
    "            return jnp.not_equal(inputs, 0)"
   ],
   "id": "6164a0979cc6406d",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Decoder",
   "id": "43ec7f5b49379475"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:15:06.430805Z",
     "start_time": "2025-04-06T15:15:06.422380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerDecoder(nnx.Module):\n",
    "    def __init__(self, embed_dim: int, latent_dim: int, num_heads: int, rngs: nnx.Rngs, **kwargs):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = nnx.MultiHeadAttention(num_heads=num_heads,\n",
    "                                                  in_features=embed_dim,\n",
    "                                                  decode=False,\n",
    "                                                  rngs=rngs)\n",
    "        self.attention_2 = nnx.MultiHeadAttention(num_heads=num_heads,\n",
    "                                                  in_features=embed_dim,\n",
    "                                                  decode=False,\n",
    "                                                  rngs=rngs)\n",
    "\n",
    "        self.dense_proj = nnx.Sequential(\n",
    "                nnx.Linear(embed_dim, latent_dim, rngs=rngs),\n",
    "                nnx.relu,\n",
    "                nnx.Linear(latent_dim, embed_dim, rngs=rngs),\n",
    "        )\n",
    "        self.layernorm_1 = nnx.LayerNorm(embed_dim, rngs=rngs)\n",
    "        self.layernorm_2 = nnx.LayerNorm(embed_dim, rngs=rngs)\n",
    "        self.layernorm_3 = nnx.LayerNorm(embed_dim, rngs=rngs)\n",
    "\n",
    "    def __call__(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs.shape[1])\n",
    "        if mask is not None:\n",
    "            padding_mask = jnp.expand_dims(mask, axis=1).astype(jnp.int32)\n",
    "            padding_mask = jnp.minimum(padding_mask, causal_mask)\n",
    "        else:\n",
    "            padding_mask = None\n",
    "        attention_output_1 = self.attention_1(\n",
    "                inputs_q=inputs, inputs_v=inputs, inputs_k=inputs,  mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2( ## https://github.com/google/flax/blob/main/flax/nnx/nn/attention.py#L403-L405\n",
    "                inputs_q=out_1,\n",
    "                inputs_v=encoder_outputs,\n",
    "                inputs_k=encoder_outputs,\n",
    "                mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, sequence_length):\n",
    "        i = jnp.arange(sequence_length)[:, None]\n",
    "        j = jnp.arange(sequence_length)\n",
    "        mask = (i >= j).astype(jnp.int32)\n",
    "        mask = jnp.reshape(mask, (1, 1, sequence_length, sequence_length))\n",
    "        return mask"
   ],
   "id": "27a24cacc385a1fc",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Transformer Model",
   "id": "19d11e69545dc57d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:15:38.008566Z",
     "start_time": "2025-04-06T15:15:38.004491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerModel(nnx.Module):\n",
    "    def __init__(self, sequence_length: int, vocab_size: int, embed_dim: int, latent_dim: int, num_heads: int, dropout_rate: float, rngs: nnx.Rngs):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.encoder = TransformerEncoder(embed_dim, latent_dim, num_heads, rngs=rngs)\n",
    "        self.positional_embedding = PositionalEmbedding(sequence_length, vocab_size, embed_dim, rngs=rngs)\n",
    "        self.decoder = TransformerDecoder(embed_dim, latent_dim, num_heads, rngs=rngs)\n",
    "        self.dropout = nnx.Dropout(rate=dropout_rate, rngs=rngs)\n",
    "        self.dense = nnx.Linear(embed_dim, vocab_size, rngs=rngs)\n",
    "\n",
    "    def __call__(self, encoder_inputs: jnp.array, decoder_inputs: jnp.array, mask: jnp.array = None, deterministic: bool = False):\n",
    "        x = self.positional_embedding(encoder_inputs)\n",
    "        encoder_outputs = self.encoder(x, mask=mask)\n",
    "\n",
    "        x = self.positional_embedding(decoder_inputs)\n",
    "        decoder_outputs = self.decoder(x, encoder_outputs, mask=mask)\n",
    "        # per nnx.Dropout - disable (deterministic=True) for eval, keep (False) for training\n",
    "        decoder_outputs = self.dropout(decoder_outputs, deterministic=deterministic)\n",
    "\n",
    "        logits = self.dense(decoder_outputs)\n",
    "        return logits"
   ],
   "id": "24d3db8de8ec87e6",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Data Loader\n",
    "# Create data loaders for training and validation."
   ],
   "id": "f5056b41b95990f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:14:38.301213Z",
     "start_time": "2025-04-09T16:14:38.295414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define batch size\n",
    "batch_size = 512"
   ],
   "id": "c25f90f73af22dd8",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:14:45.874011Z",
     "start_time": "2025-04-09T16:14:45.871014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Custom preprocessing class\n",
    "class CustomPreprocessing(grain.MapTransform):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def map(self, data):\n",
    "        return {\n",
    "                \"encoder_inputs\": np.array(data[\"encoder_inputs\"]),\n",
    "                \"decoder_inputs\": np.array(data[\"decoder_inputs\"]),\n",
    "                \"target_output\": np.array(data[\"target_output\"]),\n",
    "        }\n"
   ],
   "id": "d89388edf6de198",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:14:58.682197Z",
     "start_time": "2025-04-09T16:14:58.675556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create train sampler\n",
    "train_sampler = grain.IndexSampler(\n",
    "        len(train_data),\n",
    "        shuffle=True,\n",
    "        seed=12,                        # Seed for reproducibility\n",
    "        shard_options=grain.NoSharding(), # No sharding since it's a single-device setup\n",
    "        num_epochs=1,                    # Iterate over the dataset for one epoch\n",
    ")\n",
    "\n",
    "# Create validation sampler\n",
    "val_sampler = grain.IndexSampler(\n",
    "        len(val_data),\n",
    "        shuffle=False,\n",
    "        seed=12,\n",
    "        shard_options=grain.NoSharding(),\n",
    "        num_epochs=1,\n",
    ")\n",
    "\n",
    "# Create train data loader\n",
    "train_loader = grain.DataLoader(\n",
    "        data_source=train_data,\n",
    "        sampler=train_sampler,                 # Sampler to determine how to access the data\n",
    "        worker_count=4,                        # Number of child processes launched to parallelize the transformations\n",
    "        worker_buffer_size=2,                  # Count of output batches to produce in advance per worker\n",
    "        operations=[\n",
    "                CustomPreprocessing(),\n",
    "                grain.Batch(batch_size=batch_size, drop_remainder=True),\n",
    "        ]\n",
    ")\n",
    "\n",
    "# Create validation data loader\n",
    "val_loader = grain.DataLoader(\n",
    "        data_source=val_data,\n",
    "        sampler=val_sampler,\n",
    "        worker_count=4,\n",
    "        worker_buffer_size=2,\n",
    "        operations=[\n",
    "                CustomPreprocessing(),\n",
    "                grain.Batch(batch_size=batch_size),\n",
    "        ]\n",
    ")\n"
   ],
   "id": "62df163125c7019",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Training and Evaluation\n",
    "# Define the training and evaluation steps."
   ],
   "id": "144ba8abdc4599ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T16:15:24.116854Z",
     "start_time": "2025-04-09T16:15:24.113348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compute loss function\n",
    "def compute_loss(logits, labels):\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=labels)\n",
    "    return jnp.mean(loss)"
   ],
   "id": "46a7bad4e61c46aa",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:16:32.529891Z",
     "start_time": "2025-04-06T15:16:32.523790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training step\n",
    "@nnx.jit\n",
    "def train_step(model, optimizer, batch):\n",
    "    def loss_fn(model, train_encoder_input, train_decoder_input, train_target_input):\n",
    "        logits = model(train_encoder_input, train_decoder_input)\n",
    "        loss = compute_loss(logits, train_target_input)\n",
    "        return loss\n",
    "\n",
    "    grad_fn = nnx.value_and_grad(loss_fn)\n",
    "    loss, grads = grad_fn(model, jnp.array(batch[\"encoder_inputs\"]), jnp.array(batch[\"decoder_inputs\"]), jnp.array(batch[\"target_output\"]))\n",
    "    optimizer.update(grads)\n",
    "    return loss\n"
   ],
   "id": "226f2a8ec83292eb",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Evaluation step\n",
    "@nnx.jit\n",
    "def eval_step(model, batch, eval_metrics):\n",
    "    logits = model(jnp.array(batch[\"encoder_inputs\"]), jnp.array(batch[\"decoder_inputs\"]))\n",
    "    loss = compute_loss(logits, jnp.array(batch[\"target_output\"]))\n",
    "    labels = jnp.array(batch[\"target_output\"])\n",
    "\n",
    "    eval_metrics.update(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            labels=labels,\n",
    "    )"
   ],
   "id": "afec0615196efcc3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:16:39.272510Z",
     "start_time": "2025-04-06T15:16:38.866381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize evaluation metrics\n",
    "eval_metrics = nnx.MultiMetric(\n",
    "        loss=nnx.metrics.Average('loss'),\n",
    "        accuracy=nnx.metrics.Accuracy(),\n",
    ")\n",
    "\n",
    "# Initialize metric history\n",
    "train_metrics_history = {\n",
    "        \"train_loss\": [],\n",
    "}\n",
    "\n",
    "eval_metrics_history = {\n",
    "        \"test_loss\": [],\n",
    "        \"test_accuracy\": [],\n",
    "}"
   ],
   "id": "3c03afa10e442698",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:17:00.510964Z",
     "start_time": "2025-04-06T15:17:00.507512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Hyperparameters\n",
    "rng = nnx.Rngs(0)\n",
    "embed_dim = 256\n",
    "latent_dim = 2048\n",
    "num_heads = 8\n",
    "dropout_rate = 0.5\n",
    "vocab_size = tokenizer.n_vocab\n",
    "sequence_length = 512\n",
    "learning_rate = 1.5e-3\n",
    "num_epochs = 10"
   ],
   "id": "7ef71f6b40a8b95b",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:17:08.914057Z",
     "start_time": "2025-04-06T15:17:08.909768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training loop\n",
    "bar_format = \"{desc}[{n_fmt}/{total_fmt}]{postfix} [{elapsed}<{remaining}]\"\n",
    "train_total_steps = len(train_data) // batch_size\n",
    "\n",
    "def train_one_epoch(epoch):\n",
    "    model.train()  # Set the model to the training mode: e.g., update batch statistics\n",
    "    with tqdm.tqdm(\n",
    "            desc=f\"[train] epoch: {epoch}/{num_epochs}, \",\n",
    "            total=train_total_steps,\n",
    "            bar_format=bar_format,\n",
    "            leave=True,\n",
    "    ) as pbar:\n",
    "        for batch in train_loader:\n",
    "            loss = train_step(model, optimizer, batch)\n",
    "            train_metrics_history[\"train_loss\"].append(loss.item())\n",
    "            pbar.set_postfix({\"loss\": loss.item()})\n",
    "            pbar.update(1)\n",
    "\n",
    "\n",
    "def evaluate_model(epoch):\n",
    "    # Compute the metrics on the train and val sets after each training epoch.\n",
    "    model.eval()  # Set the model to an evaluation model: e.g., use stored batch statistics\n",
    "\n",
    "    eval_metrics.reset()  # Reset the eval metrics\n",
    "    for val_batch in val_loader:\n",
    "        eval_step(model, val_batch, eval_metrics)\n",
    "\n",
    "    for metric, value in eval_metrics.compute().items():\n",
    "        eval_metrics_history[f'test_{metric}'].append(value)\n",
    "\n",
    "    print(f\"[test] epoch: {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"- total loss: {eval_metrics_history['test_loss'][-1]:0.4f}\")\n",
    "    print(f\"- Accuracy: {eval_metrics_history['test_accuracy'][-1]:0.4f}\")"
   ],
   "id": "52709e5327da0556",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T15:17:26.887800Z",
     "start_time": "2025-04-06T15:17:24.923523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = TransformerModel(sequence_length, vocab_size, embed_dim, latent_dim, num_heads, dropout_rate, rngs=rng)\n",
    "optimizer = nnx.Optimizer(model, optax.adamw(learning_rate))"
   ],
   "id": "26a1386d3a59a9d3",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-04-06T15:18:50.510276Z",
     "start_time": "2025-04-06T15:18:22.055451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(epoch)\n",
    "    evaluate_model(epoch)"
   ],
   "id": "108773aa7c88e38c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[train] epoch: 0/10, [0/31] [00:00<?]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_metrics_history[\"train_loss\"], label=\"Loss value during the training\")\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ],
   "id": "20a5baad8a78fce9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 10))\n",
    "axs[0].set_title(\"Loss value on eval set\")\n",
    "axs[0].plot(eval_metrics_history[\"test_loss\"])\n",
    "axs[1].set_title(\"Accuracy on eval set\")\n",
    "axs[1].plot(eval_metrics_history[\"test_accuracy\"])"
   ],
   "id": "e39942d4677b2966"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e83f9865ab53f7c1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
