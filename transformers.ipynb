{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:38.608755Z",
     "start_time": "2025-02-20T18:38:38.172264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import functools  # Used for creating partial functions\n",
    "import tqdm.notebook  as tqdm # Used for displaying progress bars\n",
    "import matplotlib.pyplot as plt  # Used for plotting graphs"
   ],
   "id": "c80c621282d78e23",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:43.587076Z",
     "start_time": "2025-02-20T18:38:38.634447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf# We only import it for the tokenizer\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import jax.random as random\n",
    "import numpy as np"
   ],
   "id": "df05bc2dfcdf92ec",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:43.866144Z",
     "start_time": "2025-02-20T18:38:43.662246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from flax import linen as nn\n",
    "import optax\n",
    "from flax.training.train_state import TrainState\n",
    "import pandas as pd"
   ],
   "id": "7bb139fa2cdd0c43",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:43.917004Z",
     "start_time": "2025-02-20T18:38:43.914974Z"
    }
   },
   "cell_type": "code",
   "source": "jax.config.update(\"jax_enable_x64\", True)",
   "id": "b3629aceac59d3af",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:43.974753Z",
     "start_time": "2025-02-20T18:38:43.971539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict(params, inputs):\n",
    "    \"\"\" Implementation of the MLP architecture.\"\"\"\n",
    "    outputs = None\n",
    "    for w, b in params:\n",
    "        outputs = jnp.matmul(w, inputs.transpose()) + b\n",
    "        outputs = outputs.transpose()\n",
    "        inputs = jax.nn.relu(outputs)\n",
    "    return outputs"
   ],
   "id": "b4361f4d904ad0be",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:44.000001Z",
     "start_time": "2025-02-20T18:38:43.997698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def loss(params, batch):\n",
    "    \"\"\" Implementation of the square loss function.\"\"\"\n",
    "    inputs, targets = batch\n",
    "    predictions = predict(params, inputs)\n",
    "    return jnp.mean((predictions - targets) ** 2)"
   ],
   "id": "db3f236a6f2574dc",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:44.597995Z",
     "start_time": "2025-02-20T18:38:44.018130Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# prepare data\n",
    "# here: target (y_train) is a linear function of input (x_train) plus some noise\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "num_examples = 10_000\n",
    "dim = 100\n",
    "x_train = random.normal(key, (num_examples, dim))\n",
    "w = random.normal(key, (dim,))\n",
    "y_train =jnp.dot(x_train, w) + 0.2 * random.normal(key, (num_examples,))\n",
    "\n",
    "x_train = x_train.astype(jnp.float32)\n",
    "y_train = y_train.astype(jnp.float32)\n",
    "\n",
    "batch = (x_train, y_train)"
   ],
   "id": "ec56c69dd276c8cf",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:44.719676Z",
     "start_time": "2025-02-20T18:38:44.610870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# initialize model parameters\n",
    "W1 = jnp.identity(dim)  # identity matrix\n",
    "b1 = 0.\n",
    "\n",
    "W2 = random.normal(key, (dim,))\n",
    "b2 = 0.\n",
    "\n",
    "params = [(W1, b1), (W2, b2)]  # two layers"
   ],
   "id": "1dc2c1f455470891",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:44.963098Z",
     "start_time": "2025-02-20T18:38:44.726351Z"
    }
   },
   "cell_type": "code",
   "source": "loss(params, batch)",
   "id": "4238c32bad718340",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(48.71164087, dtype=float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:50.793338Z",
     "start_time": "2025-02-20T18:38:44.971522Z"
    }
   },
   "cell_type": "code",
   "source": "%timeit loss(params, batch)",
   "id": "b5567f1435a8e0f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.92 ms ± 1.42 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:50.828223Z",
     "start_time": "2025-02-20T18:38:50.825669Z"
    }
   },
   "cell_type": "code",
   "source": "# Text Tokenization",
   "id": "56aa0151074d3149",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:50.850346Z",
     "start_time": "2025-02-20T18:38:50.846403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sample_difference_dataset(\n",
    "        dataset_size: int,\n",
    "        lengths: list[int],\n",
    "        k: int,\n",
    "):\n",
    "    key = random.PRNGKey(0)  # Initialize the key\n",
    "    data_all = []\n",
    "    for length in lengths:\n",
    "        if length < 2:\n",
    "            raise ValueError(\"The length of the expression must be at least two.\")\n",
    "\n",
    "        # Convert JAX array to integer using item()\n",
    "        key, subkey = random.split(key)\n",
    "        length_n = random.randint(subkey, (dataset_size,), minval=1, maxval=length - 1)\n",
    "        length_m = length - 1 - length_n\n",
    "\n",
    "        # Generate random integers for each length with int64 dtype\n",
    "        integer_n = []\n",
    "        integer_m = []\n",
    "        for len_n, len_m in zip(length_n, length_m):\n",
    "            key, subkey_n = random.split(key)\n",
    "            maxval_n = k ** int(len_n) - 1\n",
    "            integer_n.append(random.randint(subkey_n, (), minval=1, maxval=maxval_n, dtype=jnp.int64))\n",
    "\n",
    "            key, subkey_m = random.split(key)\n",
    "            maxval_m = k ** int(len_m) - 1\n",
    "            integer_m.append(random.randint(subkey_m, (), minval=1, maxval=maxval_m, dtype=jnp.int64))\n",
    "\n",
    "        # Calculate differences and signs\n",
    "        diff_sign = [int(x > y) for x, y in zip(integer_n, integer_m)]\n",
    "\n",
    "        # Convert to strings and create expressions\n",
    "        integer_n_strs = [str(x.item())[::-1] for x in integer_n]\n",
    "        integer_m_strs = [str(x.item())[::-1] for x in integer_m]\n",
    "        expressions = [f\"{n}-{m}\" for n, m in zip(integer_n_strs, integer_m_strs)]\n",
    "\n",
    "        # Combine expressions with signs\n",
    "        data = list(zip(expressions, diff_sign))\n",
    "        data_all.extend(data)\n",
    "\n",
    "    # Remove duplicates and shuffle\n",
    "    data_all = list(set(data_all))\n",
    "    np.random.shuffle(data_all)\n",
    "    return data_all"
   ],
   "id": "9d82680a3c311b9e",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:54.443820Z",
     "start_time": "2025-02-20T18:38:50.859403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MAX_TRAIN_LENGTH = 10  # the maximum length allowed in the training split\n",
    "MAX_TEST_LENGTH = 15  # the maximum length allowed in the test split\n",
    "\n",
    "train_ds = sample_difference_dataset(\n",
    "        dataset_size=2500,\n",
    "        lengths=list(range(3, MAX_TRAIN_LENGTH + 1)),\n",
    "        k=10,\n",
    ")\n",
    "test_ds = sample_difference_dataset(\n",
    "        dataset_size=1000,\n",
    "        lengths=list(range(MAX_TRAIN_LENGTH + 1, MAX_TEST_LENGTH + 1)),\n",
    "        k=10,\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size {len(train_ds)}\")\n",
    "print(f\"Test dataset size {len(test_ds)}\")"
   ],
   "id": "7eef324828ddbfcf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size 15769\n",
      "Test dataset size 5000\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:54.468068Z",
     "start_time": "2025-02-20T18:38:54.465675Z"
    }
   },
   "cell_type": "code",
   "source": "it = iter(train_ds)",
   "id": "b39f5ee445b9535d",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:54.486247Z",
     "start_time": "2025-02-20T18:38:54.483202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for _ in range(5):\n",
    "    text, label = next(it)\n",
    "    print(\"text: \", text)\n",
    "    print(\"label: \", label)\n",
    "    print()"
   ],
   "id": "80822cdcc98cbe89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  57-995\n",
      "label:  0\n",
      "\n",
      "text:  816-09\n",
      "label:  1\n",
      "\n",
      "text:  0257-73\n",
      "label:  1\n",
      "\n",
      "text:  7-689\n",
      "label:  0\n",
      "\n",
      "text:  9978-9345\n",
      "label:  1\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:54.510380Z",
     "start_time": "2025-02-20T18:38:54.508308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# size of corpus to build the tokenizer\n",
    "corpus_size = 5_000  #@param = 'int'\n",
    "\n",
    "# size of the vocabulary\n",
    "vocab_size = 12  #@param = 'int'\n",
    "\n",
    "# maximum length of examples in tokens\n",
    "max_len = MAX_TEST_LENGTH + 1  #@param = 'int'\n",
    "\n",
    "# pad value\n",
    "pad_value = 0  #@param = 'int'\n"
   ],
   "id": "bb475ee87eaff451",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:54.528560Z",
     "start_time": "2025-02-20T18:38:54.525569Z"
    }
   },
   "cell_type": "code",
   "source": "corpus = [text for text, _ in train_ds[:corpus_size]]",
   "id": "bbf3ce569c342ec6",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:54.534603Z",
     "start_time": "2025-02-20T18:38:54.532196Z"
    }
   },
   "cell_type": "code",
   "source": "corpus[:10]",
   "id": "3d2f36c8c3f57b46",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['57-995',\n",
       " '816-09',\n",
       " '0257-73',\n",
       " '7-689',\n",
       " '9978-9345',\n",
       " '06-8',\n",
       " '8-90912',\n",
       " '5114457-07',\n",
       " '1530812-4',\n",
       " '933-6']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:54.591007Z",
     "start_time": "2025-02-20T18:38:54.556453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# now, we build the tokenizer\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=vocab_size,\n",
    "        oov_token=None,\n",
    "        char_level=True,\n",
    ")\n",
    "tokenizer.fit_on_texts(corpus)"
   ],
   "id": "b06baae3cbc1f2fe",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:54.601528Z",
     "start_time": "2025-02-20T18:38:54.598322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# note how the tokenizer figured out it was best to tokenize each digit separately\n",
    "tokenizer.index_word"
   ],
   "id": "490b8624a0adbf70",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '-',\n",
       " 2: '5',\n",
       " 3: '2',\n",
       " 4: '3',\n",
       " 5: '6',\n",
       " 6: '7',\n",
       " 7: '8',\n",
       " 8: '1',\n",
       " 9: '4',\n",
       " 10: '9',\n",
       " 11: '0'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:54.612329Z",
     "start_time": "2025-02-20T18:38:54.609735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example usage:\n",
    "print(\"original text: \", text)\n",
    "\n",
    "# tokenize text\n",
    "tokens = tokenizer.texts_to_sequences([text])\n",
    "print(\"tokens: \", tokens)\n",
    "print(\"number of tokens: \", len(tokens[0]))"
   ],
   "id": "ec8ece77b1b9f0ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text:  9978-9345\n",
      "tokens:  [[10, 10, 6, 7, 1, 10, 4, 9, 2]]\n",
      "number of tokens:  9\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:54.699378Z",
     "start_time": "2025-02-20T18:38:54.650035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# we can see the actual tokens by converting each token individually to text\n",
    "print(tokenizer.sequences_to_texts([token.tolist() for token in jnp.array(tokens).reshape((-1, 1))]))"
   ],
   "id": "8e48b9a050189c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['9', '9', '7', '8', '-', '9', '3', '4', '5']\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:54.732374Z",
     "start_time": "2025-02-20T18:38:54.728313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's examine the distribution of tokens in the corpus:\n",
    "print(\"Token frequency:\")\n",
    "dict(list(tokenizer.word_counts.items()))"
   ],
   "id": "f822b390bd6fc406",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token frequency:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'5': 3296,\n",
       " '7': 3235,\n",
       " '-': 5000,\n",
       " '9': 2812,\n",
       " '8': 3187,\n",
       " '1': 3169,\n",
       " '6': 3237,\n",
       " '0': 2079,\n",
       " '2': 3266,\n",
       " '3': 3243,\n",
       " '4': 3136}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:54.748764Z",
     "start_time": "2025-02-20T18:38:54.746727Z"
    }
   },
   "cell_type": "code",
   "source": "# preprocessing the data\n",
   "id": "a718e78940212a58",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:54.787313Z",
     "start_time": "2025-02-20T18:38:54.784431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_function(text, label):\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.texts_to_sequences([text])\n",
    "    # Pad the sequences\n",
    "    tokens = tf.keras.preprocessing.sequence.pad_sequences(tokens, maxlen=max_len, padding='post', value=pad_value)\n",
    "    # Convert to torch tensors of a type int64\n",
    "    tokens = jnp.array(tokens[0]).astype(jnp.int64)\n",
    "    label = jnp.array(label).astype(jnp.int64)\n",
    "    # Return the tokens and label\n",
    "    return tokens, label\n"
   ],
   "id": "116c4dc7606ee668",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:57.264652Z",
     "start_time": "2025-02-20T18:38:54.799023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply the preprocessing function to the training and test datasets\n",
    "print(\"preprocessing training examples ... \")\n",
    "x_train = []\n",
    "y_train = []\n",
    "for text, label in tqdm.tqdm(train_ds):\n",
    "    tokens, label = preprocess_function(text, label)\n",
    "    x_train.append(tokens)\n",
    "    y_train.append(label)\n",
    "\n",
    "print(\"preprocessing test examples ... \")\n",
    "x_test = []\n",
    "y_test = []\n",
    "for text, label in tqdm.tqdm(test_ds):\n",
    "    tokens, label = preprocess_function(text, label)\n",
    "    x_test.append(tokens)\n",
    "    y_test.append(label)"
   ],
   "id": "a7da718ad0c667d2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing training examples ... \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/15769 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5c83951bd6dc447b81ee4ac40ed0f617"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing test examples ... \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f915989640be43d5a651e9b5c38754ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:59.059905Z",
     "start_time": "2025-02-20T18:38:57.279500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_train = jnp.stack(x_train)\n",
    "y_train = jnp.array(y_train)\n",
    "x_test = jnp.stack(x_test)\n",
    "y_test = jnp.array(y_test)"
   ],
   "id": "6900a250f020dc41",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:59.094838Z",
     "start_time": "2025-02-20T18:38:59.091862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "print(\"x_train.shape: \", x_train.shape)\n",
    "print(\"y_train.shape: \", y_train.shape)\n",
    "print(\"x_test.shape: \", x_test.shape)\n",
    "print(\"y_test.shape: \", y_test.shape)\n"
   ],
   "id": "da3eb895efd97a98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape:  (15769, 16)\n",
      "y_train.shape:  (15769,)\n",
      "x_test.shape:  (5000, 16)\n",
      "y_test.shape:  (5000,)\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:59.180665Z",
     "start_time": "2025-02-20T18:38:59.113873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# let's see what it looks like\n",
    "x_train[0], y_train[0]"
   ],
   "id": "6a072f4adb74bbea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([ 2,  6,  1, 10, 10,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],      dtype=int64),\n",
       " Array(0, dtype=int64))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:59.193431Z",
     "start_time": "2025-02-20T18:38:59.189554Z"
    }
   },
   "cell_type": "code",
   "source": "max_len",
   "id": "99fb97d7b0f100bc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:59.226709Z",
     "start_time": "2025-02-20T18:38:59.224453Z"
    }
   },
   "cell_type": "code",
   "source": "# Transformer Architecture - Classification task",
   "id": "2ce8209c6102142e",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:59.247857Z",
     "start_time": "2025-02-20T18:38:59.241639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(Model, epochs=10, batch_size=32, lr=3e-4, wd=1e-5, **kwargs):\n",
    "    # Initialize the model\n",
    "    model = Model(**kwargs)\n",
    "    params_model = model.init(random.PRNGKey(0), x_train[:128])\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    optimizer = optax.adamw(learning_rate=lr, weight_decay=wd)\n",
    "    opt_state = optimizer.init(params_model)\n",
    "\n",
    "    # Create a TrainState\n",
    "    state = TrainState.create(apply_fn=model.apply, params=params_model, tx=optimizer)\n",
    "\n",
    "    # Define the loss function\n",
    "    def loss_fn(params, x, y):\n",
    "        logits = state.apply_fn(params, x)\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, y)\n",
    "        return loss.mean()\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "\n",
    "    @jax.jit\n",
    "    def train_step(state, x, y):\n",
    "        loss, grads = grad_fn(state.params, x, y)\n",
    "        new_state = state.apply_gradients(grads=grads)\n",
    "        return new_state, loss\n",
    "\n",
    "    # Report accuracy\n",
    "    def report(state, x, y):\n",
    "        logits = state.apply_fn(state.params, x)\n",
    "        predictions = (logits[:, 1] > 0).astype(jnp.int32)\n",
    "        return jnp.mean(predictions == y)\n",
    "\n",
    "    # Training loop\n",
    "    num_train_batches = len(x_train) // batch_size\n",
    "    num_test_batches = len(x_test) // batch_size\n",
    "\n",
    "    print(\"Training starts...\")\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        epoch_loss = 0\n",
    "        for i in range(0, len(x_train), batch_size):\n",
    "            x_batch = x_train[i:i + batch_size]\n",
    "            y_batch = y_train[i:i + batch_size]\n",
    "            state, loss = train_step(state, x_batch, y_batch)\n",
    "            epoch_loss += loss\n",
    "\n",
    "        # Evaluation\n",
    "        train_accuracy = 0\n",
    "        for i in range(0, len(x_train), batch_size):\n",
    "            x_batch = x_train[i:i + batch_size]\n",
    "            y_batch = y_train[i:i + batch_size]\n",
    "            train_accuracy += report(state, x_batch, y_batch)\n",
    "        train_accuracy /= num_train_batches\n",
    "\n",
    "        test_accuracy = 0\n",
    "        for i in range(0, len(x_test), batch_size):\n",
    "            x_batch = x_test[i:i + batch_size]\n",
    "            y_batch = y_test[i:i + batch_size]\n",
    "            test_accuracy += report(state, x_batch, y_batch)\n",
    "        test_accuracy /= num_test_batches\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(f\"Loss: {epoch_loss/num_train_batches:.4f}\")\n",
    "        print(f\"Train accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ],
   "id": "d52b96fd284c4da2",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:38:59.262407Z",
     "start_time": "2025-02-20T18:38:59.259086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the self-attention layer\n",
    "class SelfAttention(nn.Module):\n",
    "    embed_dim : int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Calculate query, key, and value matrices using linear layers\n",
    "        query = nn.Dense(self.embed_dim)(x)\n",
    "        key = nn.Dense(self.embed_dim)(x)\n",
    "        value = nn.Dense(self.embed_dim)(x)\n",
    "\n",
    "        # Calculate attention scores (scaled dot-product attention)\n",
    "        attention_scores = jnp.matmul(query, jnp.swapaxes(key, -2, -1)) / jnp.sqrt(self.embed_dim)\n",
    "        attention_weights = nn.softmax(attention_scores, axis=-1)\n",
    "\n",
    "        # Apply attention weights to values\n",
    "        output = jnp.matmul(attention_weights, value)\n",
    "        return output\n"
   ],
   "id": "2d55454a29359f4f",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:57:47.355988Z",
     "start_time": "2025-02-20T18:57:47.341832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the model\n",
    "class SimpleTransformer(nn.Module):\n",
    "    vocab_size: int\n",
    "    embed_dim: int =128\n",
    "    num_classes: int = 2\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Embedding layer\n",
    "        x = nn.Embed(self.vocab_size, self.embed_dim)(x)\n",
    "        # Self-attention layer\n",
    "        x = SelfAttention(self.embed_dim)(x)\n",
    "        # Extract the CLS token (the last token)\n",
    "        cls_token = x[:, -1, :]  # Assuming the last token is the CLS token\n",
    "\n",
    "        # Linear classifier\n",
    "        logits = nn.Dense(self.num_classes)(cls_token)\n",
    "        return logits"
   ],
   "id": "b22cf74ced23ddb1",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:58:28.816721Z",
     "start_time": "2025-02-20T18:57:47.794385Z"
    }
   },
   "cell_type": "code",
   "source": [
    "kwg = dict(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=64,\n",
    ")\n",
    "train(SimpleTransformer, **kwg)"
   ],
   "id": "9e3accf8e9ed9b58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts...\n",
      "Epoch 1/10\n",
      "Loss: 0.6948\n",
      "Train accuracy: 0.4961\n",
      "Test accuracy: 0.4958\n",
      "Epoch 2/10\n",
      "Loss: 0.6947\n",
      "Train accuracy: 0.4996\n",
      "Test accuracy: 0.5022\n",
      "Epoch 3/10\n",
      "Loss: 0.6946\n",
      "Train accuracy: 0.5029\n",
      "Test accuracy: 0.4940\n",
      "Epoch 4/10\n",
      "Loss: 0.6946\n",
      "Train accuracy: 0.5037\n",
      "Test accuracy: 0.4950\n",
      "Epoch 5/10\n",
      "Loss: 0.6946\n",
      "Train accuracy: 0.5048\n",
      "Test accuracy: 0.4970\n",
      "Epoch 6/10\n",
      "Loss: 0.6946\n",
      "Train accuracy: 0.5053\n",
      "Test accuracy: 0.4958\n",
      "Epoch 7/10\n",
      "Loss: 0.6946\n",
      "Train accuracy: 0.5059\n",
      "Test accuracy: 0.4960\n",
      "Epoch 8/10\n",
      "Loss: 0.6946\n",
      "Train accuracy: 0.5056\n",
      "Test accuracy: 0.4956\n",
      "Epoch 9/10\n",
      "Loss: 0.6945\n",
      "Train accuracy: 0.5053\n",
      "Test accuracy: 0.4956\n",
      "Epoch 10/10\n",
      "Loss: 0.6945\n",
      "Train accuracy: 0.5053\n",
      "Test accuracy: 0.4944\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:58:28.823264Z",
     "start_time": "2025-02-20T18:58:28.821144Z"
    }
   },
   "cell_type": "code",
   "source": "# MLP layers",
   "id": "c1981a46fcbd1c46",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:58:28.837221Z",
     "start_time": "2025-02-20T18:58:28.832526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    vocab_size: int\n",
    "    embed_dim: int = 128\n",
    "    mlp_dim: int = 256\n",
    "    num_classes: int = 2\n",
    "    max_seq_length: int = None\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Embedding layer\n",
    "        x = nn.Embed(self.vocab_size, self.embed_dim)(x)\n",
    "        # Self-attention layer\n",
    "        x = SelfAttention(self.embed_dim)(x)\n",
    "        # Extract the CLS token (the last token)\n",
    "        cls_token = x[:, -1, :]  # Assuming the last token is the CLS token\n",
    "\n",
    "        # MLP layers\n",
    "        x = nn.Dense(self.mlp_dim)(cls_token)\n",
    "        x = nn.relu(x)\n",
    "        logits = nn.Dense(self.num_classes)(x)\n",
    "        return logits"
   ],
   "id": "4d291971b846742b",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:59:09.858355Z",
     "start_time": "2025-02-20T18:58:28.845587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "kwg = dict(\n",
    "        embed_dim=64,\n",
    "        mlp_dim=64*4,\n",
    "        vocab_size=vocab_size,\n",
    ")\n",
    "train(SimpleTransformer, **kwg)"
   ],
   "id": "f14ccbcc07fd181",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts...\n",
      "Epoch 1/10\n",
      "Loss: 0.6949\n",
      "Train accuracy: 0.5099\n",
      "Test accuracy: 0.4916\n",
      "Epoch 2/10\n",
      "Loss: 0.6947\n",
      "Train accuracy: 0.5107\n",
      "Test accuracy: 0.4886\n",
      "Epoch 3/10\n",
      "Loss: 0.6946\n",
      "Train accuracy: 0.5071\n",
      "Test accuracy: 0.4988\n",
      "Epoch 4/10\n",
      "Loss: 0.6945\n",
      "Train accuracy: 0.5048\n",
      "Test accuracy: 0.4906\n",
      "Epoch 5/10\n",
      "Loss: 0.6944\n",
      "Train accuracy: 0.5097\n",
      "Test accuracy: 0.4990\n",
      "Epoch 6/10\n",
      "Loss: 0.6942\n",
      "Train accuracy: 0.5127\n",
      "Test accuracy: 0.4980\n",
      "Epoch 7/10\n",
      "Loss: 0.6940\n",
      "Train accuracy: 0.5154\n",
      "Test accuracy: 0.4938\n",
      "Epoch 8/10\n",
      "Loss: 0.6939\n",
      "Train accuracy: 0.5166\n",
      "Test accuracy: 0.4918\n",
      "Epoch 9/10\n",
      "Loss: 0.6938\n",
      "Train accuracy: 0.5185\n",
      "Test accuracy: 0.4930\n",
      "Epoch 10/10\n",
      "Loss: 0.6937\n",
      "Train accuracy: 0.5204\n",
      "Test accuracy: 0.4904\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T18:59:09.868668Z",
     "start_time": "2025-02-20T18:59:09.865012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    vocab_size: int\n",
    "    max_seq_length: int\n",
    "    embed_dim: int = 128\n",
    "    mlp_dim: int = 256\n",
    "    num_classes: int = 2\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Embedding layer\n",
    "        x = nn.Embed(self.vocab_size, self.embed_dim)(x)\n",
    "\n",
    "        # Positional embeddings\n",
    "        positions = jnp.arange(x.shape[1])\n",
    "        pos_embeddings = nn.Embed(self.max_seq_length, self.embed_dim)(positions)\n",
    "        x = x + pos_embeddings / jnp.sqrt(self.embed_dim) # Scale positional embeddings\n",
    "\n",
    "        # Self-attention layer\n",
    "        x = SelfAttention(self.embed_dim)(x)\n",
    "\n",
    "        # MLP layers\n",
    "        x = nn.Dense(self.mlp_dim)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.num_classes)(x)\n",
    "\n",
    "        # Extract the CLS token (the last token)\n",
    "        cls_token = x[:, -1, :]  # Assuming the last token is the CLS token\n",
    "\n",
    "        # Linear classifier\n",
    "        logits = nn.Dense(self.num_classes)(cls_token)\n",
    "        return logits\n",
    "\n"
   ],
   "id": "93bf770a4969a099",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T19:00:05.768385Z",
     "start_time": "2025-02-20T18:59:09.876304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "kwg = dict(\n",
    "        embed_dim=64,\n",
    "        mlp_dim=64*4,\n",
    "        vocab_size=vocab_size,\n",
    "        max_seq_length=max_len,\n",
    ")\n",
    "train(SimpleTransformer, **kwg)"
   ],
   "id": "9d6f9987cbd9a0b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts...\n",
      "Epoch 1/10\n",
      "Loss: 0.6948\n",
      "Train accuracy: 0.5123\n",
      "Test accuracy: 0.5032\n",
      "Epoch 2/10\n",
      "Loss: 0.6945\n",
      "Train accuracy: 0.5147\n",
      "Test accuracy: 0.5016\n",
      "Epoch 3/10\n",
      "Loss: 0.6584\n",
      "Train accuracy: 0.7629\n",
      "Test accuracy: 0.5831\n",
      "Epoch 4/10\n",
      "Loss: 0.3379\n",
      "Train accuracy: 0.9078\n",
      "Test accuracy: 0.5509\n",
      "Epoch 5/10\n",
      "Loss: 0.2022\n",
      "Train accuracy: 0.9385\n",
      "Test accuracy: 0.6747\n",
      "Epoch 6/10\n",
      "Loss: 0.1685\n",
      "Train accuracy: 0.9464\n",
      "Test accuracy: 0.7123\n",
      "Epoch 7/10\n",
      "Loss: 0.1526\n",
      "Train accuracy: 0.9484\n",
      "Test accuracy: 0.7264\n",
      "Epoch 8/10\n",
      "Loss: 0.1423\n",
      "Train accuracy: 0.9490\n",
      "Test accuracy: 0.7332\n",
      "Epoch 9/10\n",
      "Loss: 0.1349\n",
      "Train accuracy: 0.9514\n",
      "Test accuracy: 0.7364\n",
      "Epoch 10/10\n",
      "Loss: 0.1293\n",
      "Train accuracy: 0.9533\n",
      "Test accuracy: 0.7352\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  normalization layer\n",
   "id": "64d6add8d92003d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T19:00:05.789644Z",
     "start_time": "2025-02-20T19:00:05.784626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    vocab_size: int\n",
    "    max_seq_length: int\n",
    "    embed_dim: int = 128\n",
    "    mlp_dim: int = 256\n",
    "    num_classes: int = 2\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Embedding layer\n",
    "        x = nn.Embed(self.vocab_size, self.embed_dim)(x)\n",
    "\n",
    "        # Positional embeddings\n",
    "        positions = jnp.arange(x.shape[1])\n",
    "        pos_embeddings = nn.Embed(self.max_seq_length, self.embed_dim)(positions)\n",
    "        x = x + pos_embeddings / jnp.sqrt(self.embed_dim)  # Scale positional embeddings\n",
    "\n",
    "        # self-attention layer\n",
    "        x = SelfAttention(self.embed_dim)(x)\n",
    "\n",
    "        # We add layer norm\n",
    "        x = nn.LayerNorm(self.embed_dim)(x)\n",
    "\n",
    "        # MLP layers\n",
    "        x = nn.Dense(self.mlp_dim)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.num_classes)(x)\n",
    "\n",
    "        # We add layer norm\n",
    "        x = nn.LayerNorm(self.embed_dim)(x)\n",
    "\n",
    "        # Extract the CLS token (the last token)\n",
    "        cls_token = x[:, -1, :]  # Assuming the last token is the CLS token\n",
    "\n",
    "        # Linear classifier\n",
    "        logits = nn.Dense(self.num_classes)(cls_token)\n",
    "        return logits"
   ],
   "id": "1503241ab85b2f56",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T19:01:13.739678Z",
     "start_time": "2025-02-20T19:00:05.802416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "kwg = dict(\n",
    "        embed_dim=64,\n",
    "        mlp_dim=64*4,\n",
    "        vocab_size=vocab_size,\n",
    "        max_seq_length=max_len,\n",
    ")\n",
    "train(SimpleTransformer, **kwg)"
   ],
   "id": "7e04db106807b400",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts...\n",
      "Epoch 1/10\n",
      "Loss: 0.6945\n",
      "Train accuracy: 0.5078\n",
      "Test accuracy: 0.4958\n",
      "Epoch 2/10\n",
      "Loss: 0.6945\n",
      "Train accuracy: 0.5078\n",
      "Test accuracy: 0.4958\n",
      "Epoch 3/10\n",
      "Loss: 0.6945\n",
      "Train accuracy: 0.5078\n",
      "Test accuracy: 0.4958\n",
      "Epoch 4/10\n",
      "Loss: 0.6945\n",
      "Train accuracy: 0.5078\n",
      "Test accuracy: 0.4958\n",
      "Epoch 5/10\n",
      "Loss: 0.6945\n",
      "Train accuracy: 0.5078\n",
      "Test accuracy: 0.4958\n",
      "Epoch 6/10\n",
      "Loss: 0.5774\n",
      "Train accuracy: 0.9336\n",
      "Test accuracy: 0.5379\n",
      "Epoch 7/10\n",
      "Loss: 0.1889\n",
      "Train accuracy: 0.9624\n",
      "Test accuracy: 0.7075\n",
      "Epoch 8/10\n",
      "Loss: 0.1195\n",
      "Train accuracy: 0.9596\n",
      "Test accuracy: 0.7125\n",
      "Epoch 9/10\n",
      "Loss: 0.1017\n",
      "Train accuracy: 0.9577\n",
      "Test accuracy: 0.7151\n",
      "Epoch 10/10\n",
      "Loss: 0.0933\n",
      "Train accuracy: 0.9590\n",
      "Test accuracy: 0.7169\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T19:01:13.795585Z",
     "start_time": "2025-02-20T19:01:13.794Z"
    }
   },
   "cell_type": "code",
   "source": "## deeper architectures",
   "id": "a26d9457cf4a987a",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T19:01:13.864116Z",
     "start_time": "2025-02-20T19:01:13.860960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    embed_dim: int\n",
    "    mlp_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Self-attention layer\n",
    "        x = SelfAttention(self.embed_dim)(x)\n",
    "        # We add layer norm\n",
    "        x = nn.LayerNorm(self.embed_dim)(x)\n",
    "        # MLP layers\n",
    "        x = nn.Dense(self.mlp_dim)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.embed_dim)(x)\n",
    "        # We add layer norm\n",
    "        x = nn.LayerNorm(self.embed_dim)(x)\n",
    "        return x"
   ],
   "id": "f55c5f9d2d25bbd6",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T19:01:13.897831Z",
     "start_time": "2025-02-20T19:01:13.894331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    vocab_size: int\n",
    "    max_seq_length: int\n",
    "    num_layers: int\n",
    "    mlp_dim: int = 256\n",
    "    embed_dim: int = 128\n",
    "    num_classes: int = 2\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Embedding layer\n",
    "        x = nn.Embed(self.vocab_size, self.embed_dim)(x)\n",
    "\n",
    "        # Positional embeddings\n",
    "        positions = jnp.arange(x.shape[1])\n",
    "        pos_embeddings = nn.Embed(self.max_seq_length, self.embed_dim)(positions)\n",
    "        x = x + pos_embeddings / jnp.sqrt(self.embed_dim)  # Scale positional embeddings\n",
    "\n",
    "        # Stack multiple transformer encoder blocks\n",
    "        x = nn.Sequential([TransformerEncoderBlock(self.embed_dim, self.mlp_dim)\n",
    "                           for _ in range(self.num_layers)])(x)\n",
    "        # Extract the CLS token (the last token)\n",
    "        cls_token = x[:, -1, :]  # Assuming the last token is the CLS token\n",
    "\n",
    "        # Linear classifier\n",
    "        logits = nn.Dense(self.num_classes)(cls_token)\n",
    "        return logits"
   ],
   "id": "9a644a960cc096ce",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T19:04:17.830101Z",
     "start_time": "2025-02-20T19:01:13.911251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "kwg = dict(\n",
    "        embed_dim=64,\n",
    "        mlp_dim=64*4,\n",
    "        vocab_size=vocab_size,\n",
    "        max_seq_length=max_len,\n",
    "        num_layers=3,\n",
    ")\n",
    "train(SimpleTransformer, **kwg)"
   ],
   "id": "53840a0fb9264f87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts...\n",
      "Epoch 1/10\n",
      "Loss: 0.6946\n",
      "Train accuracy: 0.4942\n",
      "Test accuracy: 0.5106\n",
      "Epoch 2/10\n",
      "Loss: 0.6946\n",
      "Train accuracy: 0.4942\n",
      "Test accuracy: 0.5106\n",
      "Epoch 3/10\n",
      "Loss: 0.6946\n",
      "Train accuracy: 0.5078\n",
      "Test accuracy: 0.4958\n",
      "Epoch 4/10\n",
      "Loss: 0.6946\n",
      "Train accuracy: 0.5078\n",
      "Test accuracy: 0.4958\n",
      "Epoch 5/10\n",
      "Loss: 0.6945\n",
      "Train accuracy: 0.5078\n",
      "Test accuracy: 0.4958\n",
      "Epoch 6/10\n",
      "Loss: 0.6945\n",
      "Train accuracy: 0.5078\n",
      "Test accuracy: 0.4958\n",
      "Epoch 7/10\n",
      "Loss: 0.6945\n",
      "Train accuracy: 0.5078\n",
      "Test accuracy: 0.4958\n",
      "Epoch 8/10\n",
      "Loss: 0.6945\n",
      "Train accuracy: 0.5078\n",
      "Test accuracy: 0.4958\n",
      "Epoch 9/10\n",
      "Loss: 0.6945\n",
      "Train accuracy: 0.5078\n",
      "Test accuracy: 0.4958\n",
      "Epoch 10/10\n",
      "Loss: 0.6945\n",
      "Train accuracy: 0.5078\n",
      "Test accuracy: 0.4958\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T19:04:17.921895Z",
     "start_time": "2025-02-20T19:04:17.920263Z"
    }
   },
   "cell_type": "code",
   "source": "# skip connections",
   "id": "fcc16d7ecdf84c57",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T19:04:18.012437Z",
     "start_time": "2025-02-20T19:04:18.009572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    embed_dim: int\n",
    "    mlp_dim: int\n",
    "\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Self-attention layer\n",
    "        x = SelfAttention(self.embed_dim)(x)\n",
    "        x = nn.LayerNorm(self.embed_dim)(x)\n",
    "\n",
    "        # MLP layers\n",
    "        y = nn.Dense(self.mlp_dim)(x)\n",
    "        y = nn.relu(y)\n",
    "        y = nn.Dense(self.embed_dim)(y)\n",
    "\n",
    "        # We introduce a skip connection\n",
    "        x = x + y\n",
    "        x= nn.LayerNorm(self.embed_dim)(x)\n",
    "        return x"
   ],
   "id": "2b43f12f533e4093",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T19:04:18.052979Z",
     "start_time": "2025-02-20T19:04:18.049380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    vocab_size: int\n",
    "    max_seq_length: int\n",
    "    num_layers: int\n",
    "    mlp_dim: int = 256\n",
    "    embed_dim: int = 128\n",
    "    num_classes: int = 2\n",
    "\n",
    "    def setup(self):\n",
    "        # Create list of transformer blocks with unique parameters\n",
    "        self.transformer_blocks = [\n",
    "                TransformerEncoderBlock(self.embed_dim, self.mlp_dim)\n",
    "                for _ in range(self.num_layers)\n",
    "        ]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Embedding layer\n",
    "        x = nn.Embed(self.vocab_size, self.embed_dim)(x)\n",
    "\n",
    "        # Positional embeddings\n",
    "        positions = jnp.arange(x.shape[1])\n",
    "        pos_embeddings = nn.Embed(self.max_seq_length, self.embed_dim)(positions)\n",
    "        x = x + pos_embeddings / jnp.sqrt(self.embed_dim)\n",
    "\n",
    "        # Stack multiple transformer encoder blocks\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Extract the CLS token (the last token)\n",
    "        cls_token = x[:, -1, :]  # Assuming the last token is the CLS token\n",
    "\n",
    "        # Linear classifier\n",
    "        logits = nn.Dense(self.num_classes)(cls_token)\n",
    "        return logits"
   ],
   "id": "d7e67f8fbedb2a38",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T19:08:21.270642Z",
     "start_time": "2025-02-20T19:04:18.069134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "kwg = dict(\n",
    "        embed_dim=64,\n",
    "        mlp_dim=64*4,\n",
    "        vocab_size=vocab_size,\n",
    "        max_seq_length=max_len,\n",
    "        num_layers=3,\n",
    ")\n",
    "train(SimpleTransformer, **kwg)"
   ],
   "id": "5bbb5885ba3b6589",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts...\n",
      "Epoch 1/10\n",
      "Loss: 0.6946\n",
      "Train accuracy: 0.4942\n",
      "Test accuracy: 0.5106\n",
      "Epoch 2/10\n",
      "Loss: 0.6946\n",
      "Train accuracy: 0.4942\n",
      "Test accuracy: 0.5106\n",
      "Epoch 3/10\n",
      "Loss: 0.6946\n",
      "Train accuracy: 0.4942\n",
      "Test accuracy: 0.5106\n",
      "Epoch 4/10\n",
      "Loss: 0.6946\n",
      "Train accuracy: 0.4942\n",
      "Test accuracy: 0.5106\n",
      "Epoch 5/10\n",
      "Loss: 0.6946\n",
      "Train accuracy: 0.5078\n",
      "Test accuracy: 0.4958\n",
      "Epoch 6/10\n",
      "Loss: 0.6945\n",
      "Train accuracy: 0.5078\n",
      "Test accuracy: 0.4958\n",
      "Epoch 7/10\n",
      "Loss: 0.6945\n",
      "Train accuracy: 0.5078\n",
      "Test accuracy: 0.4958\n",
      "Epoch 8/10\n",
      "Loss: 0.6945\n",
      "Train accuracy: 0.5078\n",
      "Test accuracy: 0.4958\n",
      "Epoch 9/10\n",
      "Loss: 0.6945\n",
      "Train accuracy: 0.5078\n",
      "Test accuracy: 0.4958\n",
      "Epoch 10/10\n",
      "Loss: 0.6945\n",
      "Train accuracy: 0.5078\n",
      "Test accuracy: 0.4958\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T19:08:21.322127Z",
     "start_time": "2025-02-20T19:08:21.320769Z"
    }
   },
   "cell_type": "code",
   "source": "# Transformer Architecture - Sequence generation task",
   "id": "116f368bb14367d",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T19:08:21.385222Z",
     "start_time": "2025-02-20T19:08:21.380396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AdditionTask:\n",
    "    def sample_batch(self, batch_size: int, length: int):\n",
    "        \"\"\"Returns a batch of additions and their results.\"\"\"\n",
    "        if length <= 2:\n",
    "            raise ValueError(\"Length must be greater than 2.\")\n",
    "        # We only use `length - 1` tokens for the two values to account for the `+`.\n",
    "        # Generate random lengths for the two numbers in each addition problem.\n",
    "        length_n = random.randint(random.PRNGKey(0), (batch_size,), minval=1, maxval=length - 1)\n",
    "        length_m = length - 1 - length_n\n",
    "\n",
    "        # Generate random integers for each length with int64 dtype\n",
    "        integer_n = []\n",
    "        integer_m = []\n",
    "        key = random.PRNGKey(0)\n",
    "        for len_n, len_m in zip(length_n, length_m):\n",
    "            key, subkey_n = random.split(key)\n",
    "            maxval_n = 10 ** int(len_n) - 1\n",
    "            integer_n.append(random.randint(subkey_n, (), minval=1, maxval=maxval_n, dtype=jnp.int64))\n",
    "\n",
    "            key, subkey_m = random.split(key)\n",
    "            maxval_m = 10 ** int(len_m) - 1\n",
    "            integer_m.append(random.randint(subkey_m, (), minval=1, maxval=maxval_m, dtype=jnp.int64))\n",
    "\n",
    "        # Calculate the sum of the generated integers.\n",
    "        integer_sum = list(map(sum, zip(integer_n, integer_m)))\n",
    "\n",
    "        # Convert integers to reversed strings (e.g., 123 becomes \"321\").\n",
    "        knary_n = [str(x)[::-1] for x in integer_n]\n",
    "        knary_m = [str(x)[::-1] for x in integer_m]\n",
    "\n",
    "        # Create the addition expressions by concatenating the reversed strings with a \"+\".\n",
    "        expressions = [f\"{a}+{b}\" for a, b in zip(knary_n, knary_m)]\n",
    "\n",
    "        # Pad the expressions with zeros to reach the desired length.\n",
    "        expressions = [a + \"\".join([\"0\"] * (length - len(a))) for a in expressions]\n",
    "\n",
    "        # Convert the sums to strings.\n",
    "        results = list(map(str, integer_sum))\n",
    "        # Append \"#\" to the results and pad with zeros.\n",
    "        results = [\n",
    "                res + \"#\" + \"\".join([\"0\"] * (length - len(res))) for res in results\n",
    "        ]\n",
    "        return {\n",
    "                \"input\": expressions,\n",
    "                \"output\": results,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def input_size(self) -> int:\n",
    "        \"\"\"Returns the input size for the models.\"\"\"\n",
    "        return 12\n",
    "\n",
    "    @property\n",
    "    def output_size(self) -> int:\n",
    "        \"\"\"Returns the output size for the models.\"\"\"\n",
    "        return 12\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        \"\"\"Returns the output size for the models.\"\"\"\n",
    "        return 12\n",
    "\n",
    "    def output_length(self, input_length: int) -> int:\n",
    "        return input_length + 1"
   ],
   "id": "c4eaba5fe7bec470",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T19:08:21.568331Z",
     "start_time": "2025-02-20T19:08:21.417148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# # Instantiate an AdditionTask object. This object will handle data generation for our addition task.\n",
    "task = AdditionTask()\n",
    "\n",
    "MAX_TRAIN_LENGTH = 10\n",
    "MAX_TEST_LENGTH = 20\n",
    "\n",
    "# Generate a sample batch of addition problems.\n",
    "data = task.sample_batch(batch_size=16, length=MAX_TRAIN_LENGTH)"
   ],
   "id": "fdf5a91691fff7cc",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T19:08:21.623604Z",
     "start_time": "2025-02-20T19:08:21.603803Z"
    }
   },
   "cell_type": "code",
   "source": "pd.DataFrame(data)",
   "id": "5dec5948a3d545fa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         input       output\n",
       "0   2+00239732  23793202#00\n",
       "1   78749186+4  68194791#00\n",
       "2   976584+919  486598#0000\n",
       "3   436635+705  537141#0000\n",
       "4   5+72286476  67468232#00\n",
       "5   33516+6677  69299#00000\n",
       "6   974+906346  644088#0000\n",
       "7   6202483+22  3842048#000\n",
       "8   1783+11382  32182#00000\n",
       "9   0589909+98  9099939#000\n",
       "10  27+0265523  3255692#000\n",
       "11  14095008+2  80059043#00\n",
       "12  02232+7894  28207#00000\n",
       "13  47822+4202  24898#00000\n",
       "14  52204366+3  66340228#00\n",
       "15  089+172505  506251#0000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2+00239732</td>\n",
       "      <td>23793202#00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78749186+4</td>\n",
       "      <td>68194791#00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>976584+919</td>\n",
       "      <td>486598#0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>436635+705</td>\n",
       "      <td>537141#0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5+72286476</td>\n",
       "      <td>67468232#00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33516+6677</td>\n",
       "      <td>69299#00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>974+906346</td>\n",
       "      <td>644088#0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6202483+22</td>\n",
       "      <td>3842048#000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1783+11382</td>\n",
       "      <td>32182#00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0589909+98</td>\n",
       "      <td>9099939#000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>27+0265523</td>\n",
       "      <td>3255692#000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14095008+2</td>\n",
       "      <td>80059043#00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>02232+7894</td>\n",
       "      <td>28207#00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>47822+4202</td>\n",
       "      <td>24898#00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>52204366+3</td>\n",
       "      <td>66340228#00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>089+172505</td>\n",
       "      <td>506251#0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T19:08:22.010336Z",
     "start_time": "2025-02-20T19:08:22.003707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define and fit the tokenizer\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=None,\n",
    "        oov_token=None,\n",
    "        char_level=True,\n",
    ")\n",
    "tokenizer.fit_on_texts(data[\"input\"] + data[\"output\"])\n",
    "tokenizer.word_index"
   ],
   "id": "127762a2052657ae",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 1,\n",
       " '2': 2,\n",
       " '9': 3,\n",
       " '8': 4,\n",
       " '6': 5,\n",
       " '3': 6,\n",
       " '4': 7,\n",
       " '7': 8,\n",
       " '5': 9,\n",
       " '+': 10,\n",
       " '#': 11,\n",
       " '1': 12}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T20:32:15.506764Z",
     "start_time": "2025-02-20T20:32:15.494384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_data(batch, tokenizer):\n",
    "    \"\"\"Tokenizes and pads the input and output sequences for the model.\"\"\"\n",
    "    # Tokenize the input sequences using the provided tokenizer.\n",
    "    tokens_input = jnp.array(tokenizer.texts_to_sequences(batch[\"input\"]), dtype=jnp.float32) - 1  # Changed to int32\n",
    "    tokens_output = jnp.array(tokenizer.texts_to_sequences(batch[\"output\"]), dtype=jnp.float32) - 1  # Changed to int32\n",
    "\n",
    "    # Pad the sequences to the maximum length within the batch for consistent tensor shapes.\n",
    "    tokens_input = jnp.pad(\n",
    "            tokens_input,\n",
    "            pad_width=((0, 0), (0, tokens_input.shape[1] - tokens_input.shape[1])),\n",
    "            mode='constant',\n",
    "            constant_values=0\n",
    "    )\n",
    "    tokens_output = jnp.pad(\n",
    "            tokens_output,\n",
    "            pad_width=((0, 0), (0, tokens_output.shape[1] - tokens_output.shape[1])),\n",
    "            mode='constant',\n",
    "            constant_values=0\n",
    "    )\n",
    "    return dict(input=tokens_input, output=tokens_output)"
   ],
   "id": "e8e0e77f8f628f2d",
   "outputs": [],
   "execution_count": 211
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T20:32:15.836291Z",
     "start_time": "2025-02-20T20:32:15.653703Z"
    }
   },
   "cell_type": "code",
   "source": "preprocess_data(data, tokenizer)",
   "id": "e690e7788918ff51",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': Array([[ 1.,  9.,  0.,  0.,  1.,  5.,  2.,  7.,  5.,  1.],\n",
       "        [ 7.,  3.,  7.,  6.,  2., 11.,  3.,  4.,  9.,  6.],\n",
       "        [ 2.,  7.,  4.,  8.,  3.,  6.,  9.,  2., 11.,  2.],\n",
       "        [ 6.,  5.,  4.,  4.,  5.,  8.,  9.,  7.,  0.,  8.],\n",
       "        [ 8.,  9.,  7.,  1.,  1.,  3.,  4.,  6.,  7.,  4.],\n",
       "        [ 5.,  5.,  8., 11.,  4.,  9.,  4.,  4.,  7.,  7.],\n",
       "        [ 2.,  7.,  6.,  9.,  2.,  0.,  4.,  5.,  6.,  4.],\n",
       "        [ 4.,  1.,  0.,  1.,  6.,  3.,  5.,  9.,  1.,  1.],\n",
       "        [11.,  7.,  3.,  5.,  9., 11., 11.,  5.,  3.,  1.],\n",
       "        [ 0.,  8.,  3.,  2.,  2.,  0.,  2.,  9.,  2.,  3.],\n",
       "        [ 1.,  7.,  9.,  0.,  1.,  4.,  8.,  8.,  1.,  5.],\n",
       "        [11.,  6.,  0.,  2.,  8.,  0.,  0.,  3.,  9.,  1.],\n",
       "        [ 0.,  1.,  1.,  5.,  1.,  9.,  7.,  3.,  2.,  6.],\n",
       "        [ 6.,  7.,  3.,  1.,  1.,  9.,  6.,  1.,  0.,  1.],\n",
       "        [ 8.,  1.,  1.,  0.,  6.,  5.,  4.,  4.,  9.,  5.],\n",
       "        [ 0.,  3.,  2.,  9., 11.,  7.,  1.,  8.,  0.,  8.]], dtype=float32),\n",
       " 'output': Array([[ 1.,  5.,  7.,  2.,  5.,  1.,  0.,  1., 10.,  0.,  0.],\n",
       "        [ 4.,  3., 11.,  2.,  6.,  7.,  2., 11., 10.,  0.,  0.],\n",
       "        [ 6.,  3.,  4.,  8.,  2.,  3., 10.,  0.,  0.,  0.,  0.],\n",
       "        [ 8.,  5.,  7., 11.,  6., 11., 10.,  0.,  0.,  0.,  0.],\n",
       "        [ 4.,  7.,  6.,  4.,  3.,  1.,  5.,  1., 10.,  0.,  0.],\n",
       "        [ 4.,  2.,  1.,  2.,  2., 10.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 4.,  6.,  6.,  0.,  3.,  3., 10.,  0.,  0.,  0.,  0.],\n",
       "        [ 5.,  3.,  6.,  1.,  0.,  6.,  3., 10.,  0.,  0.,  0.],\n",
       "        [ 5.,  1., 11.,  3.,  1., 10.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 2.,  0.,  2.,  2.,  2.,  5.,  2., 10.,  0.,  0.,  0.],\n",
       "        [ 5.,  1.,  8.,  8.,  4.,  2.,  1., 10.,  0.,  0.,  0.],\n",
       "        [ 3.,  0.,  0.,  8.,  2.,  0.,  6.,  5., 10.,  0.,  0.],\n",
       "        [ 1.,  3.,  1.,  0.,  7., 10.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 1.,  6.,  3.,  2.,  3., 10.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 4.,  4.,  5.,  6.,  0.,  1.,  1.,  3., 10.,  0.,  0.],\n",
       "        [ 8.,  0.,  4.,  1.,  8., 11., 10.,  0.,  0.,  0.,  0.]],      dtype=float32)}"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 212
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T20:32:15.853460Z",
     "start_time": "2025-02-20T20:32:15.849934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _pointwise_loss_fn(output: jnp.array, target: jnp.array) -> jnp.array:\n",
    "    \"\"\"Calculates the pointwise cross-entropy loss between predicted probabilities and the true target values.\n",
    "\n",
    "  This function computes the loss for each token in the sequence individually.\n",
    "  \"\"\"\n",
    "    target = target.astype(jnp.int32)\n",
    "    target_one_hot = jax.nn.one_hot(target, num_classes=output.shape[-1]).astype(jnp.float32)\n",
    "    return target_one_hot * jax.nn.log_softmax(output, axis=-1)"
   ],
   "id": "5077b52200c37873",
   "outputs": [],
   "execution_count": 213
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T20:32:16.047065Z",
     "start_time": "2025-02-20T20:32:16.044277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def loss_fn(output, target):\n",
    "    return -jnp.mean(jnp.sum(_pointwise_loss_fn(output, target), axis=-1))"
   ],
   "id": "ccaeb268d19c0738",
   "outputs": [],
   "execution_count": 214
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T20:32:16.281815Z",
     "start_time": "2025-02-20T20:32:16.266588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "batch_size = 2\n",
    "sequence_length = 3\n",
    "num_classes = 4  # Let's say we have 4 possible tokens\n",
    "\n",
    "output = jnp.array([\n",
    "        [[1.0, 2.0, 3.0, 0.5], [0.1, 0.5, 1.5, 2.0], [2.5, 1.0, 0.2, 0.1]],  # Sequence 1\n",
    "        [[0.2, 0.3, 0.5, 0.1], [1.0, 1.5, 2.0, 0.5], [0.1, 0.2, 0.5, 2.5]],  # Sequence 2\n",
    "], dtype=jnp.float32)\n",
    "\n",
    "target = jnp.array(\n",
    "        [[0,  3,  2], [1, 0, 0],]\n",
    ")\n",
    "\n",
    "pointwise_loss = _pointwise_loss_fn(output, target)\n",
    "print(\"Pointwise Loss:\\n\", pointwise_loss)"
   ],
   "id": "bf18092a10e5ea07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pointwise Loss:\n",
      " [[[-2.4607735 -0.        -0.        -0.       ]\n",
      "  [-0.        -0.        -0.        -0.6827076]\n",
      "  [-0.        -0.        -2.6464982 -0.       ]]\n",
      "\n",
      " [[-0.        -1.3724415 -0.        -0.       ]\n",
      "  [-1.7873387 -0.        -0.        -0.       ]\n",
      "  [-2.6824024 -0.        -0.        -0.       ]]]\n"
     ]
    }
   ],
   "execution_count": 215
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T20:32:16.673979Z",
     "start_time": "2025-02-20T20:32:16.668744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _accuracy_fn(output: jnp.array, target: jnp.array) -> jnp.array:\n",
    "    return jnp.mean(jnp.argmax(output, axis=-1) == target).astype(jnp.float32)\n",
    "\n",
    "def accuracy_fn(output, target):\n",
    "    acc = _accuracy_fn(output, target)\n",
    "    return jnp.mean(acc)"
   ],
   "id": "25d146f504101fd2",
   "outputs": [],
   "execution_count": 216
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T20:32:16.847903Z",
     "start_time": "2025-02-20T20:32:16.844896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _apply_loss_and_metrics_fn(\n",
    "        state,\n",
    "        batch: dict[str, jnp.array],\n",
    "\n",
    "):\n",
    "    logits = state.apply_fn(batch[\"inputs\"], batch[\"output\"])\n",
    "    loss = loss_fn(logits, batch[\"output\"])\n",
    "    accuracy = accuracy_fn(logits, batch[\"output\"])\n",
    "    return loss, (accuracy)\n",
    "\n",
    "grad_fn = jax.value_and_grad(_apply_loss_and_metrics_fn, argnums=1, has_aux=True)"
   ],
   "id": "9a6b7668ec7715ff",
   "outputs": [],
   "execution_count": 217
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T20:32:16.977544Z",
     "start_time": "2025-02-20T20:32:16.975030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def _update_fn( batch, state):\n",
    "    (loss, (accuracy)), grads = grad_fn(state,batch)\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    return new_state, (loss, accuracy)"
   ],
   "id": "f1b4843fe82dbbe6",
   "outputs": [],
   "execution_count": 218
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T20:32:17.126112Z",
     "start_time": "2025-02-20T20:32:17.121019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_training(\n",
    "        *,\n",
    "        task,\n",
    "        model,\n",
    "        max_sequence_length: int,\n",
    "        train_steps: int = 10_000,\n",
    "        seed: int = 0,  # Used to sample during forward pass (e.g. from final logits).\n",
    "        model_init_seed: int = 0,  # Used to initialize model parameters.\n",
    "        log_frequency: int = 50,\n",
    "        batch_size: int = 128,\n",
    "        learning_rate: float = 1e-3,\n",
    "        max_grad_norm: float = 1.0,\n",
    "):\n",
    "\n",
    "    # Sample a batch to fit the tokenizer\n",
    "    dummy_batch = task.sample_batch(\n",
    "            length=max_sequence_length,\n",
    "            batch_size=256,\n",
    "    )\n",
    "\n",
    "    # Define and fit the tokenizer\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=None, oov_token=None, char_level=True,\n",
    "    )\n",
    "    tokenizer.fit_on_texts(dummy_batch[\"input\"] + dummy_batch[\"output\"])\n",
    "\n",
    "    print(f\"The tokenizer index is: {tokenizer.word_index}\")\n",
    "\n",
    "\n",
    "    # Initialize the model\n",
    "    choices = jnp.array(range(3 , max_sequence_length + 1))\n",
    "    length = random.choice(random.PRNGKey(model_init_seed) , choices).item()\n",
    "    batch_init = task.sample_batch(length=length, batch_size=batch_size)\n",
    "    batch_init = preprocess_data(batch_init, tokenizer)\n",
    "    params_model = model.init(random.PRNGKey(0), batch_init[\"input\"], batch_init[\"output\"])\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    optimizer = optax.adamw(learning_rate=learning_rate)\n",
    "    opt_state = optimizer.init(params_model)\n",
    "\n",
    "    # Create a TrainState\n",
    "    state = TrainState.create(apply_fn=model.apply, params=params_model, tx=optimizer)\n",
    "\n",
    "    results = []\n",
    "    for step in tqdm.tqdm(range(train_steps + 1)):\n",
    "        choices = jnp.array(range(3 , max_sequence_length + 1))\n",
    "        length = random.choice(random.PRNGKey(seed) , choices).item()\n",
    "\n",
    "        # Randomness handled by either torch, python.random or numpy.\n",
    "        train_batch = task.sample_batch(length=length, batch_size=batch_size)\n",
    "        train_batch = preprocess_data(train_batch, tokenizer)\n",
    "        # Update the parameters.\n",
    "        state, (train_loss, train_accuracy) = _update_fn(train_batch, state)\n",
    "\n",
    "        # Log the training metrics\n",
    "        if (log_frequency > 0) and (step % log_frequency == 0):\n",
    "            log_data = {\n",
    "                    \"step\": step,\n",
    "                    \"train_loss\": float(train_loss),\n",
    "                    \"train_accuracy\": float(train_accuracy),\n",
    "            }\n",
    "            print(log_data)\n",
    "            results.append(log_data)\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    return df_results, params, tokenizer\n"
   ],
   "id": "31f632e6c230a615",
   "outputs": [],
   "execution_count": 219
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T20:32:17.275046Z",
     "start_time": "2025-02-20T20:32:17.272394Z"
    }
   },
   "cell_type": "code",
   "source": "# Eval function",
   "id": "9fc915726869f2aa",
   "outputs": [],
   "execution_count": 220
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T20:32:17.498720Z",
     "start_time": "2025-02-20T20:32:17.494687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_evaluation(*, state, tokenizer, task,  max_test_length: int = 20,  # The largest sequence length to evaluate on\n",
    "                   total_batch_size: int = 512,\n",
    "                   sub_batch_size: int = 64,  # We use this to avoid memory overflow.\n",
    "                   seed: int = 1,\n",
    "                   is_autoregressive: bool = False,):\n",
    "    results = []\n",
    "    lengths = range(3, max_test_length + 1)\n",
    "\n",
    "    for length in tqdm.tqdm(lengths, desc=\"Lengths\"):\n",
    "        sub_accuracies = []\n",
    "        # Evaluate on multiple sub-batches to avoid memory overflow.\n",
    "        for _ in range(total_batch_size // sub_batch_size):\n",
    "            # Generate a batch of addition problems with the current length.\n",
    "            batch = task.sample_batch(sub_batch_size, length)\n",
    "            batch = preprocess_data(batch, tokenizer)\n",
    "\n",
    "            outputs = state.apply_fn(batch[\"input\"], batch[\"output\"])\n",
    "\n",
    "            # Calculate the accuracy for the current sub-batch.\n",
    "            sub_accuracies.append(accuracy_fn(outputs, batch[\"output\"]))\n",
    "\n",
    "        # Calculate the average accuracy for the current length.\n",
    "        log_data = {\n",
    "                \"length\": length,\n",
    "                \"accuracy\": np.mean(sub_accuracies),\n",
    "        }\n",
    "        print(log_data)\n",
    "        results.append(log_data)\n",
    "\n",
    "    # Return the results as a pandas DataFrame.\n",
    "    return pd.DataFrame(results)"
   ],
   "id": "830609da2f241afd",
   "outputs": [],
   "execution_count": 221
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T20:32:17.651586Z",
     "start_time": "2025-02-20T20:32:17.649327Z"
    }
   },
   "cell_type": "code",
   "source": "# Architecture - Encoder",
   "id": "78f4f71a129e610e",
   "outputs": [],
   "execution_count": 222
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T20:32:18.045586Z",
     "start_time": "2025-02-20T20:32:18.028666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BaseTransformerEncoder(nn.Module):\n",
    "    max_seq_length: int\n",
    "    num_layers: int\n",
    "    vocab_size: int\n",
    "    embed_dim: int = 128\n",
    "    mlp_dim: int = 256\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Embedding layer\n",
    "        x = nn.Embed(self.vocab_size, self.embed_dim)(x)\n",
    "\n",
    "        # Positional embeddings\n",
    "        positions = jnp.arange(x.shape[1])\n",
    "        pos_embeddings = nn.Embed(self.max_seq_length, self.embed_dim)(positions)\n",
    "        x = x + pos_embeddings / jnp.sqrt(self.embed_dim)\n",
    "\n",
    "        # Stack multiple transformer encoder blocks\n",
    "        for _ in range(self.num_layers):\n",
    "            x = TransformerEncoderBlock(self.embed_dim, self.mlp_dim)(x)\n",
    "\n",
    "        return x"
   ],
   "id": "4658eb1febc5bf5c",
   "outputs": [],
   "execution_count": 223
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T20:32:18.243667Z",
     "start_time": "2025-02-20T20:32:18.239990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Architecture - Cross attention\n",
    "class CrossAttention(nn.Module):\n",
    "    embed_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs_q, inputs_kv):\n",
    "        # Calculate query, key, and value matrices using linear layers\n",
    "        query = nn.Dense(self.embed_dim)(inputs_q)\n",
    "        key = nn.Dense(self.embed_dim)(inputs_kv)\n",
    "        value = nn.Dense(self.embed_dim)(inputs_kv)\n",
    "\n",
    "        print(f\"query : {inputs_q.shape}\")\n",
    "        print(f\"key : {inputs_kv.shape}\")\n",
    "\n",
    "        # Calculate attention scores (scaled dot-product attention)\n",
    "        attention_scores = jnp.matmul(query, jnp.transpose(key, (0, 2, 1))) / jnp.sqrt(self.embed_dim)\n",
    "        attention_weights = nn.softmax(attention_scores, axis=-1)\n",
    "\n",
    "        # Apply attention weights to values\n",
    "        output = jnp.matmul(attention_weights, value)\n",
    "        return output"
   ],
   "id": "834c2bbb02e4f731",
   "outputs": [],
   "execution_count": 224
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T20:32:18.426618Z",
     "start_time": "2025-02-20T20:32:18.423969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def shift_right(x: jnp.array, vocab_size: int):\n",
    "    x = x.astype(jnp.int32)\n",
    "    if x.ndim == 1:\n",
    "        x = x.unsqueeze(1)  # Ensure 2D shape (batch, seq)\n",
    "    x_shifted = x[:, :-1]  # Remove last token\n",
    "    # Pad sequence axis (axis=1) with 1 at start, 0 at end\n",
    "    return jnp.pad(x_shifted, ((0, 0), (1, 0)), mode='constant', constant_values=vocab_size - 1)"
   ],
   "id": "3730719342ba19a1",
   "outputs": [],
   "execution_count": 225
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T20:32:18.697655Z",
     "start_time": "2025-02-20T20:32:18.692280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Define the output size (vocabulary size).\n",
    "output_size = 4  # Tokens will be 0, 1, 2, 3\n",
    "\n",
    "# Create a one-hot encoded toy input sequence.\n",
    "# Represents the sequence [2, 3, 0]\n",
    "x = jnp.array([[\n",
    "        [0, 0, 1, 0],  # Token 2\n",
    "        [0, 0, 0, 1],  # Token 3\n",
    "        [1, 0, 0, 0]   # Token 0\n",
    "]], dtype=jnp.float32)\n",
    "print(f\"input : \\n{x}\\n\")\n",
    "\n",
    "# Compute the argmax\n",
    "x = jnp.argmax(x, axis=-1)\n",
    "print(f\"argmax : {x}\\n\")\n",
    "\n",
    "padded = jnp.pad(x[:, :-1], (1, 0), mode='constant', constant_values=output_size)\n",
    "print(f\"padded : \\n{padded}\\n\")"
   ],
   "id": "a4a6476d8b3495ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : \n",
      "[[[0. 0. 1. 0.]\n",
      "  [0. 0. 0. 1.]\n",
      "  [1. 0. 0. 0.]]]\n",
      "\n",
      "argmax : [[2 3 0]]\n",
      "\n",
      "padded : \n",
      "[[4 4 4]\n",
      " [4 2 3]]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 226
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T20:32:18.927806Z",
     "start_time": "2025-02-20T20:32:18.924671Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Decoder architecture\n",
    "class TransformerDecoderBlock(nn.Module):\n",
    "    embed_dim:int\n",
    "    mlp_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, enc_emb, dec_emb):\n",
    "        # Self-attention layer\n",
    "        x = SelfAttention(self.embed_dim)(dec_emb)\n",
    "        x = nn.LayerNorm(self.embed_dim)(x)\n",
    "\n",
    "        # MLP layers\n",
    "        y =CrossAttention(self.embed_dim)(x, enc_emb)\n",
    "        y = nn.LayerNorm(self.embed_dim)(y)\n",
    "\n",
    "        # MLP layers\n",
    "        y = nn.Dense(self.mlp_dim)(y)\n",
    "        y = nn.relu(y)\n",
    "        y = nn.Dense(self.embed_dim)(y)\n",
    "\n",
    "        # Layer normalization\n",
    "        y = nn.LayerNorm(self.embed_dim)(y)\n",
    "        return y"
   ],
   "id": "7b141e44d0f7f84",
   "outputs": [],
   "execution_count": 227
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T20:32:19.271260Z",
     "start_time": "2025-02-20T20:32:19.265881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BaseTransformerDecoder(nn.Module):\n",
    "    vocab_size: int\n",
    "    max_seq_length: int\n",
    "    num_layers: int\n",
    "    mlp_dim: int = 256\n",
    "    embed_dim: int = 128\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, encoded, targets):\n",
    "        x = shift_right(targets, self.vocab_size)\n",
    "\n",
    "        # Embedding layer\n",
    "        x = nn.Embed(self.vocab_size + 1, self.embed_dim)(x)\n",
    "\n",
    "        # Positional embeddings\n",
    "        positions = jnp.arange(x.shape[1])\n",
    "        pos_embeddings = nn.Embed(self.max_seq_length, self.embed_dim)(positions)\n",
    "        x = x + pos_embeddings / jnp.sqrt(self.embed_dim)\n",
    "        print(f\"x : {x.shape}\")\n",
    "        # Stack multiple transformer decoder blocks\n",
    "        for _ in range(self.num_layers):\n",
    "            x = TransformerDecoderBlock(self.embed_dim, self.mlp_dim)(encoded, x)\n",
    "        return x"
   ],
   "id": "20d62159d66f092b",
   "outputs": [],
   "execution_count": 228
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T20:32:19.618911Z",
     "start_time": "2025-02-20T20:32:19.615707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Architecture Encoder-Decoder\n",
    "\n",
    "class BaseTransformer(nn.Module):\n",
    "    num_layers: int\n",
    "    max_seq_length: int\n",
    "    vocab_size: int\n",
    "    embed_dim: int = 32\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs, targets):\n",
    "        encoded = BaseTransformerEncoder(\n",
    "                max_seq_length=self.max_seq_length,\n",
    "                num_layers=self.num_layers,\n",
    "                vocab_size=self.vocab_size,\n",
    "                embed_dim=self.embed_dim,\n",
    "        )(inputs)\n",
    "\n",
    "        decoded = BaseTransformerDecoder(\n",
    "                max_seq_length=self.max_seq_length,\n",
    "                num_layers=self.num_layers,\n",
    "                vocab_size=self.vocab_size,\n",
    "                embed_dim=self.embed_dim,\n",
    "        )(encoded, targets)\n",
    "\n",
    "        logits = nn.Dense(self.vocab_size)(decoded)\n",
    "        return logits\n"
   ],
   "id": "d18c7f42caa75abd",
   "outputs": [],
   "execution_count": 229
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T20:32:19.951358Z",
     "start_time": "2025-02-20T20:32:19.948988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the model\n",
    "model = BaseTransformer(\n",
    "        num_layers=1,\n",
    "        vocab_size=task.vocab_size,\n",
    "        max_seq_length=MAX_TEST_LENGTH + 2,\n",
    ")"
   ],
   "id": "848706224bb65fc7",
   "outputs": [],
   "execution_count": 230
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T20:32:20.618502Z",
     "start_time": "2025-02-20T20:32:20.305364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run the training loop\n",
    "df_train, params, tokenizer = run_training(\n",
    "        max_sequence_length=MAX_TRAIN_LENGTH,\n",
    "        task=task,\n",
    "        model=model,\n",
    "        batch_size=128,\n",
    "        train_steps=2_500\n",
    ")"
   ],
   "id": "ad7023e68583b0b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokenizer index is: {'0': 1, '3': 2, '9': 3, '5': 4, '7': 5, '4': 6, '1': 7, '6': 8, '8': 9, '2': 10, '+': 11, '#': 12}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input type must be an integer or unsigned integer.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[231], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Run the training loop\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m df_train, params, tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mrun_training\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_sequence_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mMAX_TRAIN_LENGTH\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2_500\u001B[39;49m\n\u001B[1;32m      8\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[219], line 35\u001B[0m, in \u001B[0;36mrun_training\u001B[0;34m(task, model, max_sequence_length, train_steps, seed, model_init_seed, log_frequency, batch_size, learning_rate, max_grad_norm)\u001B[0m\n\u001B[1;32m     33\u001B[0m batch_init \u001B[38;5;241m=\u001B[39m task\u001B[38;5;241m.\u001B[39msample_batch(length\u001B[38;5;241m=\u001B[39mlength, batch_size\u001B[38;5;241m=\u001B[39mbatch_size)\n\u001B[1;32m     34\u001B[0m batch_init \u001B[38;5;241m=\u001B[39m preprocess_data(batch_init, tokenizer)\n\u001B[0;32m---> 35\u001B[0m params_model \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mPRNGKey\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_init\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_init\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moutput\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# Initialize the optimizer\u001B[39;00m\n\u001B[1;32m     38\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optax\u001B[38;5;241m.\u001B[39madamw(learning_rate\u001B[38;5;241m=\u001B[39mlearning_rate)\n",
      "    \u001B[0;31m[... skipping hidden 9 frame]\u001B[0m\n",
      "Cell \u001B[0;32mIn[229], line 11\u001B[0m, in \u001B[0;36mBaseTransformer.__call__\u001B[0;34m(self, inputs, targets)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;129m@nn\u001B[39m\u001B[38;5;241m.\u001B[39mcompact\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, targets):\n\u001B[0;32m---> 11\u001B[0m     encoded \u001B[38;5;241m=\u001B[39m \u001B[43mBaseTransformerEncoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmax_seq_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_seq_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnum_layers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m            \u001B[49m\u001B[43mvocab_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocab_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m            \u001B[49m\u001B[43membed_dim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_dim\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m     decoded \u001B[38;5;241m=\u001B[39m BaseTransformerDecoder(\n\u001B[1;32m     19\u001B[0m             max_seq_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_seq_length,\n\u001B[1;32m     20\u001B[0m             num_layers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers,\n\u001B[1;32m     21\u001B[0m             vocab_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab_size,\n\u001B[1;32m     22\u001B[0m             embed_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_dim,\n\u001B[1;32m     23\u001B[0m     )(encoded, targets)\n\u001B[1;32m     25\u001B[0m     logits \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mDense(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab_size)(decoded)\n",
      "    \u001B[0;31m[... skipping hidden 2 frame]\u001B[0m\n",
      "Cell \u001B[0;32mIn[223], line 11\u001B[0m, in \u001B[0;36mBaseTransformerEncoder.__call__\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;129m@nn\u001B[39m\u001B[38;5;241m.\u001B[39mcompact\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;66;03m# Embedding layer\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEmbed\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvocab_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_dim\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;66;03m# Positional embeddings\u001B[39;00m\n\u001B[1;32m     14\u001B[0m     positions \u001B[38;5;241m=\u001B[39m jnp\u001B[38;5;241m.\u001B[39marange(x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m])\n",
      "    \u001B[0;31m[... skipping hidden 2 frame]\u001B[0m\n",
      "File \u001B[0;32m/opt/miniconda3/envs/Implementation/lib/python3.12/site-packages/flax/linen/linear.py:1120\u001B[0m, in \u001B[0;36mEmbed.__call__\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m   1109\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Embeds the inputs along the last dimension.\u001B[39;00m\n\u001B[1;32m   1110\u001B[0m \n\u001B[1;32m   1111\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1117\u001B[0m \u001B[38;5;124;03m  with an additional ``features`` dimension appended.\u001B[39;00m\n\u001B[1;32m   1118\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1119\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m jnp\u001B[38;5;241m.\u001B[39missubdtype(inputs\u001B[38;5;241m.\u001B[39mdtype, jnp\u001B[38;5;241m.\u001B[39minteger):\n\u001B[0;32m-> 1120\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInput type must be an integer or unsigned integer.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m   1121\u001B[0m \u001B[38;5;66;03m# Use take because fancy indexing numpy arrays with JAX indices does not\u001B[39;00m\n\u001B[1;32m   1122\u001B[0m \u001B[38;5;66;03m# work correctly.\u001B[39;00m\n\u001B[1;32m   1123\u001B[0m (embedding,) \u001B[38;5;241m=\u001B[39m promote_dtype(\n\u001B[1;32m   1124\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membedding, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdtype, inexact\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   1125\u001B[0m )\n",
      "\u001B[0;31mValueError\u001B[0m: Input type must be an integer or unsigned integer."
     ]
    }
   ],
   "execution_count": 231
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T20:32:20.649226Z",
     "start_time": "2025-02-20T20:30:37.478176Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e77869ea4dba1554",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T19:55:02.687851Z",
     "start_time": "2025-02-20T19:55:02.686351Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2948fa979ab4af0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-20T19:55:03.178043Z",
     "start_time": "2025-02-20T19:55:03.176609Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d51182c61498b46f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dc2206cf80472e9b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
