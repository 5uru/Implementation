{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "import functools  # Used for creating partial functions\n",
    "import tqdm.notebook  as tqdm # Used for displaying progress bars\n",
    "import matplotlib.pyplot as plt  # Used for plotting graphs"
   ],
   "id": "c80c621282d78e23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import tensorflow as tf# We only import it for the tokenizer\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import jax.random as random\n",
    "import numpy as np"
   ],
   "id": "df05bc2dfcdf92ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T20:26:00.234469Z",
     "start_time": "2025-02-19T20:26:00.227616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from flax import linen as nn\n",
    "import optax\n",
    "from flax.training.train_state import TrainState\n",
    "import pandas as pd"
   ],
   "id": "7bb139fa2cdd0c43",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "jax.config.update(\"jax_enable_x64\", True)",
   "id": "b3629aceac59d3af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def predict(params, inputs):\n",
    "    \"\"\" Implementation of the MLP architecture.\"\"\"\n",
    "    outputs = None\n",
    "    for w, b in params:\n",
    "        outputs = jnp.matmul(w, inputs.transpose()) + b\n",
    "        outputs = outputs.transpose()\n",
    "        inputs = jax.nn.relu(outputs)\n",
    "    return outputs"
   ],
   "id": "b4361f4d904ad0be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def loss(params, batch):\n",
    "    \"\"\" Implementation of the square loss function.\"\"\"\n",
    "    inputs, targets = batch\n",
    "    predictions = predict(params, inputs)\n",
    "    return jnp.mean((predictions - targets) ** 2)"
   ],
   "id": "db3f236a6f2574dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# prepare data\n",
    "# here: target (y_train) is a linear function of input (x_train) plus some noise\n",
    "\n",
    "key = random.PRNGKey(0)\n",
    "num_examples = 10_000\n",
    "dim = 100\n",
    "x_train = random.normal(key, (num_examples, dim))\n",
    "w = random.normal(key, (dim,))\n",
    "y_train =jnp.dot(x_train, w) + 0.2 * random.normal(key, (num_examples,))\n",
    "\n",
    "x_train = x_train.astype(jnp.float32)\n",
    "y_train = y_train.astype(jnp.float32)\n",
    "\n",
    "batch = (x_train, y_train)"
   ],
   "id": "ec56c69dd276c8cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# initialize model parameters\n",
    "W1 = jnp.identity(dim)  # identity matrix\n",
    "b1 = 0.\n",
    "\n",
    "W2 = random.normal(key, (dim,))\n",
    "b2 = 0.\n",
    "\n",
    "params = [(W1, b1), (W2, b2)]  # two layers"
   ],
   "id": "1dc2c1f455470891"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "loss(params, batch)",
   "id": "4238c32bad718340"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "%timeit loss(params, batch)",
   "id": "b5567f1435a8e0f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Text Tokenization",
   "id": "56aa0151074d3149"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def sample_difference_dataset(\n",
    "        dataset_size: int,\n",
    "        lengths: list[int],\n",
    "        k: int,\n",
    "):\n",
    "    key = random.PRNGKey(0)  # Initialize the key\n",
    "    data_all = []\n",
    "    for length in lengths:\n",
    "        if length < 2:\n",
    "            raise ValueError(\"The length of the expression must be at least two.\")\n",
    "\n",
    "        # Convert JAX array to integer using item()\n",
    "        key, subkey = random.split(key)\n",
    "        length_n = random.randint(subkey, (dataset_size,), minval=1, maxval=length - 1)\n",
    "        length_m = length - 1 - length_n\n",
    "\n",
    "        # Generate random integers for each length with int64 dtype\n",
    "        integer_n = []\n",
    "        integer_m = []\n",
    "        for len_n, len_m in zip(length_n, length_m):\n",
    "            key, subkey_n = random.split(key)\n",
    "            maxval_n = k ** int(len_n) - 1\n",
    "            integer_n.append(random.randint(subkey_n, (), minval=1, maxval=maxval_n, dtype=jnp.int64))\n",
    "\n",
    "            key, subkey_m = random.split(key)\n",
    "            maxval_m = k ** int(len_m) - 1\n",
    "            integer_m.append(random.randint(subkey_m, (), minval=1, maxval=maxval_m, dtype=jnp.int64))\n",
    "\n",
    "        # Calculate differences and signs\n",
    "        diff_sign = [int(x > y) for x, y in zip(integer_n, integer_m)]\n",
    "\n",
    "        # Convert to strings and create expressions\n",
    "        integer_n_strs = [str(x.item())[::-1] for x in integer_n]\n",
    "        integer_m_strs = [str(x.item())[::-1] for x in integer_m]\n",
    "        expressions = [f\"{n}-{m}\" for n, m in zip(integer_n_strs, integer_m_strs)]\n",
    "\n",
    "        # Combine expressions with signs\n",
    "        data = list(zip(expressions, diff_sign))\n",
    "        data_all.extend(data)\n",
    "\n",
    "    # Remove duplicates and shuffle\n",
    "    data_all = list(set(data_all))\n",
    "    np.random.shuffle(data_all)\n",
    "    return data_all"
   ],
   "id": "9d82680a3c311b9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "MAX_TRAIN_LENGTH = 10  # the maximum length allowed in the training split\n",
    "MAX_TEST_LENGTH = 15  # the maximum length allowed in the test split\n",
    "\n",
    "train_ds = sample_difference_dataset(\n",
    "        dataset_size=2500,\n",
    "        lengths=list(range(3, MAX_TRAIN_LENGTH + 1)),\n",
    "        k=10,\n",
    ")\n",
    "test_ds = sample_difference_dataset(\n",
    "        dataset_size=1000,\n",
    "        lengths=list(range(MAX_TRAIN_LENGTH + 1, MAX_TEST_LENGTH + 1)),\n",
    "        k=10,\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size {len(train_ds)}\")\n",
    "print(f\"Test dataset size {len(test_ds)}\")"
   ],
   "id": "7eef324828ddbfcf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "it = iter(train_ds)",
   "id": "b39f5ee445b9535d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for _ in range(5):\n",
    "    text, label = next(it)\n",
    "    print(\"text: \", text)\n",
    "    print(\"label: \", label)\n",
    "    print()"
   ],
   "id": "80822cdcc98cbe89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# size of corpus to build the tokenizer\n",
    "corpus_size = 5_000  #@param = 'int'\n",
    "\n",
    "# size of the vocabulary\n",
    "vocab_size = 12  #@param = 'int'\n",
    "\n",
    "# maximum length of examples in tokens\n",
    "max_len = MAX_TEST_LENGTH + 1  #@param = 'int'\n",
    "\n",
    "# pad value\n",
    "pad_value = 0  #@param = 'int'\n"
   ],
   "id": "bb475ee87eaff451"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "corpus = [text for text, _ in train_ds[:corpus_size]]",
   "id": "bbf3ce569c342ec6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "corpus[:10]",
   "id": "3d2f36c8c3f57b46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# now, we build the tokenizer\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=vocab_size,\n",
    "        oov_token=None,\n",
    "        char_level=True,\n",
    ")\n",
    "tokenizer.fit_on_texts(corpus)"
   ],
   "id": "b06baae3cbc1f2fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# note how the tokenizer figured out it was best to tokenize each digit separately\n",
    "tokenizer.index_word"
   ],
   "id": "490b8624a0adbf70"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Example usage:\n",
    "print(\"original text: \", text)\n",
    "\n",
    "# tokenize text\n",
    "tokens = tokenizer.texts_to_sequences([text])\n",
    "print(\"tokens: \", tokens)\n",
    "print(\"number of tokens: \", len(tokens[0]))"
   ],
   "id": "ec8ece77b1b9f0ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# we can see the actual tokens by converting each token individually to text\n",
    "print(tokenizer.sequences_to_texts([token.tolist() for token in jnp.array(tokens).reshape((-1, 1))]))"
   ],
   "id": "8e48b9a050189c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Let's examine the distribution of tokens in the corpus:\n",
    "print(\"Token frequency:\")\n",
    "dict(list(tokenizer.word_counts.items()))"
   ],
   "id": "f822b390bd6fc406"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# preprocessing the data\n",
   "id": "a718e78940212a58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def preprocess_function(text, label):\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.texts_to_sequences([text])\n",
    "    # Pad the sequences\n",
    "    tokens = tf.keras.preprocessing.sequence.pad_sequences(tokens, maxlen=max_len, padding='post', value=pad_value)\n",
    "    # Convert to torch tensors of a type int64\n",
    "    tokens = jnp.array(tokens[0]).astype(jnp.int64)\n",
    "    label = jnp.array(label).astype(jnp.int64)\n",
    "    # Return the tokens and label\n",
    "    return tokens, label\n"
   ],
   "id": "116c4dc7606ee668"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Apply the preprocessing function to the training and test datasets\n",
    "print(\"preprocessing training examples ... \")\n",
    "x_train = []\n",
    "y_train = []\n",
    "for text, label in tqdm.tqdm(train_ds):\n",
    "    tokens, label = preprocess_function(text, label)\n",
    "    x_train.append(tokens)\n",
    "    y_train.append(label)\n",
    "\n",
    "print(\"preprocessing test examples ... \")\n",
    "x_test = []\n",
    "y_test = []\n",
    "for text, label in tqdm.tqdm(test_ds):\n",
    "    tokens, label = preprocess_function(text, label)\n",
    "    x_test.append(tokens)\n",
    "    y_test.append(label)"
   ],
   "id": "a7da718ad0c667d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x_train = jnp.stack(x_train)\n",
    "y_train = jnp.array(y_train)\n",
    "x_test = jnp.stack(x_test)\n",
    "y_test = jnp.array(y_test)"
   ],
   "id": "6900a250f020dc41"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "print(\"x_train.shape: \", x_train.shape)\n",
    "print(\"y_train.shape: \", y_train.shape)\n",
    "print(\"x_test.shape: \", x_test.shape)\n",
    "print(\"y_test.shape: \", y_test.shape)\n"
   ],
   "id": "da3eb895efd97a98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# let's see what it looks like\n",
    "x_train[0], y_train[0]"
   ],
   "id": "6a072f4adb74bbea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "max_len",
   "id": "99fb97d7b0f100bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Transformer Architecture - Classification task",
   "id": "2ce8209c6102142e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train(Model, epochs=10, batch_size=32, lr=3e-4, wd=1e-5, **kwargs):\n",
    "    # Initialize the model\n",
    "    model = Model(**kwargs)\n",
    "    params_model = model.init(random.PRNGKey(0), x_train[:128])\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    optimizer = optax.adamw(learning_rate=lr, weight_decay=wd)\n",
    "    opt_state = optimizer.init(params_model)\n",
    "\n",
    "    # Create a TrainState\n",
    "    state = TrainState.create(apply_fn=model.apply, params=params_model, tx=optimizer)\n",
    "\n",
    "    # Define the loss function\n",
    "    def loss_fn(params, x, y):\n",
    "        logits = state.apply_fn(params, x)\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, y)\n",
    "        return loss.mean()\n",
    "\n",
    "    grad_fn = jax.value_and_grad(loss_fn)\n",
    "\n",
    "    @jax.jit\n",
    "    def train_step(state, x, y):\n",
    "        loss, grads = grad_fn(state.params, x, y)\n",
    "        new_state = state.apply_gradients(grads=grads)\n",
    "        return new_state, loss\n",
    "\n",
    "    # Report accuracy\n",
    "    def report(state, x, y):\n",
    "        logits = state.apply_fn(state.params, x)\n",
    "        predictions = (logits[:, 1] > 0).astype(jnp.int32)\n",
    "        return jnp.mean(predictions == y)\n",
    "\n",
    "    # Training loop\n",
    "    num_train_batches = len(x_train) // batch_size\n",
    "    num_test_batches = len(x_test) // batch_size\n",
    "\n",
    "    print(\"Training starts...\")\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        epoch_loss = 0\n",
    "        for i in range(0, len(x_train), batch_size):\n",
    "            x_batch = x_train[i:i + batch_size]\n",
    "            y_batch = y_train[i:i + batch_size]\n",
    "            state, loss = train_step(state, x_batch, y_batch)\n",
    "            epoch_loss += loss\n",
    "\n",
    "        # Evaluation\n",
    "        train_accuracy = 0\n",
    "        for i in range(0, len(x_train), batch_size):\n",
    "            x_batch = x_train[i:i + batch_size]\n",
    "            y_batch = y_train[i:i + batch_size]\n",
    "            train_accuracy += report(state, x_batch, y_batch)\n",
    "        train_accuracy /= num_train_batches\n",
    "\n",
    "        test_accuracy = 0\n",
    "        for i in range(0, len(x_test), batch_size):\n",
    "            x_batch = x_test[i:i + batch_size]\n",
    "            y_batch = y_test[i:i + batch_size]\n",
    "            test_accuracy += report(state, x_batch, y_batch)\n",
    "        test_accuracy /= num_test_batches\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(f\"Loss: {epoch_loss/num_train_batches:.4f}\")\n",
    "        print(f\"Train accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ],
   "id": "d52b96fd284c4da2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the self-attention layer\n",
    "class SelfAttention(nn.Module):\n",
    "    embed_dim : int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Calculate query, key, and value matrices using linear layers\n",
    "        query = nn.Dense(self.embed_dim)(x)\n",
    "        key = nn.Dense(self.embed_dim)(x)\n",
    "        value = nn.Dense(self.embed_dim)(x)\n",
    "\n",
    "        # Calculate attention scores (scaled dot-product attention)\n",
    "        attention_scores = jnp.matmul(query, jnp.swapaxes(key, -2, -1)) / jnp.sqrt(self.embed_dim)\n",
    "        attention_weights = nn.softmax(attention_scores, axis=-1)\n",
    "\n",
    "        # Apply attention weights to values\n",
    "        output = jnp.matmul(attention_weights, value)\n",
    "        return output\n"
   ],
   "id": "2d55454a29359f4f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the model\n",
    "class SimpleTransformer(nn.Module):\n",
    "    vocab_size: int\n",
    "    embed_dim: int =128\n",
    "    num_classes: int = 2\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Embedding layer\n",
    "        x = nn.Embed(self.vocab_size, self.embed_dim)(x)\n",
    "        # Self-attention layer\n",
    "        x = SelfAttention(self.embed_dim)(x)\n",
    "        # Extract the CLS token (the last token)\n",
    "        cls_token = x[:, -1, :]  # Assuming the last token is the CLS token\n",
    "\n",
    "        # Linear classifier\n",
    "        logits = nn.Dense(self.num_classes)(cls_token)\n",
    "        return logits"
   ],
   "id": "b22cf74ced23ddb1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "kwg = dict(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=64,\n",
    ")\n",
    "train(SimpleTransformer, **kwg)"
   ],
   "id": "9e3accf8e9ed9b58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# MLP layers",
   "id": "c1981a46fcbd1c46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    vocab_size: int\n",
    "    embed_dim: int = 128\n",
    "    mlp_dim: int = 256\n",
    "    num_classes: int = 2\n",
    "    max_seq_length: int = None\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Embedding layer\n",
    "        x = nn.Embed(self.vocab_size, self.embed_dim)(x)\n",
    "        # Self-attention layer\n",
    "        x = SelfAttention(self.embed_dim)(x)\n",
    "        # Extract the CLS token (the last token)\n",
    "        cls_token = x[:, -1, :]  # Assuming the last token is the CLS token\n",
    "\n",
    "        # MLP layers\n",
    "        x = nn.Dense(self.mlp_dim)(cls_token)\n",
    "        x = nn.relu(x)\n",
    "        logits = nn.Dense(self.num_classes)(x)\n",
    "        return logits"
   ],
   "id": "4d291971b846742b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "kwg = dict(\n",
    "        embed_dim=64,\n",
    "        mlp_dim=64*4,\n",
    "        vocab_size=vocab_size,\n",
    ")\n",
    "train(SimpleTransformer, **kwg)"
   ],
   "id": "f14ccbcc07fd181"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    vocab_size: int\n",
    "    max_seq_length: int\n",
    "    embed_dim: int = 128\n",
    "    mlp_dim: int = 256\n",
    "    num_classes: int = 2\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Embedding layer\n",
    "        x = nn.Embed(self.vocab_size, self.embed_dim)(x)\n",
    "\n",
    "        # Positional embeddings\n",
    "        positions = jnp.arange(x.shape[1])\n",
    "        pos_embeddings = nn.Embed(self.max_seq_length, self.embed_dim)(positions)\n",
    "        x = x + pos_embeddings / jnp.sqrt(self.embed_dim) # Scale positional embeddings\n",
    "\n",
    "        # Self-attention layer\n",
    "        x = SelfAttention(self.embed_dim)(x)\n",
    "\n",
    "        # MLP layers\n",
    "        x = nn.Dense(self.mlp_dim)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.num_classes)(x)\n",
    "\n",
    "        # Extract the CLS token (the last token)\n",
    "        cls_token = x[:, -1, :]  # Assuming the last token is the CLS token\n",
    "\n",
    "        # Linear classifier\n",
    "        logits = nn.Dense(self.num_classes)(cls_token)\n",
    "        return logits\n",
    "\n"
   ],
   "id": "93bf770a4969a099"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "kwg = dict(\n",
    "        embed_dim=64,\n",
    "        mlp_dim=64*4,\n",
    "        vocab_size=vocab_size,\n",
    "        max_seq_length=max_len,\n",
    ")\n",
    "train(SimpleTransformer, **kwg)"
   ],
   "id": "9d6f9987cbd9a0b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##  normalization layer\n",
   "id": "64d6add8d92003d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    vocab_size: int\n",
    "    max_seq_length: int\n",
    "    embed_dim: int = 128\n",
    "    mlp_dim: int = 256\n",
    "    num_classes: int = 2\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Embedding layer\n",
    "        x = nn.Embed(self.vocab_size, self.embed_dim)(x)\n",
    "\n",
    "        # Positional embeddings\n",
    "        positions = jnp.arange(x.shape[1])\n",
    "        pos_embeddings = nn.Embed(self.max_seq_length, self.embed_dim)(positions)\n",
    "        x = x + pos_embeddings / jnp.sqrt(self.embed_dim)  # Scale positional embeddings\n",
    "\n",
    "        # self-attention layer\n",
    "        x = SelfAttention(self.embed_dim)(x)\n",
    "\n",
    "        # We add layer norm\n",
    "        x = nn.LayerNorm(self.embed_dim)(x)\n",
    "\n",
    "        # MLP layers\n",
    "        x = nn.Dense(self.mlp_dim)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.num_classes)(x)\n",
    "\n",
    "        # We add layer norm\n",
    "        x = nn.LayerNorm(self.embed_dim)(x)\n",
    "\n",
    "        # Extract the CLS token (the last token)\n",
    "        cls_token = x[:, -1, :]  # Assuming the last token is the CLS token\n",
    "\n",
    "        # Linear classifier\n",
    "        logits = nn.Dense(self.num_classes)(cls_token)\n",
    "        return logits"
   ],
   "id": "1503241ab85b2f56"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "kwg = dict(\n",
    "        embed_dim=64,\n",
    "        mlp_dim=64*4,\n",
    "        vocab_size=vocab_size,\n",
    "        max_seq_length=max_len,\n",
    ")\n",
    "train(SimpleTransformer, **kwg)"
   ],
   "id": "7e04db106807b400"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## deeper architectures",
   "id": "a26d9457cf4a987a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    embed_dim: int\n",
    "    mlp_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Self-attention layer\n",
    "        x = SelfAttention(self.embed_dim)(x)\n",
    "        # We add layer norm\n",
    "        x = nn.LayerNorm(self.embed_dim)(x)\n",
    "        # MLP layers\n",
    "        x = nn.Dense(self.mlp_dim)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(self.embed_dim)(x)\n",
    "        # We add layer norm\n",
    "        x = nn.LayerNorm(self.embed_dim)(x)\n",
    "        return x"
   ],
   "id": "f55c5f9d2d25bbd6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    vocab_size: int\n",
    "    max_seq_length: int\n",
    "    num_layers: int\n",
    "    mlp_dim: int = 256\n",
    "    embed_dim: int = 128\n",
    "    num_classes: int = 2\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Embedding layer\n",
    "        x = nn.Embed(self.vocab_size, self.embed_dim)(x)\n",
    "\n",
    "        # Positional embeddings\n",
    "        positions = jnp.arange(x.shape[1])\n",
    "        pos_embeddings = nn.Embed(self.max_seq_length, self.embed_dim)(positions)\n",
    "        x = x + pos_embeddings / jnp.sqrt(self.embed_dim)  # Scale positional embeddings\n",
    "\n",
    "        # Stack multiple transformer encoder blocks\n",
    "        x = nn.Sequential([TransformerEncoderBlock(self.embed_dim, self.mlp_dim)\n",
    "                           for _ in range(self.num_layers)])(x)\n",
    "        # Extract the CLS token (the last token)\n",
    "        cls_token = x[:, -1, :]  # Assuming the last token is the CLS token\n",
    "\n",
    "        # Linear classifier\n",
    "        logits = nn.Dense(self.num_classes)(cls_token)\n",
    "        return logits"
   ],
   "id": "9a644a960cc096ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "kwg = dict(\n",
    "        embed_dim=64,\n",
    "        mlp_dim=64*4,\n",
    "        vocab_size=vocab_size,\n",
    "        max_seq_length=max_len,\n",
    "        num_layers=3,\n",
    ")\n",
    "train(SimpleTransformer, **kwg)"
   ],
   "id": "53840a0fb9264f87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# skip connections",
   "id": "fcc16d7ecdf84c57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    embed_dim: int\n",
    "    mlp_dim: int\n",
    "\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Self-attention layer\n",
    "        x = SelfAttention(self.embed_dim)(x)\n",
    "        x = nn.LayerNorm(self.embed_dim)(x)\n",
    "\n",
    "        # MLP layers\n",
    "        y = nn.Dense(self.mlp_dim)(x)\n",
    "        y = nn.relu(y)\n",
    "        y = nn.Dense(self.embed_dim)(y)\n",
    "\n",
    "        # We introduce a skip connection\n",
    "        x = x + y\n",
    "        x= nn.LayerNorm(self.embed_dim)(x)\n",
    "        return x"
   ],
   "id": "2b43f12f533e4093"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    vocab_size: int\n",
    "    max_seq_length: int\n",
    "    num_layers: int\n",
    "    mlp_dim: int = 256\n",
    "    embed_dim: int = 128\n",
    "    num_classes: int = 2\n",
    "\n",
    "    def setup(self):\n",
    "        # Create list of transformer blocks with unique parameters\n",
    "        self.transformer_blocks = [\n",
    "                TransformerEncoderBlock(self.embed_dim, self.mlp_dim)\n",
    "                for _ in range(self.num_layers)\n",
    "        ]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Embedding layer\n",
    "        x = nn.Embed(self.vocab_size, self.embed_dim)(x)\n",
    "\n",
    "        # Positional embeddings\n",
    "        positions = jnp.arange(x.shape[1])\n",
    "        pos_embeddings = nn.Embed(self.max_seq_length, self.embed_dim)(positions)\n",
    "        x = x + pos_embeddings / jnp.sqrt(self.embed_dim)\n",
    "\n",
    "        # Stack multiple transformer encoder blocks\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Extract the CLS token (the last token)\n",
    "        cls_token = x[:, -1, :]  # Assuming the last token is the CLS token\n",
    "\n",
    "        # Linear classifier\n",
    "        logits = nn.Dense(self.num_classes)(cls_token)\n",
    "        return logits"
   ],
   "id": "d7e67f8fbedb2a38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "kwg = dict(\n",
    "        embed_dim=64,\n",
    "        mlp_dim=64*4,\n",
    "        vocab_size=vocab_size,\n",
    "        max_seq_length=max_len,\n",
    "        num_layers=3,\n",
    ")\n",
    "train(SimpleTransformer, **kwg)"
   ],
   "id": "5bbb5885ba3b6589"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Transformer Architecture - Sequence generation task",
   "id": "116f368bb14367d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T20:25:07.489603Z",
     "start_time": "2025-02-19T20:25:07.483784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AdditionTask:\n",
    "    def sample_batch(self, batch_size: int, length: int):\n",
    "        \"\"\"Returns a batch of additions and their results.\"\"\"\n",
    "        if length <= 2:\n",
    "            raise ValueError(\"Length must be greater than 2.\")\n",
    "        # We only use `length - 1` tokens for the two values to account for the `+`.\n",
    "        # Generate random lengths for the two numbers in each addition problem.\n",
    "        length_n = random.randint(random.PRNGKey(0), (batch_size,), minval=1, maxval=length - 1)\n",
    "        length_m = length - 1 - length_n\n",
    "\n",
    "        # Generate random integers for each length with int64 dtype\n",
    "        integer_n = []\n",
    "        integer_m = []\n",
    "        key = random.PRNGKey(0)\n",
    "        for len_n, len_m in zip(length_n, length_m):\n",
    "            key, subkey_n = random.split(key)\n",
    "            maxval_n = 10 ** int(len_n) - 1\n",
    "            integer_n.append(random.randint(subkey_n, (), minval=1, maxval=maxval_n, dtype=jnp.int64))\n",
    "\n",
    "            key, subkey_m = random.split(key)\n",
    "            maxval_m = 10 ** int(len_m) - 1\n",
    "            integer_m.append(random.randint(subkey_m, (), minval=1, maxval=maxval_m, dtype=jnp.int64))\n",
    "\n",
    "        # Calculate the sum of the generated integers.\n",
    "        integer_sum = list(map(sum, zip(integer_n, integer_m)))\n",
    "\n",
    "        # Convert integers to reversed strings (e.g., 123 becomes \"321\").\n",
    "        knary_n = [str(x)[::-1] for x in integer_n]\n",
    "        knary_m = [str(x)[::-1] for x in integer_m]\n",
    "\n",
    "        # Create the addition expressions by concatenating the reversed strings with a \"+\".\n",
    "        expressions = [f\"{a}+{b}\" for a, b in zip(knary_n, knary_m)]\n",
    "\n",
    "        # Pad the expressions with zeros to reach the desired length.\n",
    "        expressions = [a + \"\".join([\"0\"] * (length - len(a))) for a in expressions]\n",
    "\n",
    "        # Convert the sums to strings.\n",
    "        results = list(map(str, integer_sum))\n",
    "        # Append \"#\" to the results and pad with zeros.\n",
    "        results = [\n",
    "                res + \"#\" + \"\".join([\"0\"] * (length - len(res))) for res in results\n",
    "        ]\n",
    "        return {\n",
    "                \"input\": expressions,\n",
    "                \"output\": results,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def input_size(self) -> int:\n",
    "        \"\"\"Returns the input size for the models.\"\"\"\n",
    "        return 12\n",
    "\n",
    "    @property\n",
    "    def output_size(self) -> int:\n",
    "        \"\"\"Returns the output size for the models.\"\"\"\n",
    "        return 12\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        \"\"\"Returns the output size for the models.\"\"\"\n",
    "        return 12\n",
    "\n",
    "    def output_length(self, input_length: int) -> int:\n",
    "        return input_length + 1"
   ],
   "id": "c4eaba5fe7bec470",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T20:25:08.008167Z",
     "start_time": "2025-02-19T20:25:07.978175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# # Instantiate an AdditionTask object. This object will handle data generation for our addition task.\n",
    "task = AdditionTask()\n",
    "\n",
    "MAX_TRAIN_LENGTH = 10\n",
    "MAX_TEST_LENGTH = 20\n",
    "\n",
    "# Generate a sample batch of addition problems.\n",
    "data = task.sample_batch(batch_size=16, length=MAX_TRAIN_LENGTH)"
   ],
   "id": "fdf5a91691fff7cc",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T20:26:08.946947Z",
     "start_time": "2025-02-19T20:26:08.902862Z"
    }
   },
   "cell_type": "code",
   "source": "pd.DataFrame(data)",
   "id": "5dec5948a3d545fa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         input       output\n",
       "0   2+00239732  23793202#00\n",
       "1   78749186+4  68194791#00\n",
       "2   976584+919  486598#0000\n",
       "3   436635+705  537141#0000\n",
       "4   5+72286476  67468232#00\n",
       "5   33516+6677  69299#00000\n",
       "6   974+906346  644088#0000\n",
       "7   6202483+22  3842048#000\n",
       "8   1783+11382  32182#00000\n",
       "9   0589909+98  9099939#000\n",
       "10  27+0265523  3255692#000\n",
       "11  14095008+2  80059043#00\n",
       "12  02232+7894  28207#00000\n",
       "13  47822+4202  24898#00000\n",
       "14  52204366+3  66340228#00\n",
       "15  089+172505  506251#0000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2+00239732</td>\n",
       "      <td>23793202#00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78749186+4</td>\n",
       "      <td>68194791#00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>976584+919</td>\n",
       "      <td>486598#0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>436635+705</td>\n",
       "      <td>537141#0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5+72286476</td>\n",
       "      <td>67468232#00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>33516+6677</td>\n",
       "      <td>69299#00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>974+906346</td>\n",
       "      <td>644088#0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6202483+22</td>\n",
       "      <td>3842048#000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1783+11382</td>\n",
       "      <td>32182#00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0589909+98</td>\n",
       "      <td>9099939#000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>27+0265523</td>\n",
       "      <td>3255692#000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14095008+2</td>\n",
       "      <td>80059043#00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>02232+7894</td>\n",
       "      <td>28207#00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>47822+4202</td>\n",
       "      <td>24898#00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>52204366+3</td>\n",
       "      <td>66340228#00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>089+172505</td>\n",
       "      <td>506251#0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T20:27:08.326623Z",
     "start_time": "2025-02-19T20:27:08.317958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define and fit the tokenizer\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=None,\n",
    "        oov_token=None,\n",
    "        char_level=True,\n",
    ")\n",
    "tokenizer.fit_on_texts(data[\"input\"] + data[\"output\"])\n",
    "tokenizer.word_index"
   ],
   "id": "127762a2052657ae",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 1,\n",
       " '2': 2,\n",
       " '9': 3,\n",
       " '8': 4,\n",
       " '6': 5,\n",
       " '3': 6,\n",
       " '4': 7,\n",
       " '7': 8,\n",
       " '5': 9,\n",
       " '+': 10,\n",
       " '#': 11,\n",
       " '1': 12}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T20:31:06.250681Z",
     "start_time": "2025-02-19T20:31:06.242182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_data(batch, tokenizer):\n",
    "    \"\"\"Tokenizes and pads the input and output sequences for the model.\n",
    "\n",
    "    Args:\n",
    "      batch: A dictionary containing the input and output sequences as lists of strings.\n",
    "      tokenizer: A fitted Tokenizer object (e.g., from torchtext.data.utils.get_tokenizer or a custom implementation).\n",
    "\n",
    "    Returns:\n",
    "      A dictionary containing the processed input and output sequences as PyTorch tensors,\n",
    "    \"\"\"\n",
    "    # Tokenize the input sequences using the provided tokenizer.\n",
    "    # This converts each string into a sequence of integer indices.\n",
    "    tokens_input = jnp.array(tokenizer.texts_to_sequences(batch[\"input\"]), dtype=jnp.int64) - 1\n",
    "\n",
    "    # Tokenize the output sequences.\n",
    "    tokens_output = jnp.array(tokenizer.texts_to_sequences(batch[\"output\"]), dtype=jnp.int64) - 1\n",
    "\n",
    "    # Pad the sequences to the maximum length within the batch for consistent tensor shapes.\n",
    "    tokens_input = jnp.pad(\n",
    "            tokens_input,\n",
    "            pad_width=((0, 0), (0, tokens_input.shape[1] - tokens_input.shape[1])),\n",
    "            mode='constant',\n",
    "            constant_values=0\n",
    "    )\n",
    "    tokens_output = jnp.pad(\n",
    "            tokens_output,\n",
    "            pad_width=((0, 0), (0, tokens_output.shape[1] - tokens_output.shape[1])),\n",
    "            mode='constant',\n",
    "            constant_values=0\n",
    "    )\n",
    "        # Return the processed data as a dictionary.\n",
    "    return dict(input=tokens_input, output=tokens_output)"
   ],
   "id": "e8e0e77f8f628f2d",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T20:31:18.497031Z",
     "start_time": "2025-02-19T20:31:18.367495Z"
    }
   },
   "cell_type": "code",
   "source": "preprocess_data(data, tokenizer)",
   "id": "e690e7788918ff51",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': Array([[ 1,  9,  0,  0,  1,  5,  2,  7,  5,  1],\n",
       "        [ 7,  3,  7,  6,  2, 11,  3,  4,  9,  6],\n",
       "        [ 2,  7,  4,  8,  3,  6,  9,  2, 11,  2],\n",
       "        [ 6,  5,  4,  4,  5,  8,  9,  7,  0,  8],\n",
       "        [ 8,  9,  7,  1,  1,  3,  4,  6,  7,  4],\n",
       "        [ 5,  5,  8, 11,  4,  9,  4,  4,  7,  7],\n",
       "        [ 2,  7,  6,  9,  2,  0,  4,  5,  6,  4],\n",
       "        [ 4,  1,  0,  1,  6,  3,  5,  9,  1,  1],\n",
       "        [11,  7,  3,  5,  9, 11, 11,  5,  3,  1],\n",
       "        [ 0,  8,  3,  2,  2,  0,  2,  9,  2,  3],\n",
       "        [ 1,  7,  9,  0,  1,  4,  8,  8,  1,  5],\n",
       "        [11,  6,  0,  2,  8,  0,  0,  3,  9,  1],\n",
       "        [ 0,  1,  1,  5,  1,  9,  7,  3,  2,  6],\n",
       "        [ 6,  7,  3,  1,  1,  9,  6,  1,  0,  1],\n",
       "        [ 8,  1,  1,  0,  6,  5,  4,  4,  9,  5],\n",
       "        [ 0,  3,  2,  9, 11,  7,  1,  8,  0,  8]], dtype=int64),\n",
       " 'output': Array([[ 1,  5,  7,  2,  5,  1,  0,  1, 10,  0,  0],\n",
       "        [ 4,  3, 11,  2,  6,  7,  2, 11, 10,  0,  0],\n",
       "        [ 6,  3,  4,  8,  2,  3, 10,  0,  0,  0,  0],\n",
       "        [ 8,  5,  7, 11,  6, 11, 10,  0,  0,  0,  0],\n",
       "        [ 4,  7,  6,  4,  3,  1,  5,  1, 10,  0,  0],\n",
       "        [ 4,  2,  1,  2,  2, 10,  0,  0,  0,  0,  0],\n",
       "        [ 4,  6,  6,  0,  3,  3, 10,  0,  0,  0,  0],\n",
       "        [ 5,  3,  6,  1,  0,  6,  3, 10,  0,  0,  0],\n",
       "        [ 5,  1, 11,  3,  1, 10,  0,  0,  0,  0,  0],\n",
       "        [ 2,  0,  2,  2,  2,  5,  2, 10,  0,  0,  0],\n",
       "        [ 5,  1,  8,  8,  4,  2,  1, 10,  0,  0,  0],\n",
       "        [ 3,  0,  0,  8,  2,  0,  6,  5, 10,  0,  0],\n",
       "        [ 1,  3,  1,  0,  7, 10,  0,  0,  0,  0,  0],\n",
       "        [ 1,  6,  3,  2,  3, 10,  0,  0,  0,  0,  0],\n",
       "        [ 4,  4,  5,  6,  0,  1,  1,  3, 10,  0,  0],\n",
       "        [ 8,  0,  4,  1,  8, 11, 10,  0,  0,  0,  0]], dtype=int64)}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5077b52200c37873"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
