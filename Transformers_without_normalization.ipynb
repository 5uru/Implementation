{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-26T09:43:11.498643Z",
     "start_time": "2025-03-26T09:43:11.492829Z"
    }
   },
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import math\n",
    "from flax import linen as nn\n",
    "from flax.linen.initializers import constant, ones, zeros\n",
    "from flax.training.train_state import TrainState\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import optax\n",
    "from tqdm.notebook import tqdm\n",
    "import plotly.express as px\n",
    "import time\n",
    "from typing import Any, Sequence, Tuple, Optional, Dict"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Word Embeddings",
   "id": "c07879b0c3a124dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T08:53:55.427626Z",
     "start_time": "2025-03-26T08:53:55.422375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Embedding(nn.Module):\n",
    "    vocab_size: int\n",
    "    emb_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.embed = nn.Embed(self.vocab_size, self.emb_dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.embed(x)"
   ],
   "id": "43ba905deb10d935",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Positional Encoding\n",
    "\n"
   ],
   "id": "b5eb874128529ab0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T08:54:56.904467Z",
     "start_time": "2025-03-26T08:54:56.891780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    max_seq_len: int\n",
    "    embed_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
    "        pe = np.zeros((self.max_seq_len, self.embed_dim))\n",
    "        position = np.arange(0, self.max_seq_len, dtype=np.float32)[:,None]\n",
    "        div_term = np.exp(np.arange(0, self.embed_dim, 2) * (-math.log(10000.0) / self.embed_dim))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "        pe = pe[None]\n",
    "        self.pe = jax.device_put(pe)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x + self.pe[:, :x.shape[1]]\n",
    "        return x"
   ],
   "id": "ed1ed8d7fe98b266",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Self Attention",
   "id": "4d3172f259525de4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:08:00.158549Z",
     "start_time": "2025-03-26T11:08:00.145375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    embed_dim: int = 512\n",
    "    num_heads: int = 8\n",
    "\n",
    "    def setup(self):\n",
    "        assert self.embed_dim % self.num_heads == 0\n",
    "        self.single_head_dim = self.embed_dim // self.num_heads\n",
    "\n",
    "        self.query = nn.Dense(self.embed_dim, use_bias=False)\n",
    "        self.key = nn.Dense(self.embed_dim, use_bias=False)\n",
    "        self.value = nn.Dense(self.embed_dim, use_bias=False)\n",
    "\n",
    "        self.out = nn.Dense(self.embed_dim)\n",
    "\n",
    "    def __call__(self, key, query, value, mask=None):\n",
    "        batch_size = key.shape[0]\n",
    "        seq_len_k = key.shape[1]\n",
    "        seq_len_q = query.shape[1]\n",
    "\n",
    "        # Linear projections\n",
    "        q = self.query(query)  # (batch_size, seq_len_q, embed_dim)\n",
    "        k = self.key(key)      # (batch_size, seq_len_k, embed_dim)\n",
    "        v = self.value(value)  # (batch_size, seq_len_k, embed_dim)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        q = q.reshape(batch_size, seq_len_q, self.num_heads, self.single_head_dim)\n",
    "        k = k.reshape(batch_size, seq_len_k, self.num_heads, self.single_head_dim)\n",
    "        v = v.reshape(batch_size, seq_len_k, self.num_heads, self.single_head_dim)\n",
    "\n",
    "        # Transpose to (batch_size, num_heads, seq_len, single_head_dim)\n",
    "        q = q.transpose(0, 2, 1, 3)\n",
    "        k = k.transpose(0, 2, 1, 3)\n",
    "        v = v.transpose(0, 2, 1, 3)\n",
    "\n",
    "        # Calculate attention scores\n",
    "        # (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        attention = jnp.matmul(q, k.transpose(0, 1, 3, 2)) / jnp.sqrt(self.single_head_dim)\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            attention = jnp.where(mask == 0, -1e9, attention)\n",
    "\n",
    "        # Apply softmax\n",
    "        attention_weights = nn.softmax(attention, axis=-1)\n",
    "\n",
    "        # Calculate output\n",
    "        # (batch_size, num_heads, seq_len_q, single_head_dim)\n",
    "        out = jnp.matmul(attention_weights, v)\n",
    "\n",
    "        # Transpose and reshape to (batch_size, seq_len_q, embed_dim)\n",
    "        out = out.transpose(0, 2, 1, 3).reshape(batch_size, seq_len_q, self.embed_dim)\n",
    "\n",
    "        # Final linear projection\n",
    "        out = self.out(out)\n",
    "\n",
    "        return out"
   ],
   "id": "92187cbad03f44cb",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "DyT",
   "id": "66e02a11cd5de67c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:08:00.483895Z",
     "start_time": "2025-03-26T11:08:00.481129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DyT(nn.Module):\n",
    "    num_features: int\n",
    "    alpha_init: float = 0.5\n",
    "\n",
    "    def setup(self):\n",
    "        # Fixed initializer with shape parameter\n",
    "        self.alpha = self.param('alpha', constant(self.alpha_init), ())  # Empty tuple for scalar\n",
    "        self.weight = self.param('weight', ones, (self.num_features,))\n",
    "        self.bias = self.param('bias', zeros, (self.num_features,))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = nn.tanh(self.alpha * x)\n",
    "        return x * self.weight + self.bias"
   ],
   "id": "dbdd1b0edeb250a2",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Encoder",
   "id": "7171549525bd9d46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:08:00.878634Z",
     "start_time": "2025-03-26T11:08:00.873035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    embed_dim: int\n",
    "    expansion_factor: int = 4\n",
    "    n_heads: int = 8\n",
    "\n",
    "    def setup(self):\n",
    "        self.attention = MultiHeadAttention(self.embed_dim, self.n_heads)\n",
    "\n",
    "        self.dyt1 = DyT(self.embed_dim)\n",
    "        self.dyt2 = DyT(self.embed_dim)\n",
    "\n",
    "        self.feed_forward = nn.Sequential([\n",
    "            nn.Dense(self.embed_dim * self.expansion_factor),\n",
    "            nn.relu,\n",
    "            nn.Dense(self.embed_dim)\n",
    "        ])\n",
    "\n",
    "        self.dropout1 = nn.Dropout(rate=0.2)\n",
    "        self.dropout2 = nn.Dropout(rate=0.2)\n",
    "\n",
    "    def __call__(self, key,query,value, deterministic=False):\n",
    "\n",
    "        attention_out = self.attention(key,query,value)\n",
    "        attention_residual_out = attention_out + query\n",
    "        dyt1_out = self.dropout1(self.dyt1(attention_residual_out), deterministic=deterministic)\n",
    "\n",
    "        feed_forward_out = self.feed_forward(dyt1_out)\n",
    "        feed_forward_residual_out = feed_forward_out + dyt1_out\n",
    "        dyt2_out = self.dropout2(self.dyt2(feed_forward_residual_out), deterministic=deterministic)\n",
    "\n",
    "        return dyt2_out\n",
    "\n"
   ],
   "id": "38bfe7740b9040e0",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:08:01.043768Z",
     "start_time": "2025-03-26T11:08:01.039805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    seq_len : int\n",
    "    vocab_size: int\n",
    "    embed_dim: int\n",
    "    num_layers: int = 2\n",
    "    expansion_factor: int = 4\n",
    "    n_heads: int = 8\n",
    "\n",
    "    def setup(self):\n",
    "        self.embedding = Embedding(self.vocab_size, self.embed_dim)\n",
    "        self.positional_embedding = PositionalEmbedding(self.seq_len, self.embed_dim)\n",
    "\n",
    "        self.layers = [TransformerBlock(self.embed_dim, self.expansion_factor, self.n_heads) for _ in range(self.num_layers)]\n",
    "\n",
    "    def __call__(self, x, deterministic=False):\n",
    "        x = self.embedding(x)\n",
    "        x = self.positional_embedding(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x,x,x, deterministic=deterministic)\n",
    "\n",
    "        return x\n"
   ],
   "id": "408dfc6c5291b4bc",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Decoder",
   "id": "4114deebe13026da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:08:01.298721Z",
     "start_time": "2025-03-26T11:08:01.293446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    embed_dim: int\n",
    "    expansion_factor: int = 4\n",
    "    n_heads: int = 8\n",
    "\n",
    "    def setup(self):\n",
    "        self.attention = MultiHeadAttention(self.embed_dim, self.n_heads)\n",
    "        self.dyt = DyT(self.embed_dim)\n",
    "        self.dropout = nn.Dropout(rate=0.2)\n",
    "        self.transformer_block = TransformerBlock(self.embed_dim, self.expansion_factor, self.n_heads)\n",
    "\n",
    "    def __call__(self, key, query, x, mask=None, deterministic=False):\n",
    "        # Self-attention on decoder input\n",
    "        attention = self.attention(x, x, x, mask=mask)\n",
    "        # Add & normalize\n",
    "        value = self.dropout(self.dyt(attention + x), deterministic=deterministic)\n",
    "        # Cross-attention with encoder output\n",
    "        out = self.transformer_block(key, query, value, deterministic=deterministic)\n",
    "\n",
    "        return out"
   ],
   "id": "4b11a5b38e26427e",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:15:05.471531Z",
     "start_time": "2025-03-26T11:15:05.454182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    target_vocab_size: int\n",
    "    embed_dim: int\n",
    "    seq_len: int\n",
    "    num_layers: int = 2\n",
    "    expansion_factor: int = 4\n",
    "    n_heads: int = 8\n",
    "\n",
    "    def setup(self):\n",
    "        self.word_embedding = nn.Embed(self.target_vocab_size, self.embed_dim)\n",
    "        self.positional_embedding = PositionalEmbedding(self.seq_len, self.embed_dim)\n",
    "\n",
    "        self.layers = [DecoderBlock(self.embed_dim, self.expansion_factor, self.n_heads) for _ in range(self.num_layers)]\n",
    "        self.fc_out = nn.Dense(self.target_vocab_size)\n",
    "        self.dropout = nn.Dropout(rate=0.2)\n",
    "\n",
    "    def __call__(self, x, enc_out, mask=None, deterministic=False):\n",
    "        # Get the current sequence length\n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        x = self.word_embedding(x)\n",
    "        x = self.positional_embedding(x)\n",
    "        x = self.dropout(x, deterministic=deterministic)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            # Only pass the mask for the current sequence length\n",
    "            # This prevents the reshape error\n",
    "            if mask is not None:\n",
    "                # Slice the mask to match the current sequence length\n",
    "                current_mask = mask[:, :, :seq_len, :seq_len]\n",
    "            else:\n",
    "                current_mask = None\n",
    "\n",
    "            x = layer(enc_out, enc_out, x, mask=current_mask, deterministic=deterministic)\n",
    "\n",
    "        x = self.fc_out(x)\n",
    "        x = nn.softmax(x, axis=-1)\n",
    "        return x"
   ],
   "id": "6820e5c1f3f10a0c",
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:15:22.330274Z",
     "start_time": "2025-03-26T11:15:22.325016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Transformer(nn.Module):\n",
    "    embed_dim: int\n",
    "    src_vocab_size: int\n",
    "    tgt_vocab_size: int\n",
    "    seq_len: int\n",
    "    num_layers: int = 2\n",
    "    expansion_factor: int = 4\n",
    "    n_heads: int = 8\n",
    "\n",
    "    def setup(self):\n",
    "        self.encoder = TransformerEncoder(self.seq_len, self.src_vocab_size, self.embed_dim, self.num_layers, self.expansion_factor, self.n_heads)\n",
    "        self.decoder = TransformerDecoder(self.tgt_vocab_size, self.embed_dim, self.seq_len, self.num_layers, self.expansion_factor, self.n_heads)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        batch_size, trg_len = trg.shape\n",
    "        # Create a lower triangular matrix of ones (causal mask)\n",
    "        trg_mask = jnp.tril(jnp.ones((trg_len, trg_len)))\n",
    "        # Broadcast the mask to the batch dimension\n",
    "        trg_mask = jnp.broadcast_to(trg_mask[None, None, :, :], (batch_size, 1, trg_len, trg_len))\n",
    "        return trg_mask\n",
    "\n",
    "    def decode(self, src, trg, deterministic=False):\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_out = self.encoder(src, deterministic=deterministic)\n",
    "        out_labels = []\n",
    "        batch_size,seq_len = src.shape[0],src.shape[1]\n",
    "\n",
    "        out = trg\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            out = self.decoder(out, enc_out, mask=trg_mask, deterministic=deterministic)\n",
    "            out = out[:, -1, :]\n",
    "            out = jnp.argmax(out, axis=-1)\n",
    "            out_labels.append(out)\n",
    "            out = out.reshape((batch_size, 1))\n",
    "\n",
    "        return out_labels\n",
    "\n",
    "    def __call__(self, src, trg, deterministic=False):\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_out = self.encoder(src, deterministic=deterministic)\n",
    "        out = self.decoder(trg, enc_out, mask=trg_mask, deterministic=deterministic)\n",
    "        return out\n"
   ],
   "id": "b8f0a54a62c837f0",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:15:23.238575Z",
     "start_time": "2025-03-26T11:15:22.871732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test Transformer\n",
    "\n",
    "src_vocab_size = 11\n",
    "target_vocab_size = 11\n",
    "num_layers = 6\n",
    "seq_length= 12\n",
    "embed_dim = 512\n",
    "expansion_factor = 4\n",
    "n_heads = 8\n",
    "transformer = Transformer(embed_dim, src_vocab_size, target_vocab_size, seq_length, num_layers, expansion_factor, n_heads)\n",
    "params = transformer.init(jax.random.PRNGKey(0), jnp.ones((1, seq_length), dtype=jnp.int32), jnp.ones((1, seq_length), dtype=jnp.int32), deterministic=True)\n",
    "out = transformer.apply(params, jnp.ones((1, seq_length), dtype=jnp.int32), jnp.ones((1, seq_length), dtype=jnp.int32), deterministic=True)"
   ],
   "id": "f52c6bbf98355ed7",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:16:23.687988Z",
     "start_time": "2025-03-26T11:16:23.677582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "src = jnp.array([[0, 2, 5, 6, 4, 3, 9, 5, 2, 9, 10, 1],\n",
    "                 [0, 2, 8, 7, 3, 4, 5, 6, 7, 2, 10, 1]])\n",
    "target = jnp.array([[0, 1, 7, 4, 3, 5, 9, 2, 8, 10, 9, 1],\n",
    "                    [0, 1, 5, 6, 2, 4, 7, 6, 2, 8, 10, 1]])"
   ],
   "id": "90c358d398d4f4fa",
   "outputs": [],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:16:24.482888Z",
     "start_time": "2025-03-26T11:16:24.297086Z"
    }
   },
   "cell_type": "code",
   "source": "out = transformer.apply(params, src, target, deterministic=True)",
   "id": "9218729a4a2fa044",
   "outputs": [],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:16:25.497705Z",
     "start_time": "2025-03-26T11:16:25.492173Z"
    }
   },
   "cell_type": "code",
   "source": "out.shape",
   "id": "bea084952d0ee1d9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 12, 11)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b22717296fe90d66"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
