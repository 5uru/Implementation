{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-26T11:54:00.618829Z",
     "start_time": "2025-03-26T11:54:00.608048Z"
    }
   },
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import math\n",
    "from flax import linen as nn\n",
    "from flax.linen.initializers import constant, ones, zeros"
   ],
   "outputs": [],
   "execution_count": 131
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Word Embeddings",
   "id": "c07879b0c3a124dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:54:01.115881Z",
     "start_time": "2025-03-26T11:54:01.112388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Embedding(nn.Module):\n",
    "    vocab_size: int\n",
    "    emb_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.embed = nn.Embed(self.vocab_size, self.emb_dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.embed(x)"
   ],
   "id": "43ba905deb10d935",
   "outputs": [],
   "execution_count": 132
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Positional Encoding\n",
    "\n"
   ],
   "id": "b5eb874128529ab0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:54:01.520085Z",
     "start_time": "2025-03-26T11:54:01.514590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    max_seq_len: int\n",
    "    embed_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
    "        pe = np.zeros((self.max_seq_len, self.embed_dim))\n",
    "        position = np.arange(0, self.max_seq_len, dtype=np.float32)[:,None]\n",
    "        div_term = np.exp(np.arange(0, self.embed_dim, 2) * (-math.log(10000.0) / self.embed_dim))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "        pe = pe[None]\n",
    "        self.pe = jax.device_put(pe)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x + self.pe[:, :x.shape[1]]\n",
    "        return x"
   ],
   "id": "ed1ed8d7fe98b266",
   "outputs": [],
   "execution_count": 133
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Self Attention",
   "id": "4d3172f259525de4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:54:01.864528Z",
     "start_time": "2025-03-26T11:54:01.860095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention mechanism as described in 'Attention Is All You Need'.\n",
    "\n",
    "    Allows the model to jointly attend to information from different representation subspaces.\n",
    "    \"\"\"\n",
    "    embed_dim: int = 512  # Total dimension of the model\n",
    "    num_heads: int = 8    # Number of attention heads\n",
    "\n",
    "    def setup(self):\n",
    "        # Ensure the dimension is divisible by the number of heads\n",
    "        assert self.embed_dim % self.num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "\n",
    "        # Linear projections for a query, key, and value\n",
    "        # All have output dimension = embed_dim, allowing for parallel computation of heads\n",
    "        self.query = nn.Dense(self.embed_dim, use_bias=False)\n",
    "        self.key = nn.Dense(self.embed_dim, use_bias=False)\n",
    "        self.value = nn.Dense(self.embed_dim, use_bias=False)\n",
    "\n",
    "        # Final output projection\n",
    "        self.out_proj = nn.Dense(self.embed_dim)\n",
    "\n",
    "    def __call__(self, key, query, value, mask=None):\n",
    "        \"\"\"Forward pass for multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            key: Key tensor of shape [batch_size, seq_len_k, embed_dim]\n",
    "            query: Query tensor of shape [batch_size, seq_len_q, embed_dim]\n",
    "            value: Value tensor of shape [batch_size, seq_len_k, embed_dim]\n",
    "            mask: Optional mask tensor for masked attention\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape [batch_size, seq_len_q, embed_dim]\n",
    "        \"\"\"\n",
    "        batch_size = query.shape[0]\n",
    "        seq_len_q = query.shape[1]\n",
    "        seq_len_k = key.shape[1]\n",
    "\n",
    "        # 1. Linear projections and reshape in one computational block\n",
    "        q = self.query(query).reshape(batch_size, seq_len_q, self.num_heads, self.head_dim)\n",
    "        k = self.key(key).reshape(batch_size, seq_len_k, self.num_heads, self.head_dim)\n",
    "        v = self.value(value).reshape(batch_size, seq_len_k, self.num_heads, self.head_dim)\n",
    "\n",
    "        # 2. Transpose to [batch_size, num_heads, seq_len, head_dim]\n",
    "        q = q.transpose(0, 2, 1, 3)  # Shape: [B, H, Lq, D]\n",
    "        k = k.transpose(0, 2, 1, 3)  # Shape: [B, H, Lk, D]\n",
    "        v = v.transpose(0, 2, 1, 3)  # Shape: [B, H, Lk, D]\n",
    "\n",
    "        # 3. Compute scaled dot-product attention\n",
    "        # Matmul q and k, and scale\n",
    "        scale = jnp.sqrt(self.head_dim)\n",
    "        attention_scores = jnp.matmul(q, k.transpose(0, 1, 3, 2)) / scale  # [B, H, Lq, Lk]\n",
    "\n",
    "        # 4. Apply attention mask if provided\n",
    "        if mask is not None:\n",
    "            # A large negative value in softmax becomes ~0 after normalization\n",
    "            attention_scores = jnp.where(mask == 0, jnp.finfo(attention_scores.dtype).min, attention_scores)\n",
    "\n",
    "        # 5. Apply softmax to get attention weights\n",
    "        attention_weights = nn.softmax(attention_scores, axis=-1)  # [B, H, Lq, Lk]\n",
    "\n",
    "        # 6. Apply attention weights to values\n",
    "        context = jnp.matmul(attention_weights, v)  # [B, H, Lq, D]\n",
    "\n",
    "        # 7. Reshape back to the original sequence length and embedding dimension\n",
    "        context = context.transpose(0, 2, 1, 3).reshape(batch_size, seq_len_q, self.embed_dim)\n",
    "\n",
    "        # 8. Final linear projection\n",
    "        output = self.out_proj(context)\n",
    "\n",
    "        return output"
   ],
   "id": "92187cbad03f44cb",
   "outputs": [],
   "execution_count": 134
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "DyT",
   "id": "66e02a11cd5de67c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:54:02.667108Z",
     "start_time": "2025-03-26T11:54:02.662059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DyT(nn.Module):\n",
    "    \"\"\"Dynamic Tanh (DyT) normalization layer.\n",
    "\n",
    "    This layer performs normalization using a learnable scaled tanh activation\n",
    "    followed by an affine transformation (weight * x + bias).\n",
    "\n",
    "    Attributes:\n",
    "        num_features: Number of features/channels to normalize\n",
    "        alpha_init: Initial value for the tanh scaling parameter (default: 0.5)\n",
    "    \"\"\"\n",
    "    num_features: int\n",
    "    alpha_init: float = 0.5\n",
    "\n",
    "    def setup(self):\n",
    "        # Learnable scaling parameter for tanh activation\n",
    "        self.alpha = self.param('alpha', constant(self.alpha_init), ())  # Scalar parameter\n",
    "\n",
    "        # Affine transformation parameters (similar to BatchNorm)\n",
    "        self.weight = self.param('weight', ones, (self.num_features,))    # Scale parameter\n",
    "        self.bias = self.param('bias', zeros, (self.num_features,))       # Shift parameter\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Apply scaled tanh activation\n",
    "        # constrains values between -1 and 1 while alpha controls steepness\n",
    "        normalized = nn.tanh(self.alpha * x)\n",
    "\n",
    "        # Apply affine transformation (channel-wise scaling and shifting)\n",
    "        return normalized * self.weight + self.bias"
   ],
   "id": "dbdd1b0edeb250a2",
   "outputs": [],
   "execution_count": 135
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Encoder",
   "id": "7171549525bd9d46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:54:03.444251Z",
     "start_time": "2025-03-26T11:54:03.439916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block with Dynamic Tanh (DyT) normalization instead of LayerNorm.\n",
    "\n",
    "    Implements a standard transformer block with multi-head attention followed by\n",
    "    a feed-forward network, using DyT for normalization and residual connections.\n",
    "    \"\"\"\n",
    "    embed_dim: int\n",
    "    expansion_factor: int = 4\n",
    "    n_heads: int = 8\n",
    "\n",
    "    def setup(self):\n",
    "        # Multi-head self-attention mechanism\n",
    "        self.attention = MultiHeadAttention(self.embed_dim, self.n_heads)\n",
    "\n",
    "        # DyT normalization layers (alternative to LayerNorm)\n",
    "        self.dyt1 = DyT(self.embed_dim)  # After attention block\n",
    "        self.dyt2 = DyT(self.embed_dim)  # After feed-forward block\n",
    "\n",
    "        # Position-wise feed-forward network\n",
    "        self.feed_forward = nn.Sequential([\n",
    "                nn.Dense(self.embed_dim * self.expansion_factor),\n",
    "                nn.relu,\n",
    "                nn.Dense(self.embed_dim)\n",
    "        ])\n",
    "\n",
    "        # Dropout layers for regularization\n",
    "        self.dropout1 = nn.Dropout(rate=0.2)\n",
    "        self.dropout2 = nn.Dropout(rate=0.2)\n",
    "\n",
    "    def __call__(self, key, query, value, deterministic=False):\n",
    "        # 1. Multi-head attention sublayer\n",
    "        attention_out = self.attention(key, query, value)\n",
    "        attention_residual_out = attention_out + query  # Residual connection\n",
    "\n",
    "        # 2. Normalization and dropout\n",
    "        dyt1_out = self.dropout1(self.dyt1(attention_residual_out), deterministic=deterministic)\n",
    "\n",
    "        # 3. Feed-forward sublayer\n",
    "        feed_forward_out = self.feed_forward(dyt1_out)\n",
    "        feed_forward_residual_out = feed_forward_out + dyt1_out  # Residual connection\n",
    "\n",
    "        # 4. Final normalization and dropout\n",
    "        output = self.dropout2(self.dyt2(feed_forward_residual_out), deterministic=deterministic)\n",
    "\n",
    "        return output"
   ],
   "id": "38bfe7740b9040e0",
   "outputs": [],
   "execution_count": 136
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:54:03.927969Z",
     "start_time": "2025-03-26T11:54:03.923055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"Transformer encoder with multiple transformer blocks.\n",
    "\n",
    "    Processes input sequences through word embeddings, positional encodings,\n",
    "    and multiple transformer layers for contextual representation.\n",
    "    \"\"\"\n",
    "    seq_len: int         # Maximum sequence length supported\n",
    "    vocab_size: int      # Size of input vocabulary\n",
    "    embed_dim: int       # Dimension of embeddings\n",
    "    num_layers: int = 2  # Number of transformer blocks\n",
    "    expansion_factor: int = 4  # Expansion factor for feed-forward network\n",
    "    n_heads: int = 8     # Number of attention heads\n",
    "\n",
    "    def setup(self):\n",
    "        # Initialize embedding layers\n",
    "        self.embedding = Embedding(self.vocab_size, self.embed_dim)\n",
    "        self.positional_embedding = PositionalEmbedding(self.seq_len, self.embed_dim)\n",
    "\n",
    "        # Create a stack of transformer blocks\n",
    "        self.layers = [\n",
    "                TransformerBlock(\n",
    "                        embed_dim=self.embed_dim,\n",
    "                        expansion_factor=self.expansion_factor,\n",
    "                        n_heads=self.n_heads\n",
    "                )\n",
    "                for _ in range(self.num_layers)\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x, deterministic=False):\n",
    "        \"\"\"Forward pass through the encoder.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of token indices [batch_size, seq_len]\n",
    "            deterministic: Whether to run in deterministic mode (no dropout)\n",
    "\n",
    "        Returns:\n",
    "            Encoded representation [batch_size, seq_len, embed_dim]\n",
    "        \"\"\"\n",
    "        # Convert tokens to embeddings\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Add positional information\n",
    "        x = self.positional_embedding(x)\n",
    "\n",
    "        # Pass through each transformer block sequentially\n",
    "        # Using the same tensor for key, query and value (self-attention)\n",
    "        for layer in self.layers:\n",
    "            x = layer(key=x, query=x, value=x, deterministic=deterministic)\n",
    "\n",
    "        return x"
   ],
   "id": "408dfc6c5291b4bc",
   "outputs": [],
   "execution_count": 137
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Decoder",
   "id": "4114deebe13026da"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:54:04.393952Z",
     "start_time": "2025-03-26T11:54:04.389704Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"Transformer decoder block using DyT normalization.\n",
    "\n",
    "    Performs masked self-attention on decoder inputs, followed by\n",
    "    cross-attention with encoder outputs and a feed-forward network.\n",
    "    \"\"\"\n",
    "    embed_dim: int         # Dimension of embeddings\n",
    "    expansion_factor: int = 4  # Expansion factor for feed-forward network\n",
    "    n_heads: int = 8      # Number of attention heads\n",
    "\n",
    "    def setup(self):\n",
    "        # First attention sub-layer (masked self-attention)\n",
    "        self.self_attention = MultiHeadAttention(self.embed_dim, self.n_heads)\n",
    "        self.dyt1 = DyT(self.embed_dim)\n",
    "        self.dropout1 = nn.Dropout(rate=0.2)\n",
    "\n",
    "        # Second sublayer (cross-attention + feed-forward network)\n",
    "        # Reusing TransformerBlock for cross-attention and feed-forward\n",
    "        self.cross_attention_block = TransformerBlock(\n",
    "                self.embed_dim,\n",
    "                self.expansion_factor,\n",
    "                self.n_heads\n",
    "        )\n",
    "\n",
    "    def __call__(self, encoder_output, x, mask=None, deterministic=False):\n",
    "        \"\"\"Forward pass through decoder block.\n",
    "\n",
    "        Args:\n",
    "            encoder_output: Output from encoder [batch_size, seq_len, embed_dim]\n",
    "            x: Decoder input [batch_size, seq_len, embed_dim]\n",
    "            mask: Optional causal mask for self-attention\n",
    "            deterministic: Whether to disable dropout\n",
    "\n",
    "        Returns:\n",
    "            Decoder block output [batch_size, seq_len, embed_dim]\n",
    "        \"\"\"\n",
    "        # 1. Self-attention with a causal mask\n",
    "        attn_output = self.self_attention(x, x, x, mask=mask)\n",
    "\n",
    "        # 2. Residual connection + DyT normalization + dropout\n",
    "        norm_output = self.dropout1(\n",
    "                self.dyt1(attn_output + x),  # Add & normalize\n",
    "                deterministic=deterministic\n",
    "        )\n",
    "\n",
    "        # 3. Cross-attention with encoder output + feed-forward network\n",
    "        # TransformerBlock handles cross-attention, normalization, and feed-forward\n",
    "        output = self.cross_attention_block(\n",
    "                key=encoder_output,    # Key from encoder\n",
    "                query=encoder_output,  # Query from encoder\n",
    "                value=norm_output,     # Value from the first sublayer\n",
    "                deterministic=deterministic\n",
    "        )\n",
    "\n",
    "        return output"
   ],
   "id": "4b11a5b38e26427e",
   "outputs": [],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:54:04.753055Z",
     "start_time": "2025-03-26T11:54:04.747919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"Transformer decoder that processes target sequences with the help of encoder output.\n",
    "\n",
    "    Takes encoded source sequence representations and target tokens to produce\n",
    "    probability distributions over the target vocabulary.\n",
    "    \"\"\"\n",
    "    target_vocab_size: int  # Size of target vocabulary\n",
    "    embed_dim: int          # Dimension of embeddings\n",
    "    seq_len: int            # Maximum sequence length\n",
    "    num_layers: int = 2     # Number of decoder layers\n",
    "    expansion_factor: int = 4  # Expansion factor for feed-forward network\n",
    "    n_heads: int = 8        # Number of attention heads\n",
    "\n",
    "    def setup(self):\n",
    "        # Token embedding layer converts token IDs to vectors\n",
    "        self.word_embedding = nn.Embed(self.target_vocab_size, self.embed_dim)\n",
    "\n",
    "        # Add positional information to embeddings\n",
    "        self.positional_embedding = PositionalEmbedding(self.seq_len, self.embed_dim)\n",
    "\n",
    "        # Stack of decoder blocks for iterative refinement\n",
    "        self.layers = [\n",
    "                DecoderBlock(\n",
    "                        self.embed_dim,\n",
    "                        self.expansion_factor,\n",
    "                        self.n_heads\n",
    "                ) for _ in range(self.num_layers)\n",
    "        ]\n",
    "\n",
    "        # Final projection to vocabulary size\n",
    "        self.fc_out = nn.Dense(self.target_vocab_size)\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(rate=0.2)\n",
    "\n",
    "    def __call__(self, x, enc_out, mask=None, deterministic=False):\n",
    "        \"\"\"Forward pass through the decoder.\n",
    "\n",
    "        Args:\n",
    "            x: Target token IDs [batch_size, seq_len]\n",
    "            enc_out: Encoder output [batch_size, src_seq_len, embed_dim]\n",
    "            mask: Optional causal mask for self-attention\n",
    "            deterministic: Whether to disable dropout\n",
    "\n",
    "        Returns:\n",
    "            Token probability distributions [batch_size, seq_len, target_vocab_size]\n",
    "        \"\"\"\n",
    "        # Extract current batch and sequence dimensions\n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        # Convert token IDs to embeddings and add positional information\n",
    "        x = self.word_embedding(x)\n",
    "        x = self.positional_embedding(x)\n",
    "        x = self.dropout(x, deterministic=deterministic)\n",
    "\n",
    "        # Process through each decoder layer\n",
    "        for layer in self.layers:\n",
    "            # Adapt mask to the current sequence length to prevent shape errors\n",
    "            current_mask = None\n",
    "            if mask is not None:\n",
    "                current_mask = mask[:, :, :seq_len, :seq_len]\n",
    "\n",
    "            # Pass through the decoder layer with cross-attention to the encoder output\n",
    "            x = layer(enc_out, x, mask=current_mask, deterministic=deterministic)\n",
    "\n",
    "        # Project to vocabulary size and apply softmax for probabilities\n",
    "        return nn.softmax(self.fc_out(x), axis=-1)"
   ],
   "id": "6820e5c1f3f10a0c",
   "outputs": [],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:54:05.204320Z",
     "start_time": "2025-03-26T11:54:05.198218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"Complete Transformer model with encoder and decoder stacks.\n",
    "\n",
    "    Implements the full Transformer architecture as described in\n",
    "    \"Attention Is All You Need\" with DyT normalization instead of LayerNorm.\n",
    "    \"\"\"\n",
    "    embed_dim: int        # Dimension of token embeddings\n",
    "    src_vocab_size: int   # Size of source vocabulary\n",
    "    tgt_vocab_size: int   # Size of target vocabulary\n",
    "    seq_len: int          # Maximum sequence length\n",
    "    num_layers: int = 2   # Number of encoder/decoder layers\n",
    "    expansion_factor: int = 4  # Expansion factor in feed-forward networks\n",
    "    n_heads: int = 8      # Number of attention heads\n",
    "\n",
    "    def setup(self):\n",
    "        # Initialize encoder and decoder components\n",
    "        self.encoder = TransformerEncoder(\n",
    "                self.seq_len,\n",
    "                self.src_vocab_size,\n",
    "                self.embed_dim,\n",
    "                self.num_layers,\n",
    "                self.expansion_factor,\n",
    "                self.n_heads\n",
    "        )\n",
    "\n",
    "        self.decoder = TransformerDecoder(\n",
    "                self.tgt_vocab_size,\n",
    "                self.embed_dim,\n",
    "                self.seq_len,\n",
    "                self.num_layers,\n",
    "                self.expansion_factor,\n",
    "                self.n_heads\n",
    "        )\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        \"\"\"Create a causal attention mask for the decoder.\n",
    "\n",
    "        Ensures that predictions at position I can only attend to known\n",
    "        outputs at positions less than i.\n",
    "\n",
    "        Args:\n",
    "            trg: Target sequence tensor of shape [batch_size, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            Mask tensor of shape [batch_size, 1, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size, trg_len = trg.shape\n",
    "        # Lower triangular matrix (future tokens are masked)\n",
    "        trg_mask = jnp.tril(jnp.ones((trg_len, trg_len)))\n",
    "        # Add batch and head dimensions [batch_size, 1, trg_len, trg_len]\n",
    "        return jnp.broadcast_to(trg_mask[None, None, :, :], (batch_size, 1, trg_len, trg_len))\n",
    "\n",
    "    def decode(self, src, trg, deterministic=False):\n",
    "        \"\"\"Autoregressive decoding for inference.\n",
    "\n",
    "        Takes source sequence and initial target tokens, then generates\n",
    "        output sequence one token at a time.\n",
    "\n",
    "        Args:\n",
    "            src: Source sequence [batch_size, seq_len]\n",
    "            trg: Initial target sequence [batch_size, initial_len]\n",
    "            deterministic: Whether to disable dropout\n",
    "\n",
    "        Returns:\n",
    "            List of output token IDs for each position\n",
    "        \"\"\"\n",
    "        # Create an attention mask and encode a source sequence (done once)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_out = self.encoder(src, deterministic=deterministic)\n",
    "\n",
    "        batch_size, seq_len = src.shape\n",
    "        out_labels = []\n",
    "        out = trg  # Start with the provided target tokens\n",
    "\n",
    "        # Generate tokens sequentially\n",
    "        for _ in range(seq_len):\n",
    "            # Get probability distribution over vocabulary\n",
    "            decoder_output = self.decoder(out, enc_out, mask=trg_mask, deterministic=deterministic)\n",
    "\n",
    "            # Extract prediction for the last position\n",
    "            last_token_logits = decoder_output[:, -1, :]\n",
    "            predicted_token = jnp.argmax(last_token_logits, axis=-1)\n",
    "            out_labels.append(predicted_token)\n",
    "\n",
    "            # Append prediction to sequence for the next iteration\n",
    "            predicted_token = predicted_token.reshape((batch_size, 1))\n",
    "            out = jnp.concatenate([out, predicted_token], axis=1)\n",
    "\n",
    "            # Update mask for the new sequence length\n",
    "            trg_mask = self.make_trg_mask(out)\n",
    "\n",
    "        return out_labels\n",
    "\n",
    "    def __call__(self, src, trg, deterministic=False):\n",
    "        \"\"\"Forward pass for training.\n",
    "\n",
    "        Process source and target sequences in parallel through encoder and decoder.\n",
    "\n",
    "        Args:\n",
    "            src: Source sequence [batch_size, seq_len]\n",
    "            trg: Target sequence [batch_size, seq_len]\n",
    "            deterministic: Whether to disable dropout\n",
    "\n",
    "        Returns:\n",
    "            Output logits [batch_size, seq_len, tgt_vocab_size]\n",
    "        \"\"\"\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_out = self.encoder(src, deterministic=deterministic)\n",
    "        return self.decoder(trg, enc_out, mask=trg_mask, deterministic=deterministic)"
   ],
   "id": "b8f0a54a62c837f0",
   "outputs": [],
   "execution_count": 140
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:54:14.428648Z",
     "start_time": "2025-03-26T11:54:14.047233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test Transformer\n",
    "\n",
    "src_vocab_size = 11\n",
    "target_vocab_size = 11\n",
    "num_layers = 6\n",
    "seq_length= 12\n",
    "embed_dim = 512\n",
    "expansion_factor = 4\n",
    "n_heads = 8\n",
    "transformer = Transformer(embed_dim, src_vocab_size, target_vocab_size, seq_length, num_layers, expansion_factor, n_heads)\n",
    "params = transformer.init(jax.random.PRNGKey(0), jnp.ones((1, seq_length), dtype=jnp.int32), jnp.ones((1, seq_length), dtype=jnp.int32), deterministic=True)\n",
    "out = transformer.apply(params, jnp.ones((1, seq_length), dtype=jnp.int32), jnp.ones((1, seq_length), dtype=jnp.int32), deterministic=True)"
   ],
   "id": "f52c6bbf98355ed7",
   "outputs": [],
   "execution_count": 145
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:54:06.279208Z",
     "start_time": "2025-03-26T11:54:06.276503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "src = jnp.array([[0, 2, 5, 6, 4, 3, 9, 5, 2, 9, 10, 1],\n",
    "                 [0, 2, 8, 7, 3, 4, 5, 6, 7, 2, 10, 1]])\n",
    "target = jnp.array([[0, 1, 7, 4, 3, 5, 9, 2, 8, 10, 9, 1],\n",
    "                    [0, 1, 5, 6, 2, 4, 7, 6, 2, 8, 10, 1]])"
   ],
   "id": "90c358d398d4f4fa",
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:54:07.168508Z",
     "start_time": "2025-03-26T11:54:07.015216Z"
    }
   },
   "cell_type": "code",
   "source": "out = transformer.apply(params, src, target, deterministic=True)",
   "id": "9218729a4a2fa044",
   "outputs": [],
   "execution_count": 143
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T11:54:16.808781Z",
     "start_time": "2025-03-26T11:54:16.806112Z"
    }
   },
   "cell_type": "code",
   "source": "out.shape",
   "id": "bea084952d0ee1d9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12, 11)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 146
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b22717296fe90d66"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
